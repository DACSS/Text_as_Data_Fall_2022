---
title: "Blog Post Five: Looking at STM and K-Means"
author: "Molly Hackbarth"
description: "Working with the data"
date: "10/29/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw5
  - Molly Hackbarth
---

```{r}
#| label: setup
#| warning: false

library(tidyverse)
library(cld3)
library(dplyr)
library(here)
library(devtools)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.dictionaries)
library(quanteda.sentiment)
#new packages
library(patchwork)
library(stm)
library(tm)
knitr::opts_chunk$set(echo = TRUE)
```

# Research Question

**My current research question:** How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan*?

# Reading in the Data

```{r reading data}
#write csv has been commented out due to it continously trying to save an "updated version" in Git. 

reddit_data <- read.csv(here::here("posts", "_data", "loveisblindjapan.csv"))

twitter1 <- read.csv(here::here("posts", "_data", "tweets.csv"))

twitter2 <- read.csv(here::here("posts", "_data", "tweets#.csv"))

reddit <- subset(reddit_data, select = c("body", "created_utc")) 

reddit$created_utc <- as.Date.POSIXct(reddit$created_utc)

reddit <- reddit %>% 
  select(text = body, 
            date = created_utc)
# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])
reddit <- reddit %>% 
  filter(!text == '[deleted]') %>% 
  filter(!text == '[removed]')

#remove counting column
twitter1 <- twitter1 %>% select(!c(X, User))
twitter2 <- twitter2 %>% select(!c(X, User))

twitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)
#write.csv(twitter, here::here("posts", "_data", "twitter.csv") , all(T) )

names(twitter) <- tolower(names(twitter))
twitter <- twitter %>% 
  rename_at('tweet', ~ 'text', 
            'Date' ~ 'date')
twitter$date <- as.Date(strftime(twitter$date, format="%Y-%m-%d"))

# remove duplicate tweets
twitter <- twitter %>% distinct(text, date, .keep_all = TRUE)

#check for duplicate tweets
twitter %in% unique(twitter[ duplicated(twitter)]) 

```

```{r lemmentizing the data}
# Twiter Lemmitized
twitter_corpus <- subset(twitter, detect_language(twitter) == "en")
twitter_corpus <- corpus(twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]
twittersummary <- summary(twitter_corpus)
twitter_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", twitter_corpus))

mystopwords <- c("love is blind japan", "#loveisbindjapan", "#LoveIsBlindJapan","Love Is Blind Japan","Love is Blind Japan", "Love Is Blind: Japan", "#loveisblind", "ラブイズブラインドjapan", "#ラブイズブラインドjapan", "loveisblind", "#loveisblind2", "blind:japan", "blind", "show")

twitter_corpus_tokens <- tokens(twitter_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

twitter_lemmitized <- tokens_replace(twitter_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

# Reddit Lemmitized

reddit_corpus <- subset(reddit, detect_language(reddit) == "en")
reddit_corpus <- corpus(reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]
redditsummary <- summary(reddit_corpus)

reddit_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", reddit_corpus))

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T, 
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

```

```{r creating nrc dictionary}
#Twitter NRC

twitterDfm_nrc <- dfm(tokens(twitter_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

twitter_corpus_dfm <- twitter_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

# Reddit NRC

redditDfm_nrc <- dfm(tokens(reddit_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

reddit_corpus_dfm <- reddit_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

```

# Trying to use Dates with Average Sentiments

Something I would like to try to do is create a graph using the current sentiment from NRC and the dates these were published. In order to do this I'll first try using [this idea](https://stackoverflow.com/questions/58918872/performing-time-series-analysis-of-quanteda-tokens/58938067#58938067).

```{r a new data frame}
dfm <- dfm(twitter_lemmitized)

dfm_sub <- dfm_keep(dfm, 
                    pattern = c("love", "japan"),
                    valuetype = "fixed", 
                    case_insensitive = TRUE)

df <- convert(dfm_sub, "data.frame")
df$date <- dfm@docvars$date

df %>% 
  pivot_longer(love:japan, names_to = "word") %>% 
  ggplot(aes(x = date, y = word)) +
  geom_line()
```

Here we can see that this may work! I would like to test this with the "tdf_nrc"

```{r trying with tdf nrc}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

tdf_nrc %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()


```

After playing around I was able to add the date column to my nrc dictionary! As you can see though it's a bit of a mess. This time I'm going to try and group by month.

```{r testing summarized version}
test <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))


test %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()
```

Here is what I was thinking about! As you can see I used the mean to find the average polarity by date. Overall we're able to see that most of the time the show was trending towards positive with a few large dips throughout.

Let's set this up properly for reddit and twitter.

```{r testing dates}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$date <- redditDfm_nrc@docvars$date
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

twitter_by_date <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))

twitter_by_date %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()

reddit_by_date <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))

reddit_by_date %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()

```

Here we're able to see both graphs average sentiment over time! This is very useful to me as it allows me to compare. Now lets compare these

```{r compare average sentiment}

ggplot() +
geom_line(data = reddit_by_date, aes(x = date, y = polarity, color="Reddit")) + geom_line(data = twitter_by_date, aes(x = date, y = polarity, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Dictionary Sentiment")

```

This looks fairly good! However this may be too much detail to just look at overall sentiment. Below you will see a smoothed version.

```{r smoothed twitter vs reddit analysis}

twitter_reddit_sentiment <- ggplot() +
geom_smooth(data = reddit_by_date, aes(x = date, y = polarity, color="Reddit")) + geom_smooth(data = twitter_by_date, aes(x = date, y = polarity, color = "Twitter")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit', 'Twitter'),
                  values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Dictionary Sentiment")

twitter_reddit_sentiment

```

This is a lot easier to read! Here you can also see something interesting. Although Twitter seems to have started before 2022, Reddit data starts somewhere in late January. This is likely because Twitter had prior notice and had been tweeting about curiosity by what the show might be about. Likely Reddit did not make a sub reddit until after the show had started.

We can also see additionally that Twitter has overall had a more positive sentiment to the show than Reddit, however they are very close.

# Emotional Rating based on Polarity

Below you will find me trying to test all sentiments over time.

```{r twitter and reddit emotions}

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  pivot_longer(anger:trust, names_to = "words")

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  pivot_longer(anger:trust, names_to = "words")
  

ggplot() +
geom_point(data = reddit_emotions, aes(x = date, y = value, color = words)) + 
  labs(title="Reddit Emotional Count")

ggplot() + 
  geom_col(data = twitter_emotions, aes(x = date, y = value, color = words), position = "dodge") + labs(title="Twitter Emotional Count")
```

When I look at this I find it fairly difficult to understand. I had thought putting them all in one column to compare against each other may be best. However now I'm wondering if it would make more sense to not have them combined. This has lead me to my other thought, comparing Twitter and Reddit emotional word count on the same graph one by one.

Since Reddit and Twitter are not the same amount of text I will go ahead and use the mean for each emotion.

```{r one by one count}

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = mean(anger),
           anticipation = mean(anticipation), 
           disgust = mean(disgust),
           fear = mean(fear),
           joy = mean(joy),
           negative = mean(negative),
           positive = mean(positive),
           sadness = mean(sadness),
           surprise = mean(surprise),
           trust = mean(trust))

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = mean(anger),
           anticipation = mean(anticipation), 
           disgust = mean(disgust),
           fear = mean(fear),
           joy = mean(joy),
           negative = mean(negative),
           positive = mean(positive),
           sadness = mean(sadness),
           surprise = mean(surprise),
           trust = mean(trust))

anger <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anger, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anger, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average anger by date")

anticipation <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anticipation, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anticipation, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average anticipation by date")

disgust <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = disgust, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = disgust, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average disgust by date")

fear <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = fear, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = fear, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average fear by date")

joy <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = joy, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = joy, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average joy by date")

sadness <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = sadness, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = sadness, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average sadness by date")

surprise <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = surprise, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = surprise, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average surprise by date")

trust <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = trust, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = trust, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average trust by date")


negative <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = negative, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = negative, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average negativeness by date")

positive <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = positive, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = positive, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average positveness by date")

anger / disgust 
  
fear / sadness 

anticipation / joy 

surprise / trust

positive/negative

```

Although mean is very helpful to both I believe what I want to do is find the percent of the emotion per day rather than just the average. You will see me working on it below.

```{r percentage based twitter vs reddit}

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
    rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3) * 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3)* 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)
  

anger <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anger_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anger_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anger percentage by date")

anticipation <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anticipation_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anticipation_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anticipation percentage by date")

disgust <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = disgust_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = disgust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit disgust percentage by date")

fear <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = fear_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = fear_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit fear percentage by date")

joy <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = joy_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = joy_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit joy percentage by date")

sadness <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = sadness_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = sadness_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit sadness percentage by date")

surprise <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = surprise_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = surprise_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit surprise percentage by date")

trust <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = trust_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = trust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit trust percentage by date")


negative <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = negative_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = negative_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit negativeness percentage by date")

positive <- ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = positive_percent, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = positive_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit positveness percentage by date")

anger / disgust 
  
fear / sadness 

anticipation / joy 

surprise / trust

positive/negative
```

Overall we can see with the exception of a few spikes that are likely because of only having a few words, twitter and reddit seem to be very similar about their emotions over time for the show.

## Lack of Words on Certain Days

I noticed that some days have particularly small amount of total words. While this isn't too surprising it does make me wondering if those are useful to the data set. Thus for this data I will remove any dates that had **less than 100 words.**

```{r filter count}

twitter_emotions <- twitter_emotions %>% 
  filter(word_count > 100)

reddit_emotions <- reddit_emotions %>% 
  filter(word_count > 100)

anger <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anger_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anger_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anger percentage by date")

anticipation <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anticipation_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anticipation_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anticipation percentage by date")

disgust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = disgust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = disgust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit disgust percentage by date")

fear <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = fear_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = fear_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit fear percentage by date")

joy <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = joy_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = joy_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit joy percentage by date")

sadness <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = sadness_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = sadness_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit sadness percentage by date")

surprise <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = surprise_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = surprise_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit surprise percentage by date")

trust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = trust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = trust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit trust percentage by date")


negative <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = negative_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = negative_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit negativeness percentage by date")

positive <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = positive_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = positive_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit positveness percentage by date")

anger / disgust 
  
fear / sadness 

anticipation / joy 

surprise / trust

positive/negative


twitter_reddit_sentiment <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit', 'Twitter'),
                  values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Average Dictionary Sentiment")

twitter_reddit_sentiment

```

# Co-occurrence

After looking at co-occurrence I have decided to include a minimum term frequency of 100. think I may stick to just using the textplot version of it. This is because I believe that with each data sample if a word is used less than 100 times it's not as important. Here I'll keep the top 30 features of both Twitter and Reddit.

```{r reddit and twitter co-occurrence}

## Reddit

rsmaller_dfm <- dfm_trim(reddit_corpus_dfm, min_termfreq = 100)

# create fcm from dfm
rsmaller_fcm <- fcm(rsmaller_dfm)

rmyFeatures <- names(topfeatures(rsmaller_fcm, 30))

# retain only those top features as part of our matrix
reven_smaller_fcm <- fcm_select(rsmaller_fcm, pattern = rmyFeatures, selection = "keep")

# compute size weight for vertices in network
rsize <- log(colSums(reven_smaller_fcm))

# create plot
textplot_network(reven_smaller_fcm, vertex_size = rsize / max(rsize) * 3)

## TWITTER

tsmaller_dfm <- dfm_trim(twitter_corpus_dfm, min_termfreq = 100)

# create fcm from dfm
smaller_fcm <- fcm(tsmaller_dfm)

myFeatures <- names(topfeatures(smaller_fcm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

Something I notice right away is that Reddit connections are more linked towards the contestants names on the show. However for Twitter it's more connected to what people felt about the show.

# Checking the K-Means

Below I will focus on checking on how many "k" options I need. I will start with 25.

```{r checking k}

## reddit 
truth.K <- 25
rdm <- dfm(reddit_lemmitized)
rdm <- convert(rdm, to = "tm")
rdm.tfidf <- tm::weightTfIdf(rdm)
rdm.tfidf <- tm::removeSparseTerms(rdm.tfidf, 0.999) # (data,allowed sparsity)
rfidf.matrix <- as.matrix(rdm.tfidf)
rdist.matrix = proxy::dist(rfidf.matrix, method = "cosine")
rclustering.kmeans <- kmeans(rfidf.matrix, truth.K)
names(rclustering.kmeans)

k <- 25
rvarper <- NULL
for(i in 1:k){
  rclustering.kmeans2 <- kmeans(rfidf.matrix, i)
  rvarper <- c(rvarper, as.numeric(rclustering.kmeans2$betweenss)/as.numeric(rclustering.kmeans2$totss))
}

rvarper

plot(1:k, rvarper, xlab = "# of clusters", ylab = "explained variance")

## twitter 

truth.K <- 25
tdm <- tm::DocumentTermMatrix(twitter_lemmitized)
tdm <- dfm(twitter_lemmitized)
tdm <- convert(tdm, to = "tm")
tdm.tfidf <- tm::weightTfIdf(tdm)
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999) # (data,allowed sparsity)
tfidf.matrix <- as.matrix(tdm.tfidf)
tdist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
tclustering.kmeans <- kmeans(tfidf.matrix, truth.K)
names(tclustering.kmeans)

k <- 25
tvarper <- NULL
for(i in 1:k){
  tclustering.kmeans2 <- kmeans(tfidf.matrix, i)
  tvarper <- c(tvarper, as.numeric(tclustering.kmeans2$betweenss)/as.numeric(tclustering.kmeans2$totss))
}

tvarper

plot(1:k, tvarper, xlab = "# of clusters", ylab = "explained variance")

```

Here we can see that 25 topics may not be enough surprisingly. While there is a bit of a plateau at 20, I will try to do 50.

```{r testing 50k}

k <- 50
rvarper <- NULL
for(i in 1:k){
  rclustering.kmeans2 <- kmeans(rfidf.matrix, i)
  rvarper <- c(rvarper, as.numeric(rclustering.kmeans2$betweenss)/as.numeric(rclustering.kmeans2$totss))
}

plot(1:k, rvarper, xlab = "# of clusters", ylab = "explained variance")

k <- 50
tvarper <- NULL
for(i in 1:k){
  tclustering.kmeans2 <- kmeans(tfidf.matrix, i)
  tvarper <- c(tvarper, as.numeric(tclustering.kmeans2$betweenss)/as.numeric(tclustering.kmeans2$totss))
}

tvarper

plot(1:k, tvarper, xlab = "# of clusters", ylab = "explained variance")
```

Well 50 certainly seems like too many! It still seems that around 20-25 is the sweet spot for an equal explained variance. I think I will stick with 25.

Still lets check it one more way.

```{r checking once more}
ktwitter <- dfm(twitter_lemmitized)
ktwitter$polarity <- tdf_nrc$polarity

ktwitter$date <- as.numeric(ktwitter$date)

tdifferentKs <- searchK(ktwitter,
                       K = c(5, 25, 50),
                       N = 250,
                       max.em.its = 1000,
                       init.type = "Spectral")

plot(tdifferentKs)

kreddit <- dfm(reddit_lemmitized)
kreddit$polarity <- rdf_nrc$polarity

kreddit$date <- as.numeric(kreddit$date)

rdifferentKs <- searchK(kreddit,
                       K = c(5, 25, 50),
                       N = 250,
                       max.em.its = 1000,
                       init.type = "Spectral")

plot(rdifferentKs)
```

# STM

Below I will attempt to do structure topic modeling for both Reddit and Twitter

```{r stm reddit}
k <- 25
rModel <- dfm(reddit_lemmitized)

docvars(reddit_lemmitized) <- rModel

rModel$polarity <- rdf_nrc$polarity

rModel <- stm(rModel,
                K = k,
              prevalence = ~ polarity,
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

#labelTopics(rModel)
plot(rModel, type = "summary")

# get the words
rTopicNames <- labelTopics(rModel, n=4)$frex

# set up an empty vector
rTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
  rTopicLabels[i] <- paste(rTopicNames[i,], collapse = "_")
}

# print the names
rTopicLabels

# estimate effects
rmodelEffects <- estimateEffect(formula = 1:k ~ polarity,
                               stmobj = rModel,
                               metadata = rModel)

# plot effects
rRows <- 2
par(mfrow = c(rRows, 3), bty = "n", lwd = 2)
for (i in 1:k){
  plot.estimateEffect(rmodelEffects,
                      covariate = "polarity",
                      xlim = c(-.25, .25),
                      model = rModel,
                      topics = rmodelEffects$topics[i],
                      method = "difference",
                      cov.value1 = 1,
                      cov.value2 = 0, 
                      main = rTopicLabels[i],
                      printlegend = F,
                      linecol = "grey26",
                      labeltype = "custom",
                      verbose.labels = F,
                      custom.labels = c(""))
  par(new = F)
}


```

```{r stm twitter}
k <- 25
### TWITTER
tModel <- dfm(twitter_lemmitized)

docvars(twitter_lemmitized) <- tModel

tModel$polarity <- tdf_nrc$polarity

tModel <- stm(tModel,
                K = k,
              prevalence = ~ polarity,
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

labelTopics(tModel)
plot(tModel, type = "summary")

# get the words
tTopicNames <- labelTopics(tModel, n=4)$frex

# set up an empty vector
tTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
  tTopicLabels[i] <- paste(tTopicNames[i,], collapse = "_")
}

# print the names
tTopicLabels

# estimate effects
tmodelEffects <- estimateEffect(formula = 1:k ~ polarity,
                               stmobj = tModel,
                               metadata = tModel)

# plot effects
tRows <- 2
par(mfrow = c(tRows, 3), bty = "n", lwd = 2)
for (i in 1:k){
  plot.estimateEffect(tmodelEffects,
                      covariate = "polarity",
                      xlim = c(-.25, .25),
                      model = tModel,
                      topics = tmodelEffects$topics[i],
                      method = "difference",
                      cov.value1 = 1,
                      cov.value2 = 0, 
                      main = tTopicLabels[i],
                      printlegend = F,
                      linecol = "grey26",
                      labeltype = "custom",
                      verbose.labels = F,
                      custom.labels = c(""))
  par(new = F)
}
```

Although this worked it seems like 25 topics on this graph might be too much...

# Final Thoughts (TLDR)

## Final thoughts

-   I have found a way to use the date column with the different types of sentiment! I'm very pleased with this as it allowed me to compare the NRC dictionary categories as well over time.

-   I have decided to use co-occurrence without limiting the data set. This is because I want to see how the text plots compare to each other with all words included.

-   I am thinking of slightly tweaking my research question to: How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan* **over time**?

    -   This is because I think it's a much more interesting question to see how people felt about the show over the course of the show and the aftermath.

## Future Work

Here are a couple things I'd like to do in blog post 6:

-   I would like to see if I can use the polarity and dates with graphing the STM topics.

-   Decide how I want to use STM. I think the graph shows some very important topics that are relevant, however the graph is unable to show them all so I'm wondering if I should try K at 20.

# Final Code Moving Forward

I have decided to put this in an rscript since doing STM and K-means have made my laptop impossible to work with.
