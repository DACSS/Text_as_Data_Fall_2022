---
title: "Blog Post Four"
author: "Molly Hackbarth"
description: "Working with the data"
date: "10/29/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw5
  - Molly Hackbarth
---

```{r}
#| label: setup
#| warning: false

library(tidyverse)
library(cld3)
library(dplyr)
library(here)
library(devtools)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.dictionaries)
library(quanteda.sentiment)
#new packages
library(patchwork)

knitr::opts_chunk$set(echo = TRUE)
```

# Research Question

**My current research question:** How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan*?

# Reading in the Data

```{r reading data}
#write csv has been commented out due to it continously trying to save an "updated version" in Git. 

reddit_data <- read.csv(here::here("posts", "_data", "loveisblindjapan.csv"))

twitter1 <- read.csv(here::here("posts", "_data", "tweets.csv"))

twitter2 <- read.csv(here::here("posts", "_data", "tweets#.csv"))

reddit <- subset(reddit_data, select = c("body", "created_utc")) 

reddit$created_utc <- as.Date.POSIXct(reddit$created_utc)

reddit <- reddit %>% 
  select(text = body, 
            date = created_utc)
# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])
reddit <- reddit %>% 
  filter(!text == '[deleted]') %>% 
  filter(!text == '[removed]')

#remove counting column
twitter1 <- twitter1 %>% select(!c(X, User))
twitter2 <- twitter2 %>% select(!c(X, User))

twitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)
#write.csv(twitter, here::here("posts", "_data", "twitter.csv") , all(T) )

names(twitter) <- tolower(names(twitter))
twitter <- twitter %>% 
  rename_at('tweet', ~ 'text', 
            'Date' ~ 'date')
twitter$date <- as.Date(strftime(twitter$date, format="%Y-%m-%d"))

# remove duplicate tweets
twitter <- twitter %>% distinct(text, date, .keep_all = TRUE)

#check for duplicate tweets
twitter %in% unique(twitter[ duplicated(twitter)]) 

```

```{r lemmentizing the data}
# Twiter Lemmitized
twitter_corpus <- subset(twitter, detect_language(twitter) == "en")
twitter_corpus <- corpus(twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]
twittersummary <- summary(twitter_corpus)
twitter_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", twitter_corpus))

mystopwords <- c("love is blind japan", "#loveisbindjapan", "#LoveIsBlindJapan","Love Is Blind Japan","Love is Blind Japan", "Love Is Blind: Japan", "#loveisblind", "ラブイズブラインドjapan", "#ラブイズブラインドjapan", "loveisblind", "#loveisblind2", "blind:japan", "blind", "show")

twitter_corpus_tokens <- tokens(twitter_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

twitter_lemmitized <- tokens_replace(twitter_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

# Reddit Lemmitized

reddit_corpus <- subset(reddit, detect_language(reddit) == "en")
reddit_corpus <- corpus(reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]
redditsummary <- summary(reddit_corpus)

reddit_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", reddit_corpus))

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T, 
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

```

```{r creating nrc dictionary}
#Twitter NRC

twitterDfm_nrc <- dfm(tokens(twitter_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

twitter_corpus_dfm <- twitter_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

# Reddit NRC

redditDfm_nrc <- dfm(tokens(reddit_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

reddit_corpus_dfm <- reddit_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

```

# Trying to use Dates with Average Sentiments

Something I would like to try to do is create a graph using the current sentiment from NRC and the dates these were published. In order to do this I'll first try using [this idea](https://stackoverflow.com/questions/58918872/performing-time-series-analysis-of-quanteda-tokens/58938067#58938067).

```{r a new data frame}
dfm <- dfm(twitter_lemmitized)

dfm_sub <- dfm_keep(dfm, 
                    pattern = c("love", "japan"),
                    valuetype = "fixed", 
                    case_insensitive = TRUE)

df <- convert(dfm_sub, "data.frame")
df$date <- dfm@docvars$date

df %>% 
  pivot_longer(love:japan, names_to = "word") %>% 
  ggplot(aes(x = date, y = word)) +
  geom_line()
```

Here we can see that this may work! I would like to test this with the "tdf_nrc"

```{r trying with tdf nrc}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

tdf_nrc %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()


```

After playing around I was able to add the date column to my nrc dictionary! As you can see though it's a bit of a mess. This time I'm going to try and group by month.

```{r testing summarized version}
test <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))


test %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()
```

Here is what I was thinking about! As you can see I used the mean to find the average polarity by date. Overall we're able to see that most of the time the show was trending towards positive with a few large dips throughout.

Let's set this up properly for reddit and twitter.

```{r testing dates}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$date <- redditDfm_nrc@docvars$date
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

twitter_by_date <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))

twitter_by_date %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()

reddit_by_date <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity))

reddit_by_date %>% 
  ggplot(aes(x = date, y = polarity)) +
  geom_line()

```

Here we're able to see both graphs average sentiment over time! This is very useful to me as it allows me to compare. Now lets compare these

```{r compare average sentiment}

ggplot() +
geom_line(data = reddit_by_date, aes(x = date, y = polarity, color="Reddit")) + geom_line(data = twitter_by_date, aes(x = date, y = polarity, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Dictionary Sentiment")

```

This looks fairly good! However this may be too much detail to just look at overall sentiment. Below you will see a smoothed version.

```{r smoothed twitter vs reddit analysis}

twitter_reddit_sentiment <- ggplot() +
geom_smooth(data = reddit_by_date, aes(x = date, y = polarity, color="Reddit")) + geom_smooth(data = twitter_by_date, aes(x = date, y = polarity, color = "Twitter")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit', 'Twitter'),
                  values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Dictionary Sentiment")

twitter_reddit_sentiment

```

This is a lot easier to read! Here you can also see something interesting. Although Twitter seems to have started before 2022, Reddit data starts somewhere in late January. This is likely because Twitter had prior notice and had been tweeting about curiosity by what the show might be about. Likely Reddit did not make a sub reddit until after the show had started.

We can also see additionally that Twitter has overall had a more positive sentiment to the show than Reddit, however they are very close.

# Emotional Rating based on Polarity

Below you will find me trying to test all sentiments over time.

```{r twitter and reddit emotions}

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  pivot_longer(anger:trust, names_to = "words")

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  pivot_longer(anger:trust, names_to = "words")
  

ggplot() +
geom_point(data = reddit_emotions, aes(x = date, y = value, color = words)) + 
  labs(title="Reddit Emotional Count")

ggplot() + 
  geom_col(data = twitter_emotions, aes(x = date, y = value, color = words), position = "dodge") + labs(title="Twitter Emotional Count")
```

When I look at this I find it fairly difficult to understand. I had thought putting them all in one column to compare against each other may be best. However now I'm wondering if it would make more sense to not have them combined. This has lead me to my other thought, comparing Twitter and Reddit emotional word count on the same graph one by one.

Since Reddit and Twitter are not the same amount of text I will go ahead and use the mean for each emotion.

```{r one by one count}

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = mean(anger),
           anticipation = mean(anticipation), 
           disgust = mean(disgust),
           fear = mean(fear),
           joy = mean(joy),
           negative = mean(negative),
           positive = mean(positive),
           sadness = mean(sadness),
           surprise = mean(surprise),
           trust = mean(trust))

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = mean(anger),
           anticipation = mean(anticipation), 
           disgust = mean(disgust),
           fear = mean(fear),
           joy = mean(joy),
           negative = mean(negative),
           positive = mean(positive),
           sadness = mean(sadness),
           surprise = mean(surprise),
           trust = mean(trust))

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anger, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anger, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average anger by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = anticipation, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = anticipation, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average anticipation by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = disgust, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = disgust, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average disgust by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = fear, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = fear, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average fear by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = joy, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = joy, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average joy by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = sadness, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = sadness, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average sadness by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = surprise, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = surprise, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average surprise by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = trust, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = trust, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average trust by date")


ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = negative, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = negative, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average negativeness by date")

ggplot() +
geom_line(data = reddit_emotions, aes(x = date, y = positive, color="Reddit")) + geom_line(data = twitter_emotions, aes(x = date, y = positive, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average positveness by date")



```

# Co-occurrence

After looking at co-occurrence I think I may stick to just using the textplot version of it. Since I've broken out the sample size into two data sets I'd like to see how the words connect without limitations for each database rather than trying to limit what's there. Here I'll keep the top 30 features of both Twitter and Reddit.

```{r reddit and twitter co-occurrence}

## Reddit

# create fcm from dfm
rsmaller_fcm <- fcm(reddit_corpus_dfm)

rmyFeatures <- names(topfeatures(rsmaller_fcm, 30))

# retain only those top features as part of our matrix
reven_smaller_fcm <- fcm_select(rsmaller_fcm, pattern = rmyFeatures, selection = "keep")

# compute size weight for vertices in network
rsize <- log(colSums(reven_smaller_fcm))

# create plot
textplot_network(reven_smaller_fcm, vertex_size = rsize / max(rsize) * 3)

## TWITTER

# create fcm from dfm
smaller_fcm <- fcm(twitter_corpus_dfm)

myFeatures <- names(topfeatures(smaller_fcm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

Something I notice right away is that Reddit connections are more linked towards the contestants names on the show. However for Twitter it's more connected to what people felt about the show.

# Final Thoughts (TLDR)

## Final thoughts

-   I have found a way to keep the date column in the summary but remove it from the actual tokens afterwards.

-   I have made a list of stop words that are related to the title of the show from Twitter.

    -   I have also included the removal of the word "show" as for tweets it only referenced show as a TV series. However for Reddit I have not removed the word show as Redditors seemed to use the word show outside of of meaning a TV show.

    -   Reddit also did not have the same issues with the title of the show due to all of the posts in the sub reddit being related to Love is Blind Japan.

-   I have decided to use the NRC dictionary.

-   I am unsure how I will use co-occurrence.

## Future Work

Here are a few things I'd like to do in blog post 5:

-   I would like to look at co-occurrence more closely and decide if the code I have now is the one I would like to use for the final project. I currently have chosen it based off prior issues (such as having the show's name showing up).

    -   This may include using a maximum document frequency and a minimum word frequency.

-   I would like to see if I can create an emotional rating comparison using the dictionary method. This would include using the NRC for twitter and reddit to compare the average emotional response.

# Final Code Moving Forward

```{r reddit word cloud}
reddit_corpus <- subset(reddit, detect_language(reddit) == "en")
reddit_corpus <- corpus(reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]
redditsummary <- summary(reddit_corpus)

reddit_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", reddit_corpus))

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T, 
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

library(quanteda.textplots)

reddit_corpus_dfm <- reddit_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

textplot_wordcloud(reddit_corpus_dfm, max_words=200, color="red")
```

```{r twitter word cloud}

twitter_corpus <- subset(twitter, detect_language(twitter) == "en")
twitter_corpus <- corpus(twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]
twittersummary <- summary(twitter_corpus)
twitter_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", twitter_corpus))

mystopwords <- c("love is blind japan", "#loveisbindjapan", "#LoveIsBlindJapan","Love Is Blind Japan","Love is Blind Japan", "Love Is Blind: Japan", "#loveisblind", "ラブイズブラインドjapan", "#ラブイズブラインドjapan", "loveisblind", "#loveisblind2", "blind:japan", "blind", "show")

twitter_corpus_tokens <- tokens(twitter_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

twitter_lemmitized <- tokens_replace(twitter_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

library(quanteda.textplots)
twitter_corpus_dfm <- twitter_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

textplot_wordcloud(twitter_corpus_dfm, max_words=200, color="blue")
```

```{r twitter cooccurrence}

# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters
smaller_dfm <- dfm_trim(twitter_corpus_dfm, max_termfreq = 3400, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, max_docfreq = .3, docfreq_type = "prop")

textplot_wordcloud(smaller_dfm, min_count = 100,
                   random_order = FALSE)

# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)

myFeatures <- names(topfeatures(smaller_fcm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_smaller_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

```{r reddit cooccurrence}

# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters
rsmaller_dfm <- dfm_trim(reddit_corpus_dfm, max_termfreq = 3400, min_termfreq = 10)
rsmaller_dfm <- dfm_trim(rsmaller_dfm, max_docfreq = .3, docfreq_type = "prop")

textplot_wordcloud(rsmaller_dfm, min_count = 100,
                   random_order = FALSE)

# create fcm from dfm
rsmaller_fcm <- fcm(rsmaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(rsmaller_fcm)

rmyFeatures <- names(topfeatures(rsmaller_fcm, 30))

# retain only those top features as part of our matrix
reven_smaller_fcm <- fcm_select(rsmaller_fcm, pattern = rmyFeatures, selection = "keep")

# check dimensions
dim(reven_smaller_fcm)

# compute size weight for vertices in network
rsize <- log(colSums(reven_smaller_fcm))

# create plot
textplot_network(reven_smaller_fcm, vertex_size = rsize / max(rsize) * 3)
```

```{r reddit dfm dictionary}

redditDfm_nrc <- dfm(tokens(reddit_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

dim(redditDfm_nrc)
redditDfm_nrc

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

ggplot(rdf_nrc) + 
  geom_histogram(aes(x=polarity)) + 
  theme_bw()

```

```{r twitter dfm dictionary}
twitterDfm_nrc <- dfm(tokens(twitter_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

dim(twitterDfm_nrc)
twitterDfm_nrc

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

ggplot(tdf_nrc) + 
  geom_histogram(aes(x=polarity)) + 
  theme_bw()

```
