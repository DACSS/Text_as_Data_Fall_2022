---
title: "Blog Post 2 "
author: "Mekhala Kumar"
description: "Reading in Data and Preprocessing"
date: "11/15/2022"
format:
  html:
    toc: true
    code-copy: true
    code-tools: true
categories:
  - MekhalaKumar
  - Olympics2020
  - GenderandSports
  - BlogPost2
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(quanteda)
#devtools::install_github("quanteda/readtext") 
library(readtext)
library(striprtf)
#library(LexisNexisTools)
library(corpus)
library(quanteda.textplots)
```

# About the Data and Collection Process

I will be looking into how women's and men's sports were described, specifically in the context of the Tokyo Olympics 2020. The data used for this project is a collection of Indian newspaper archives from LexisNexis, from July 22,2021 to August 9, 2021 (as the Olympics were held between 23 July-8 August). I tried a few ways to collect the articles. At first, I tried scraping the Deccan Herald website and then tried scraping the LexisNexis archive. However, I was unable to do so. Hence, I downloaded articles from LexisNexis. The articles were collected after filtering the database. These filters were: Jul 22,2021 to Aug 09,2021, Asia, India, Men's Sports/Women's Sports/Sports Awards, Newspapers/web-based publications, a set of newspapers which were: Hindustan Times, Times of India (Electronic Edition), Free Press Journal (India), The Telegraph (India), Indian Express, Mint, DNA, India Today Online, The Hindu and Economic Times.

# Initial steps 
At first, I tried reading in the data for one file and looked for the different components present, in order to understand what I needed to extract. \
Since, it was causing errors to load the files, I have provided the code of how I read in the files and extracted information as comments.\
I have loaded in the data using an R object. 
```{r}
#my_texts <- readtext(paste0("D:/Text as Data/Files(100) Set 1.rtf"),text_field = "texts")
#testing <- as.character(my_texts)
#testing
#typeof(testing)
#str_length(testing)
```

# Components
The body of the article is the main text required. In order to extract this, I found the index value of where the body of the text started. Additionally, I collected the index values for the information of the newspaper name, date and classifications.

## 3 issues I noticed while doing this step were:
1. The titles of each article were not saved when the file was read in.
2. The dateline which included the location or date was not present for every article. 
3. I had thought of using the index position of nPublished, as it denoted the end of the body of the article. However, this was not present for all the articles. 
```{r}
#Components
#indices_name<-str_locate_all(testing, "\n\n\n\n\n\n")%>%unlist()
#indices_name

#indices_date<-str_locate_all(testing,"\nDateline")
#indices_date

#For the actual text
#indices_body<-str_locate_all(testing,"\nBody")%>%unlist()
#indices_body
#indices_body_end<-str_locate_all(testing,"\nPublished")%>%unlist()
#indices_body_end

#Subject and classifications 
#indices_classification<-str_locate_all(testing,"\nSubject")%>%unlist()
#indices_classification

#Document end
#indices_end<-str_locate_all(testing,"\nEnd of Document")%>%unlist()
#indices_end
```
# Creating the dataframe for 1 file
I tried creating a dataframe for the articles in 1 file and faced issues when I tried to separate information such as the few words before the body of the article starts (such as date and location). 
```{r}
#df_test <- data.frame(matrix(ncol = 3, nrow = 1191))%>% rename(
 #   Newspaper_Date=X1 ,
  #  Body=X2,
  #  Tags=X3
   # )
#Testing with a smaller set

#for (i in 1:3){
 # df_test[i,1]<-str_sub(testing,indices_name[i],indices_body[i])
 # df_test[i,2]<-str_sub(testing,indices_body[i],indices_body_end[i])
  #df_test[i,3]<-str_sub(testing,indices_classification[i],indices_end[i])

#}


#df_test<-df_test%>%separate(Newspaper_Date, into=c("Newspaper_Date", "Delete"), sep="Copyright")%>%select(-Delete)

#for (i in 1:100){
# df[i,1]<-str_sub(testing,indices_name[i],indices_body[i])
# df[i,2]<-str_sub(testing,indices_body[i],indices_classification[i])
# df[i,3]<-str_sub(testing,indices_classification[i],indices_end[i])
# }
#\nPublished not there everywhere so did not work
#for (i in 1:100){
 # df[i,1]<-str_sub(testing,indices_name[i],indices_body[i])
  #df[i,2]<-str_sub(testing,indices_body[i],indices_classification[i])
  #df[i,3]<-str_sub(testing,indices_classification[i],indices_end[i])
#}


```

# Reading all the files
This reads in the entire collection of files. The rest of the post works with the data from all the files. \
```{r}
#my_text1 <- readtext(paste0("Data/Files(100) Set 1.rtf"),text_field = "texts")
#my_text2 <- readtext(paste0("Data/Files(100) Set 2.rtf"),text_field = "texts")
#my_text3 <- readtext(paste0("Data/Files(100) Set 3.rtf"),text_field = "texts")
#my_text4 <- readtext(paste0("Data/Files(100) Set 4.rtf"),text_field = "texts")
#my_text5 <- readtext(paste0("Data/Files(100) Set 5.rtf"),text_field = "texts")
#my_text6 <- readtext(paste0("Data/Files(100) Set 6.rtf"),text_field = "texts")
#my_text7 <- readtext(paste0("Data/Files(100) Set 7.rtf"),text_field = "texts")
#my_text8 <- readtext(paste0("Data/Files(100) Set 8.rtf"),text_field = "texts")
#my_text9 <- readtext(paste0("Data/Files(100) Set 9.rtf"),text_field = "texts")
#my_text10 <- readtext(paste0("Data/Files(100) Set 10.rtf"),text_field = "texts")
#my_text11 <- readtext(paste0("Data/Files(100) Set 11.rtf"),text_field = "texts")
#my_text12 <- readtext(paste0("Data/Files(91) Set 12.rtf"),text_field = "texts")
#files<-c(my_text1,my_text2,my_text3,my_text4,my_text5,my_text6,my_text7,my_text8,my_text9,my_text10,my_text11,my_text12)
```


# Putting the information into a dataframe
Using a nested for loop, I collected the index positions for the different components of each newspaper article and created a dataframe to put in the data. For the last file, I made a separate for loop since it had fewer articles to be read in and put into a dataframe. 

## Some issues I faced here:
1. Before the beginning of the article, the word body as well as the location and date are often mentioned. However, there was no constant separator that could be used. 
2.  For articles 767-800, the newspaper and date were not read in. Due to this, I could not perform any operations on the column to remove unnecessary text. 
```{r}
#df1 <- data.frame(matrix(ncol = 3, nrow = 1100))%>% rename(
 #   Newspaper_Date=X1 ,
  #  Body=X2,
   # Tags=X3
   # )
#k=0
#for (i in seq(2,22,2)){
  #testing <- as.character(files[i])
  #indices_name<-str_locate_all(testing, "\n\n\n\n\n\n")%>%unlist()
  #indices_body<-str_locate_all(testing,"\nBody")%>%unlist()
  #indices_classification<-str_locate_all(testing,"\nSubject")%>%unlist()
  #indices_end<-str_locate_all(testing,"\nEnd of Document")%>%unlist()
  
 # print(k)
#  for(j in 1:100){
 #   df1[k+j,1]<-str_sub(testing,indices_name[j],indices_body[j])
  #  df1[k+j,2]<-str_sub(testing,indices_body[j],indices_classification[j])
  #  df1[k+j,3]<-str_sub(testing,indices_classification[j],indices_end[j])
  #}
  #k=k+100
  #}

#df2 <- data.frame(matrix(ncol = 3, nrow = 91))%>% rename(
 #   Newspaper_Date=X1 ,
  #  Body=X2,
   # Tags=X3
    #)
#testing <- as.character(files[24])
#indices_name<-str_locate_all(testing, "\n\n\n\n\n\n")%>%unlist()
#indices_body<-str_locate_all(testing,"\nBody")%>%unlist()
#indices_classification<-str_locate_all(testing,"\nSubject")%>%unlist()
#indices_end<-str_locate_all(testing,"\nEnd of Document")%>%unlist()


#for(l in 1:91){
 # df2[l,1]<-str_sub(testing,indices_name[l],indices_body[l])
  #df2[l,2]<-str_sub(testing,indices_body[l],indices_classification[l])
  #df2[l,3]<-str_sub(testing,indices_classification[l],indices_end[l])
#}

#df<-rbind(df1,df2)
#Does not work because the information is missing for some
#df<-df%>%separate(Newspaper_Date, into=c("Newspaper_Date", "Delete"), sep="Copyright")%>%select(-Delete)
#this does not work
#df<-df%>%separate(Newspaper_Date, into=c("Newspaper", "Date"), sep="{\n}")
#df<-df_test%>%separate(Newspaper_Date, into=c("Newspaper", "Date"), sep="[August|July]")
#df<-df%>%separate(Body,into=c("Delete","Body"),sep="--|Body")
#write.csv(df, "D:/Text as Data/Text_as_Data_Fall_2022/posts/output.csv", row.names=FALSE, quote=FALSE)
```

```{r}
df_All<-readRDS(file = "D:/Text as Data/All12Files.rds")
df<-df_All
#Does not work because the information is missing for some
df<-df%>%separate(Newspaper_Date, into=c("Newspaper_Date", "Delete"), sep="Copyright")%>%select(-Delete)
#this does not work
#df<-df%>%separate(Newspaper_Date, into=c("Newspaper", "Date"), sep="{\n}")
#df<-df_test%>%separate(Newspaper_Date, into=c("Newspaper", "Date"), sep="[August|July]")
#df<-df%>%separate(Body,into=c("Delete","Body"),sep="--|Body")
#write.csv(df, "D:/Text as Data/Text_as_Data_Fall_2022/posts/output.csv", row.names=FALSE, quote=FALSE)
```


# Creating the corpus and looking at the number of tokens in each document
```{r}
newspaper_corpus <- corpus(df,text_field = "Body")
newspaper_corpus_summary <- summary(newspaper_corpus)
head(newspaper_corpus_summary)

newspaper_corpus_summary$Tokens
ndoc(newspaper_corpus)
```

# Checking for metadata
There was metadata but too many pages were being printed so I commented out the line. 
```{r}
#docvars(newspaper_corpus)
```

# Creating tokens

as.character function prints too many pages so it was commented out. 
```{r}
newspaper_tokens <- tokens(newspaper_corpus)
head(newspaper_tokens)
#as.character(newspaper_tokens[1])
#sprintf(as.character(newspaper_tokens[1]))
```

# Removing punctuation
```{r}
newspaper_tokens <- tokens(newspaper_tokens ,
                                    remove_punct = T)
head(newspaper_tokens)
```

# Removing stopwords
```{r}
withoutstopwords_news<- tokens_select(newspaper_tokens, 
                    pattern = stopwords("en"),
                    select = "remove")
head(withoutstopwords_news)
#summary(withoutstopwords_news)
#as.character(withoutstopwords_news)

```

# Keyword in context
Since I want to look at women's and men's sports, I tried looking at the keyword-in-context for the words women and men. 
```{r}
kwic_women<- kwic(withoutstopwords_news,
                        pattern = c("women"))
kwic_men<-kwic(withoutstopwords_news,
                        pattern = c("men"))
head(kwic_women)
head(kwic_men)
```

# Creating a tokens object with bigrams and trigrams 

```{r}
news_ngrams <- tokens_ngrams(withoutstopwords_news, n=2:3)
head(news_ngrams)
tail(news_ngrams)
```

# Word Cloud
I created a word cloud to check if there are any words other than the stopwords to be removed.
The words- classification, publication-type and body should ideally be removed.
```{r}
# create the dfm
news_dfm <- dfm(tokens(withoutstopwords_news))

# find out a quick summary of the dfm
news_dfm
textplot_wordcloud(news_dfm, min_count = 50, random_order = FALSE)
```

