---
title: "Blog Post Six: Working with Time Series"
author: "Molly Hackbarth"
description: "Working with the data"
date: "12/9/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw6
  - Molly Hackbarth
---

```{r qwrap2}
library(qwraps2)
knitr::opts_chunk$set(cache = TRUE)
#qwraps2::lazyload_cache_dir(path = "sixth_post_cache/html5")
```

```{r libraries}
library(tidyverse)
library(cld3)
library(dplyr)
library(here)
library(devtools)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.dictionaries)
library(quanteda.sentiment)
library(patchwork)
library(stm)
library(tm)

## new package 
library(sentimentr)
knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
#load("blogpost6.RData")
```

# Research Question

**My current research question:** How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan* over time?

This blog post will be focused on completing the code that I want to use for my Final Poster. Anything that is new will be in the title as **New!**

# Reading in the Data

```{r reading data}
#write csv has been commented out due to it continously trying to save an "updated version" in Git. 

reddit_data <- read.csv(here::here("posts", "_data", "loveisblindjapan.csv"))

twitter1 <- read.csv(here::here("posts", "_data", "tweets.csv"))

twitter2 <- read.csv(here::here("posts", "_data", "tweets#.csv"))

reddit <- subset(reddit_data, select = c("body", "created_utc")) 

reddit$created_utc <- as.Date.POSIXct(reddit$created_utc)

reddit <- reddit %>% 
  select(text = body, 
            date = created_utc)
# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])
reddit <- reddit %>% 
  filter(!text == '[deleted]') %>% 
  filter(!text == '[removed]')

#remove counting column
twitter1 <- twitter1 %>% select(!c(X, User))
twitter2 <- twitter2 %>% select(!c(X, User))

twitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)
#write.csv(twitter, here::here("posts", "_data", "twitter.csv") , all(T) )

names(twitter) <- tolower(names(twitter))
twitter <- twitter %>% 
  rename_at('tweet', ~ 'text', 
            'Date' ~ 'date')
twitter$date <- as.Date(strftime(twitter$date, format="%Y-%m-%d"))

# remove duplicate tweets
twitter <- twitter %>% distinct(text, date, .keep_all = TRUE)

#check for duplicate tweets
twitter %in% unique(twitter[ duplicated(twitter)]) 

```

```{r lemmentizing the data}
# Twiter Lemmitized
twitter_corpus <- subset(twitter, detect_language(twitter) == "en")
twitter_corpus <- corpus(twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]
twittersummary <- summary(twitter_corpus)
twitter_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", twitter_corpus))

mystopwords <- c("love is blind japan", "#loveisbindjapan", "#LoveIsBlindJapan","Love Is Blind Japan","Love is Blind Japan", "Love Is Blind: Japan", "#loveisblind", "ラブイズブラインドjapan", "#ラブイズブラインドjapan", "loveisblind", "#loveisblind2", "blind:japan", "blind", "show")

twitter_corpus_tokens <- tokens(twitter_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

twitter_lemmitized <- tokens_replace(twitter_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

# Reddit Lemmitized

reddit_corpus <- subset(reddit, detect_language(reddit) == "en")
reddit_corpus <- corpus(reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]
redditsummary <- summary(reddit_corpus)

reddit_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", reddit_corpus))

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T, 
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

```

```{r creating nrc dictionary}
#Twitter NRC

twitterDfm_nrc <- dfm(tokens(twitter_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

twitter_corpus_dfm <- twitter_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

# Reddit NRC

redditDfm_nrc <- dfm(tokens(reddit_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

reddit_corpus_dfm <- reddit_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

```

# Sentiment by Dates

```{r emotions by percent}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$date <- redditDfm_nrc@docvars$date
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
    rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3) * 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3)* 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)
  

```

```{r filter count}

twitter_emotions <- twitter_emotions %>% 
  filter(word_count > 100)

reddit_emotions <- reddit_emotions %>% 
  filter(word_count > 100)

anger <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anger_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anger_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anger percentage by date")

anticipation <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anticipation_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anticipation_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anticipation percentage by date")

disgust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = disgust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = disgust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit disgust percentage by date")

fear <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = fear_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = fear_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit fear percentage by date")

joy <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = joy_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = joy_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit joy percentage by date")

sadness <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = sadness_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = sadness_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit sadness percentage by date")

surprise <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = surprise_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = surprise_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit surprise percentage by date")

trust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = trust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = trust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit trust percentage by date")


negative <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = negative_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = negative_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit negativeness percentage by date")

positive <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = positive_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = positive_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit positveness percentage by date")

anger / disgust 
  
fear / sadness 

anticipation / joy 

surprise / trust

positive/negative


twitter_reddit_sentiment <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit', 'Twitter'),
                  values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit Average Dictionary Sentiment")

twitter_reddit_sentiment

```

Since we have it in a line graph, lets go ahead and try to do a comparison in a bar chart for all emotions together.

## Graphs New!

```{r bar chart graph}

twitter_emotions_pivot <- twitter_emotions %>% 
  pivot_longer(c(anger, disgust, fear, sadness, anticipation, joy, surprise, trust), names_to = "words")

t_emotions <- ggplot() + 
  geom_bar(data = twitter_emotions_pivot, aes(x = words, weight = value, fill = words)) + ggtitle("Twitter sentiments")

reddit_emotions_pivot <- reddit_emotions %>% 
  pivot_longer(c(anger, disgust, fear, sadness, anticipation, joy, surprise, trust), names_to = "words")

r_emotions <- ggplot() + 
  geom_bar(data = reddit_emotions_pivot, aes(x = words, weight = value, fill = words)) + ggtitle("Reddit sentiments")

t_emotions 
r_emotions

twitter_emotions_pivot_2 <- twitter_emotions %>% 
  pivot_longer(c(positive, negative), names_to = "words")

t_np <- ggplot() + 
  geom_bar(data = twitter_emotions_pivot_2, aes(x = words, weight = value, fill = words)) + ggtitle("Twitter sentiments")

reddit_emotions_pivot_2 <- reddit_emotions %>% 
  pivot_longer(c(positive, negative), names_to = "words")

r_np <- ggplot() + 
  geom_bar(data = reddit_emotions_pivot_2, aes(x = words, weight = value, fill = words)) + ggtitle("Reddit sentiments")

t_np

r_np
```

Overall we're able to see that for Twitter the top three sentiments are: anticipation, joy and trust. For Reddit the top three sentiments are: trust, anticipation, and joy. This is interesting as it seems Reddit and Twitter share similar feelings about the show from a pure word count standpoint.

Additionally we can see there is a much higher word count for positive words for Reddit and Twitter.

## Sentiment New!

I also want to try another way of doing sentiment analysis. This is a packaged called sentimentR

```{r twitter sentiment r}
# create named array of equal lengths
twitter_sen <- sapply(twitter_lemmitized, '[', seq(max(lengths(twitter_lemmitized))))


tout <- twitter_sen %>%
   as_tibble()  %>%
   pivot_longer(cols = everything(), names_to = "text", values_to = "tokens") %>% 
  arrange(text) %>% 
  group_by(text) %>% 
summarise(tokens = paste(tokens, collapse = " "))

tout$date <- tdf_nrc$date

tout <- tout %>% filter(!is.na(tokens)) 

tout$tokens <- gsub('NA', '', tout$tokens)

twitter_sent <- tout %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral")))  %>% 
  filter(word_count > 10)
          

twitter_sent$tokens %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() %>% 
  view()

twitter_sent$tokens %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))




```

```{r new sentinment reddit r}

# create named array of equal lengths

reddit_lemmitized$date <- reddit_lemmitized$date

reddit_sen <- sapply(reddit_lemmitized, '[', seq(max(lengths(reddit_lemmitized))))

rout <- reddit_sen %>%
   as_tibble()  %>%
   pivot_longer(cols = everything(), names_to = "text", values_to = "tokens") %>%
   arrange(text) %>% 
  group_by(text) %>% 
summarise(tokens = paste(tokens, collapse = " "))

rout$date <- rdf_nrc$date

rout <- rout %>% filter(!is.na(tokens))  # remove NA values of each text

rout$tokens <- gsub('NA', '', rout$tokens)
 
reddit_sent <- rout %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral"))) %>% 
 filter(word_count > 10)
          
reddit_sent$tokens %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() 
  


reddit_sent$tokens %>% 
  get_sentences() %>% 
  sentiment_by() %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))



```

Here we're able to see what the tokens cleaned up into sentences look like to sentimentr. I had tried it originally with the full database (no changing of the words at all) and it was interesting to see the sentence_id actually count the amount of sentences! What I like about this one is it shows you the positive, negative, and neutral by highlighting it and popping up. It allows you to read the sentences to see if you agree with them or not.

Interestingly the overall sentiment according to this package is a bit positive but also fairly neutral by the density.

Unfortunately for word count in a sentence I couldn't really filter out too many due to Twitter having a character limit. To deal with this I made a filter of at least 10 words.

Below you will find the non-edited data.

```{r reddit full}

reddit_full <- reddit %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral"))) %>% 
  filter(word_count > 10)
          
reddit_full$text %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() 
  


reddit_full$text %>% 
  get_sentences() %>% 
  sentiment_by() %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))


```

```{r twitter full}

twitter_full <- twitter %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral")))  %>% 
  filter(word_count > 10)
          

twitter_full$text %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() %>% 
  view()

twitter_full$text %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))

```

```{r comparision}

test <- twitter_full %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test2 <- reddit_full %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test3 <- twitter_sent %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test4 <- reddit_sent %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

         ggplot() +
geom_smooth(data = test, aes(x = date, y = sentiment, color="Twitter")) + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average Sentiment of Uncleaned Data")
         
         ggplot() +
geom_smooth(data = test3, aes(x = date, y = sentiment, color="Twitter")) + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average Sentiment of Tokenized Data")         
         
 
 ggplot() + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit Tokenized")) + geom_smooth(data = test3, aes(x = date, y = sentiment, color = "Twitter Tokenized")) + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit Uncleaned Data")) + geom_smooth(data = test, aes(x = date, y = sentiment, color = "Twitter Uncleaned Data")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit Tokenized', 'Twitter Tokenized', 'Reddit Uncleaned Data', 'Twitter Uncleaned Data'),
                  values=c('Reddit Tokenized'='Red', 'Twitter Tokenized'='blue', 'Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = 'cadetblue1')) +
  labs(title="Twitter vs Reddit Average Sentiment")
                 
         
 ggplot() + geom_density(data = test4, aes(sentiment, color = "Reddit Tokenized")) + geom_density(data = test3, aes(sentiment, color = "Twitter Tokenized")) + geom_density(data = test2, aes(sentiment, color = "Reddit Uncleaned Data")) + geom_density(data = test, aes(sentiment, color = "Twitter Uncleaned Data")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit Tokenized', 'Twitter Tokenized', 'Reddit Uncleaned Data', 'Twitter Uncleaned Data'),
                  values=c('Reddit Tokenized'='Red', 'Twitter Tokenized'='blue', 'Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = 'cadetblue1')) +
  labs(title="Twitter vs Reddit Average Sentiment")

```

# Word Cloud

```{r}
textplot_wordcloud(twitter_corpus_dfm, max_words=200, color="blue")

textplot_wordcloud(reddit_corpus_dfm, max_words=200, color="red")
```

# STM New!

Below I will have the STM for both reddit and twitter. This time I have added two prevalance items, polarity and dates.

```{r stm reddit}
k <- 25
rModel <- dfm(reddit_lemmitized)

docvars(reddit_lemmitized) <- rModel

rModel$polarity <- rdf_nrc$polarity

rModel <- stm(rModel,
                K = k,
              prevalence = ~ polarity + date,
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

#labelTopics(rModel)
plot(rModel, type = "summary")

# get the words
rTopicNames <- labelTopics(rModel, n=4)$frex

# set up an empty vector
rTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
  rTopicLabels[i] <- paste(rTopicNames[i,], collapse = "_")
}

# print the names
rTopicLabels



```

```{r stm twitter}
k <- 25
### TWITTER
tModel <- dfm(twitter_lemmitized)

docvars(twitter_lemmitized) <- tModel

tModel$polarity <- tdf_nrc$polarity

tModel <- stm(tModel,
                K = k,
              prevalence = ~ polarity + date,
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

labelTopics(tModel)
plot(tModel, type = "summary")

# get the words
tTopicNames <- labelTopics(tModel, n=4)$frex

# set up an empty vector
tTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
  tTopicLabels[i] <- paste(tTopicNames[i,], collapse = "_")
}

# print the names
tTopicLabels
```

Here I was able to see that even with two factors (polarity and dates) it seems that the topics stayed fairly similar.

# Final Thoughts (TLDR)

-   Throughout my various blog post I feel like I was able to craft multiple graphs that I want to use in my Final Project.

-   It took a lot longer to figure out how to do sentiment by dates but it feels well worth it!

-   I think looking back it might have been a better idea to combine the Twitter and Reddit data together and done an analysis on that. This may have caused less trouble in rendering the blog and let me explore a bit more in depth.

```{r save image}
save.image(file = "blogpost6.RData")
```
