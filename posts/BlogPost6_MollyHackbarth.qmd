---
title: "Blog Post Six: Working Towards the Final"
author: "Molly Hackbarth"
description: "Working with the data"
date: "12/9/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw6
  - Molly Hackbarth
---

```{r libraries}
library(tidyverse)
library(cld3)
library(dplyr)
library(here)
library(devtools)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.dictionaries)
library(quanteda.sentiment)
library(patchwork)
library(stm)
library(tm)

## new package 
library(sentimentr)
library(text2vec)
library(LDAvis)
knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
#load("blogpost6.RData")
```

# Research Question

**My current research question:** How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan* over time?

This blog post will be focused on completing the code that I want to use for my Final Poster. Anything that is new will be in the title as **New!**

# Reading in the Data

```{r reading data}
#write csv has been commented out due to it continously trying to save an "updated version" in Git. 

reddit_data <- read.csv(here::here("posts", "_data", "loveisblindjapan.csv"))

twitter1 <- read.csv(here::here("posts", "_data", "tweets.csv"))

twitter2 <- read.csv(here::here("posts", "_data", "tweets#.csv"))

reddit <- subset(reddit_data, select = c("body", "created_utc")) 

reddit$created_utc <- as.Date.POSIXct(reddit$created_utc)

reddit <- reddit %>% 
  select(text = body, 
            date = created_utc)
# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])
reddit <- reddit %>% 
  filter(!text == '[deleted]') %>% 
  filter(!text == '[removed]')

#remove counting column
twitter1 <- twitter1 %>% select(!c(X, User))
twitter2 <- twitter2 %>% select(!c(X, User))

twitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)
#write.csv(twitter, here::here("posts", "_data", "twitter.csv") , all(T) )

names(twitter) <- tolower(names(twitter))
twitter <- twitter %>% 
  rename_at('tweet', ~ 'text', 
            'Date' ~ 'date')
twitter$date <- as.Date(strftime(twitter$date, format="%Y-%m-%d"))

# remove duplicate tweets
twitter <- twitter %>% distinct(text, date, .keep_all = TRUE)

#check for duplicate tweets
twitter %in% unique(twitter[ duplicated(twitter)]) 

```

```{r lemmentizing the data}
# Twiter Lemmitized
twitter_corpus <- subset(twitter, detect_language(twitter) == "en")
twitter_corpus <- corpus(twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]
twittersummary <- summary(twitter_corpus)
twitter_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", twitter_corpus))

mystopwords <- c("love is blind japan", "#loveisbindjapan", "#LoveIsBlindJapan","Love Is Blind Japan","Love is Blind Japan", "Love Is Blind: Japan", "#loveisblind", "ラブイズブラインドjapan", "#ラブイズブラインドjapan", "loveisblind", "#loveisblind2", "blind:japan", "blind", "show")

twitter_corpus_tokens <- tokens(twitter_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

twitter_lemmitized <- tokens_replace(twitter_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

# Reddit Lemmitized

reddit_corpus <- subset(reddit, detect_language(reddit) == "en")
reddit_corpus <- corpus(reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]
redditsummary <- summary(reddit_corpus)

reddit_corpus <- trimws(gsub("[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}", "", reddit_corpus))

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T, 
    remove_symbols = T,
    remove_url = T) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

```

```{r creating nrc dictionary}
#Twitter NRC

twitterDfm_nrc <- dfm(tokens(twitter_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

twitter_corpus_dfm <- twitter_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

# Reddit NRC

redditDfm_nrc <- dfm(tokens(reddit_lemmitized,
                              remove_punct = TRUE),
                       tolower = TRUE) %>%
                    dfm_lookup(data_dictionary_NRC)

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

reddit_corpus_dfm <- reddit_lemmitized %>% 
  dfm() %>% 
  dfm_remove(stopwords('english')) %>% 
  dfm_trim(min_termfreq = 30, verbose = FALSE)

```

# Sentiment by Dates

```{r emotions by percent}

tdf_nrc <- convert(twitterDfm_nrc, to = "data.frame")
tdf_nrc$date <- twitterDfm_nrc@docvars$date
tdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)
tdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0

tdf_nrc <- tdf_nrc %>% arrange((date)) %>% rowid_to_column(var='count')

rdf_nrc <- convert(redditDfm_nrc, to = "data.frame")
rdf_nrc$date <- redditDfm_nrc@docvars$date
rdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)
rdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0

rdf_nrc <- rdf_nrc %>% arrange((date)) %>% rowid_to_column(var='count')

reddit_emotions <- rdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
    rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3) * 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)

twitter_emotions <- tdf_nrc %>% 
  group_by(date) %>% 
 summarise(polarity = mean(polarity),
           anger = sum(anger),
           anticipation = sum(anticipation), 
           disgust = sum(disgust),
           fear = sum(fear),
           joy = sum(joy),
           negative = sum(negative),
           positive = sum(positive),
           sadness = sum(sadness),
           surprise = sum(surprise),
           trust = sum(trust)) %>% 
  rowwise() %>%
  mutate(word_count = sum(c_across(anger:trust), na.rm = TRUE)) %>% 
   mutate(anger_percent = round(anger / sum(word_count), 3)* 100,
          anticipation_percent = round(anticipation / sum(word_count), 3) * 100,
          disgust_percent = round(disgust/ sum(word_count), 3) * 100,
          fear_percent = round(fear / sum(word_count), 3) * 100,
          joy_percent = round(joy / sum(word_count), 3) * 100,
          negative_percent = round(negative / sum(word_count), 3) * 100,
          positive_percent = round(positive / sum(word_count), 3) * 100,
          sadness_percent = round(sadness / sum(word_count), 3) * 100,
          surprise_percent = round(surprise / sum(word_count), 3) * 100,
          trust_percent = round(trust / sum(word_count), 3) * 100)
  

```

```{r filter count}

twitter_emotions <- twitter_emotions %>% 
  filter(word_count > 100)

reddit_emotions <- reddit_emotions %>% 
  filter(word_count > 100)

anger <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anger_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anger_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anger percentage by date") + ylab("anger percent")

anticipation <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anticipation_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anticipation_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit anticipation percentage by date") + ylab("anticipation percent")

disgust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = disgust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = disgust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit disgust percentage by date") + ylab("disgust percent")

fear <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = fear_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = fear_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit fear percentage by date") + ylab("fear percent")

joy <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = joy_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = joy_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit joy percentage by date") + ylab("joy percent")

sadness <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = sadness_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = sadness_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit sadness percentage by date") + ylab("sadness percent")

surprise <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = surprise_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = surprise_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit surprise percentage by date") + ylab("surprise percent")

trust <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = trust_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = trust_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit trust percentage by date") + ylab("trust percent")


negative <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = negative_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = negative_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit negativeness percentage by date") + ylab("negative percent")

positive <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = positive_percent, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = positive_percent, color = "Twitter")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit positveness percentage by date") + ylab("positive percent")

anger 

disgust 
  
fear

sadness 

anticipation

joy 

surprise

trust

positive

negative


twitter_reddit_sentiment <- ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit', 'Twitter'),
                  values=c('Reddit'='red', 'Twitter'='blue')) +
  labs(title="Twitter vs Reddit average dictionary sentiment") + ylab("sentiment")

twitter_reddit_sentiment

```

Since we have it in a line graph, lets go ahead and try to do a comparison in a bar chart for all emotions together.

## Graphs New!

```{r bar chart graph}

twitter_emotions_pivot <- twitter_emotions %>% 
  pivot_longer(c(anger, disgust, fear, sadness, anticipation, joy, surprise, trust), names_to = "words")

t_emotions <- ggplot() + 
  geom_bar(data = twitter_emotions_pivot, aes(x = words, weight = value, fill = words)) + ggtitle("Twitter sentiments")

reddit_emotions_pivot <- reddit_emotions %>% 
  pivot_longer(c(anger, disgust, fear, sadness, anticipation, joy, surprise, trust), names_to = "words")

r_emotions <- ggplot() + 
  geom_bar(data = reddit_emotions_pivot, aes(x = words, weight = value, fill = words)) + ggtitle("Reddit sentiments")

t_emotions 
r_emotions

twitter_emotions_pivot_2 <- twitter_emotions %>% 
  pivot_longer(c(positive, negative), names_to = "words")

t_np <- ggplot() + 
  geom_bar(data = twitter_emotions_pivot_2, aes(x = words, weight = value, fill = words)) + ggtitle("Twitter sentiments")

reddit_emotions_pivot_2 <- reddit_emotions %>% 
  pivot_longer(c(positive, negative), names_to = "words")

r_np <- ggplot() + 
  geom_bar(data = reddit_emotions_pivot_2, aes(x = words, weight = value, fill = words)) + ggtitle("Reddit sentiments")

t_np

r_np
```

Overall we're able to see that for Twitter the top three sentiments are: anticipation, joy and trust. For Reddit the top three sentiments are: trust, anticipation, and joy. This is interesting as it seems Reddit and Twitter share similar feelings about the show from a pure word count standpoint.

Additionally we can see there is a much higher word count for positive words for Reddit and Twitter.

```{r sentiments by social media}

ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = anger_percent, color = "anger")) + geom_smooth(data = reddit_emotions, aes(x = date, y = anticipation_percent, color = "anticipation")) + geom_smooth(data = reddit_emotions, aes(x = date, y = disgust_percent, color = "disgust")) + geom_smooth(data = reddit_emotions, aes(x = date, y = fear_percent, color = "fear")) + geom_smooth(data = reddit_emotions, aes(x = date, y = joy_percent, color = "joy")) + geom_smooth(data = reddit_emotions, aes(x = date, y = sadness_percent, color = "sadness")) + geom_smooth(data = reddit_emotions, aes(x = date, y = surprise_percent, color = "surprise")) + geom_smooth(data = reddit_emotions, aes(x = date, y = trust_percent, color = "trust")) + 
    scale_color_manual(name='Sentiments',
                     breaks=c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'),
                     values=c('anger'='#4E79A7', 'anticipation'='#F28E2B', 'disgust' = '#E15759', 'fear' = '#76B7B2', 'joy' = '#59A14F', 'sadness' = '#EDC948', 'surprise' = '#B07AA1', 'trust' = '#FF9DA7')) +
  labs(title="Reddit sentiment percentages by date") + ylab("percent")

ggplot() +
geom_smooth(data = twitter_emotions, aes(x = date, y = anger_percent, color = "anger")) + geom_smooth(data = twitter_emotions, aes(x = date, y = anticipation_percent, color = "anticipation")) + geom_smooth(data = twitter_emotions, aes(x = date, y = disgust_percent, color = "disgust")) + geom_smooth(data = twitter_emotions, aes(x = date, y = fear_percent, color = "fear")) + geom_smooth(data = twitter_emotions, aes(x = date, y = joy_percent, color = "joy")) + geom_smooth(data = twitter_emotions, aes(x = date, y = sadness_percent, color = "sadness")) + geom_smooth(data = twitter_emotions, aes(x = date, y = surprise_percent, color = "surprise")) + geom_smooth(data = twitter_emotions, aes(x = date, y = trust_percent, color = "trust")) + 
    scale_color_manual(name='Sentiments',
                     breaks=c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'),
                     values=c('anger'='#4E79A7', 'anticipation'='#F28E2B', 'disgust' = '#E15759', 'fear' = '#76B7B2', 'joy' = '#59A14F', 'sadness' = '#EDC948', 'surprise' = '#B07AA1', 'trust' = '#FF9DA7')) +
  labs(title="Twitter sentiment percentages by date") + ylab("percent")


ggplot() +
geom_smooth(data = twitter_emotions, aes(x = date, y = positive_percent, color = "positive")) + geom_smooth(data = twitter_emotions, aes(x = date, y = negative_percent, color = "negative")) + 
    scale_color_manual(name='Sentiments',
                     breaks=c('positive', 'negative'),
                     values=c('positive'='#E28394', 'negative'='#77A2BB')) +
  labs(title="Twitter positive and negative sentiment percentages by date") + ylab("percent")

ggplot() +
geom_smooth(data = reddit_emotions, aes(x = date, y = positive_percent, color = "positive")) + geom_smooth(data = reddit_emotions, aes(x = date, y = negative_percent, color = "negative")) + 
    scale_color_manual(name='Sentiments',
                     breaks=c('positive', 'negative'),
                     values=c('positive'='#E28394', 'negative'='#77A2BB')) +
  labs(title="Reddit positive and negative sentiment percentages by date") + ylab("percent")

```

I have also added a few more graphs that I am unsure if I will use them in the final project. I think they're all very interesting, but would need to figure out what the best ones would be to include.

## Sentiment New!

I also want to try another way of doing sentiment analysis. This is a packaged called sentimentR

```{r twitter sentiment r}
# create named array of equal lengths
twitter_sen <- sapply(twitter_lemmitized, '[', seq(max(lengths(twitter_lemmitized))))


tout <- twitter_sen %>%
   as_tibble()  %>%
   pivot_longer(cols = everything(), names_to = "text", values_to = "tokens") %>% 
  arrange(text) %>% 
  group_by(text) %>% 
summarise(tokens = paste(tokens, collapse = " "))

tout$date <- tdf_nrc$date

tout <- tout %>% filter(!is.na(tokens)) 

tout$tokens <- gsub('NA', '', tout$tokens)

twitter_sent <- tout %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral")))  %>% 
  filter(word_count > 10)
          

twitter_sent$tokens %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() %>% 
  view()

twitter_sent$tokens %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))




```

```{r new sentinment reddit r}

# create named array of equal lengths

reddit_lemmitized$date <- reddit_lemmitized$date

reddit_sen <- sapply(reddit_lemmitized, '[', seq(max(lengths(reddit_lemmitized))))

rout <- reddit_sen %>%
   as_tibble()  %>%
   pivot_longer(cols = everything(), names_to = "text", values_to = "tokens") %>%
   arrange(text) %>% 
  group_by(text) %>% 
summarise(tokens = paste(tokens, collapse = " "))

rout$date <- rdf_nrc$date

rout <- rout %>% filter(!is.na(tokens))  # remove NA values of each text

rout$tokens <- gsub('NA', '', rout$tokens)
 
reddit_sent <- rout %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral"))) %>% 
 filter(word_count > 10)
          
reddit_sent$tokens %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() 
  


reddit_sent$tokens %>% 
  get_sentences() %>% 
  sentiment_by() %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))



```

Here we're able to see what the tokens cleaned up into sentences look like to sentimentr. I had tried it originally with the full database (no changing of the words at all) and it was interesting to see the sentence_id actually count the amount of sentences! What I like about this one is it shows you the positive, negative, and neutral by highlighting it and popping up. It allows you to read the sentences to see if you agree with them or not.

Interestingly the overall sentiment according to this package is a bit positive but also fairly neutral by the density.

Unfortunately for word count in a sentence I couldn't really filter out too many due to Twitter having a character limit. To deal with this I made a filter of at least 10 words.

Below you will find the non-edited data.

```{r reddit full}

reddit_full <- reddit %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral"))) %>% 
  filter(word_count > 10)
          
reddit_full$text %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() 
  


reddit_full$text %>% 
  get_sentences() %>% 
  sentiment_by() %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))


```

```{r twitter full}

twitter_full <- twitter %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral")))  %>% 
  filter(word_count > 10)
          

twitter_full$text %>% 
  get_sentences(by = NULL) %>% 
  sentiment_by() %>% #View()
  highlight() %>% 
  view()

twitter_full$text %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))

```

```{r comparision}

test <- twitter_full %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test2 <- reddit_full %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test3 <- twitter_sent %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

test4 <- reddit_sent %>% 
  group_by(date) %>% 
  summarise(sentiment = mean(sentiment),
         word_count = sum(word_count)) 

```

```{r testing graphs}

         ggplot() +
geom_smooth(data = test, aes(x = date, y = sentiment, color="Twitter")) + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='#EEA2AD', 'Twitter'='#00B2EE')) +
  labs(title="Twitter vs Reddit average sentiment of uncleaned data")
         
         ggplot() +
geom_smooth(data = test3, aes(x = date, y = sentiment, color="Twitter")) + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit")) +
    scale_color_manual(name='Social Medias',
                     breaks=c('Reddit', 'Twitter'),
                     values=c('Reddit'='#B22222', 'Twitter'='#8DEEEE')) +
  labs(title="Twitter vs Reddit average sentiment of tokens")         
         
 
 ggplot() + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit Tokens SentimentR")) + geom_smooth(data = test3, aes(x = date, y = sentiment, color = "Twitter Tokens SentimentR")) + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit Uncleaned Data")) + geom_smooth(data = test, aes(x = date, y = sentiment, color = "Twitter Uncleaned Data")) + geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit Tokens NRC Dictionary")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter Tokens NRC Dictionary")) +
scale_color_manual(name='Social Medias',
                  c('Reddit Tokens SentimentR', 'Twitter Tokens SentimentR', 'Reddit Uncleaned Data', 'Twitter Uncleaned Data', 'Reddit Tokens NRC Dictionary','Twitter Tokenized NRC Dictionary'),
                  values=c('Reddit Tokens SentimentR'='#B22222', 'Twitter Tokens SentimentR'='#8DEEEE', 'Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = '#00B2EE', 'Reddit Tokens NRC Dictionary' = 'red', 'Twitter Tokens NRC Dictionary' = 'blue'))+
  labs(title="Twitter vs Reddit average sentiment")
 
 
  ggplot() + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit Tokens")) + geom_smooth(data = test3, aes(x = date, y = sentiment, color = "Twitter Tokens")) + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit Uncleaned Data")) + geom_smooth(data = test, aes(x = date, y = sentiment, color = "Twitter Uncleaned Data")) +
scale_color_manual(name='Social Medias',
                  c('Reddit Tokens', 'Twitter Tokens', 'Reddit Uncleaned Data', 'Twitter Uncleaned Data'),
                  values=c('Reddit Tokens'='#B22222', 'Twitter Tokens'='#8DEEEE', 'Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = '#00B2EE'))+
  labs(title="Twitter vs Reddit average sentiment")
 
 
  ggplot() + geom_smooth(data = test4, aes(x = date, y = sentiment, color = "Reddit Tokens SentimentR")) + geom_smooth(data = test3, aes(x = date, y = sentiment, color = "Twitter Tokens SentimentR")) + geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit Tokens NRC Dictionary")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter Tokens NRC Dictionary")) +
scale_color_manual(name='Social Medias',
                  c('Reddit Tokens SentimentR', 'Twitter Tokens SentimentR', 'Reddit Tokens NRC Dictionary','Twitter Tokens NRC Dictionary'),
                  values=c('Reddit Tokens SentimentR'='#B22222', 'Twitter Tokens SentimentR'='#8DEEEE', 'Reddit Tokens NRC Dictionary' = 'red', 'Twitter Tokens NRC Dictionary' = 'blue'))+
  labs(title="Twitter vs Reddit average sentiment")
  
  
   ggplot() + geom_smooth(data = test2, aes(x = date, y = sentiment, color = "Reddit Uncleaned Data")) + geom_smooth(data = test, aes(x = date, y = sentiment, color = "Twitter Uncleaned Data")) + geom_smooth(data = reddit_emotions, aes(x = date, y = polarity, color="Reddit Tokens NRC Dictionary")) + geom_smooth(data = twitter_emotions, aes(x = date, y = polarity, color = "Twitter Tokens NRC Dictionary")) +
scale_color_manual(name='Social Medias',
                  c('Reddit Uncleaned Data', 'Twitter Uncleaned Data', 'Reddit Tokens NRC Dictionary','Twitter Tokenized NRC Dictionary'),
                  values=c('Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = '#00B2EE', 'Reddit Tokens NRC Dictionary' = 'red', 'Twitter Tokens NRC Dictionary' = 'blue'))+
  labs(title="Twitter vs Reddit average sentiment")
                 
         
 ggplot() + geom_density(data = test4, aes(sentiment, color = "Reddit Tokens")) + geom_density(data = test3, aes(sentiment, color = "Twitter Tokens")) + geom_density(data = test2, aes(sentiment, color = "Reddit Uncleaned Data")) + geom_density(data = test, aes(sentiment, color = "Twitter Uncleaned Data")) + 
scale_color_manual(name='Social Medias',
                  breaks=c('Reddit Tokens', 'Twitter Tokens', 'Reddit Uncleaned Data', 'Twitter Uncleaned Data'),
                  values=c('Reddit Tokens'='#B22222', 'Twitter Tokens'='#8DEEEE', 'Reddit Uncleaned Data' = '#EEA2AD', 'Twitter Uncleaned Data' = '#00B2EE')) +
  labs(title="Twitter vs Reddit average sentiment")

```

Here I tested a few more graphs that seemed like interesting ideas to go onto the final project. Although all of them could be good I think the most interesting one is comparing the graphs of uncleaned data to the tokenized version. This shows us the importance of tokenizing and making sure our data is showing an actual sentiment with less filler words.

# Word Cloud

```{r wordclouds}
textplot_wordcloud(twitter_corpus_dfm, max_words=200, color="blue")

textplot_wordcloud(reddit_corpus_dfm, max_words=200, color="red")
```

# LDA New!

Below I will try testing the LDA.

```{r lda}

rlda <- dfm(reddit_lemmitized)

docvars(reddit_lemmitized) <- rlda

rlda$polarity <- rdf_nrc$polarity

rlda$date <- rdf_nrc$date

rlda_model <- LDA$new(n_topics = 25, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

rdoc_topic_distr <- 
  rlda_model$fit_transform(x = rlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)


```

```{r t lda}

tlda <- dfm(twitter_lemmitized)

docvars(twitter_lemmitized) <- tlda

tlda$polarity <- tdf_nrc$polarity

tlda$date <- tdf_nrc$date

tlda_model <- LDA$new(n_topics = 25, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

tdoc_topic_distr <- 
  tlda_model$fit_transform(x = tlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)


```

```{r charting r}

barplot(rdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(rdoc_topic_distr))

rlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),
                        lambda = 1)
rlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),
                        lambda = 0.2)

```

```{r charting t}

barplot(tdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(tdoc_topic_distr))

tlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),
                        lambda = 1)
tlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),
                        lambda = 0.2)
```

```{r testing visuals}

library(LDAvis)

rlda_model$plot()

tlda_model$plot()


```

Here we're able to see some interesting things! For Reddit especially, Twitter and Reddit topics seem to have topics that overlap significantly. This was something I hadn't seen before with my STM models.

We're also able to see with a lambda of 1 and .2 there's quite a difference in topic names.

Additionally we're able to see that some bar topics stand out quite a lot more than other topics. This made me decide to try a smaller amount of topics.

```{r 2nd test}
rlda_model <- LDA$new(n_topics = 5, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

rdoc_topic_distr <- 
  rlda_model$fit_transform(x = rlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)

tlda_model <- LDA$new(n_topics = 5, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

tdoc_topic_distr <- 
  tlda_model$fit_transform(x = tlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)

barplot(rdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(rdoc_topic_distr))

barplot(tdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(tdoc_topic_distr))

rlda_model$plot()

tlda_model$plot()
```

Hmm looking at the charts while it seems to work alright, none of the topics touch each other that much. Additionally there seems to be a lot more interacting that's realistic when it was broken out into more categories. Since in my previous blog post I did find the best k-means to be 25 I'll stick with that.

```{r full lda code}

rlda_model <- LDA$new(n_topics = 25, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

rdoc_topic_distr <- 
  rlda_model$fit_transform(x = rlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)

tlda_model <- LDA$new(n_topics = 25, doc_topic_prior = 0.1,
                     topic_word_prior = 0.01)

tdoc_topic_distr <- 
  tlda_model$fit_transform(x = tlda, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)

barplot(rdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(rdoc_topic_distr))

barplot(tdoc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0,1),
        names.arg = 1:ncol(tdoc_topic_distr))

rlda_model$plot()

tlda_model$plot()

```

# STM New!

Below I will have the STM for both reddit and twitter. This time I have added two prevalence items, polarity and dates. To get a count of days I used the arrange() function to arrange the dates correctly and then the rowid_to_column(var='count') function to gather a count by row.

```{r stm reddit}
k <- 25
rModel <- dfm(reddit_lemmitized)

docvars(reddit_lemmitized) <- rModel

rModel$polarity <- rdf_nrc$polarity

rModel$count <- rdf_nrc$count

rModel <- stm(rModel,
                K = k,
              prevalence = ~ polarity + s(count),
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

#labelTopics(rModel)
plot(rModel, type = "summary", main="Reddit top topics by sentiment and date")
```

```{r stm twitter}
k <- 25
### TWITTER
tModel <- dfm(twitter_lemmitized)

docvars(twitter_lemmitized) <- tModel

tModel$polarity <- tdf_nrc$polarity

tModel$count <- tdf_nrc$count

tModel <- stm(tModel,
                K = k,
              prevalence = ~ polarity + s(count),
               max.em.its = 1000,
               seed = 1234,
               init.type = "Spectral")

labelTopics(tModel)
plot(tModel, type = "summary", main="Twitter top topics by sentiment and date")
```

Here I was able to see that even with two factors (polarity and dates) it seems that the topics stayed fairly similar. However when I used count instead of date it did turn into a somewhat different STM model! This may be because the function s() is working properly on a continuous variable.

# Final Thoughts (TLDR)

-   Throughout my various blog post I feel like I was able to craft multiple graphs that I want to use in my Final Project.

-   It took a lot longer to figure out how to do sentiment by dates but it feels well worth it!

-   I think looking back it might have been a better idea to combine the Twitter and Reddit data together and done an analysis on that. This may have caused less trouble in rendering the blog and let me explore a bit more in depth.

```{r save image}
save.image(file = "blogpost6.RData")
```
