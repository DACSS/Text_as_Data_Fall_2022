---
title: "CaitlinRowley - Blog Post 6"
author: "Caitlin Rowley"
editor: visual
---

## Data Summary:

I have switched data sets for future blog posts. I will be using open-ended survey responses from a 1996 study titled *Survey of Gun Owners in the United States*.

For the study, respondents were asked six qualifying questions related to: (1) gun ownership, (2) gun-carrying practices, (3) gun display against the respondent, (4) gun use in self-defense against animals, (5) gun use in self-defense against people, and (6) other weapons used in self-defense. A "yes" response to a qualifying question led to a series of additional questions on the same topic as the qualifying question.

The open-ended responses include descriptions specifically related to the following questions: (1) where the respondent was when he or she displayed a gun (in self-defense or otherwise), (2) specific reasons why the respondent displayed a gun, (3) how the other individual reacted when the respondent displayed the gun, (4) how the individual knew the respondent had a gun, (5) whether the police were contacted for specific self-defense events, and (6) if not, why not.

I will focus on the following research question: Can we identify the most common circumstance in which respondents displayed a gun? I am hoping that "circumstance" can include references to both the catalyst in the situation and the environment.

```{r}
# install packages:

install.packages("RColorBrewer")
install.packages("stopwords")

# load libraries: 

library(readr)
library(tidyverse)
library(dplyr)
library(quanteda)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
library(tidytext)
library(stopwords)
library(tm)
library(quanteda.textplots)
library(tokenizers)

# read in data:

Survey <- read_csv("C:\\Users\\caitr\\OneDrive\\Documents\\DACSS\\DACSS 679\\DACSS 697\\DACSS 697 - Survey.csv", col_names = TRUE)

#rename columns:

names(Survey) <- c('Respondent_ID','Survey_Question', 'Code', 'Open_Ended_Response')
print(Survey)

# remove duplicate observations: 

Survey_unique <- distinct(Survey)

Survey_unique %>% 
    separate_rows(Open_Ended_Response) %>% 
    distinct() %>%
    nrow

```

There are 812 rows in this data frame and 4 columns. The four columns represent the following variables: survey respondent ID ('Respondent_ID), survey question ('Survey_Question'), response code ('Code'), and verbatim open-ended responses ('Open_Ended_Responses'). Each of the 812 rows now represents a unique observation. Additionally, there are 8134 unique words across all open-ended responses.

## Pre-Processing:

I will next create a corpus and remove both capitalization, punctuation, and stopwords from all open-ended responses.

```{r}
# create a corpus:

Survey_corpus <- corpus(Survey_unique$Open_Ended_Response)
head(Survey_corpus)

# tokenize, remove capitalization and punctuation:

Survey_tokens <- tokens(Survey_corpus, 
    remove_punct = T)
Survey_tokens <- tokens_tolower(Survey_tokens)
head(Survey_tokens)

# remove stopwords separately: 

Survey_tokens_stopwords <- tokens_select(Survey_tokens,
                                       pattern = stopwords("en"),
                                       selection = "remove")
```

## Data Visualization:

I will next extract features from my corpus by creating a document feature matrix. From there, I will be able to generate data visualizations.

For my first data visualization, I will generate a word cloud that focuses on the 50 most frequently-occurring words that appear a minimum of five times.

```{r}
# create document feature matrix:

Survey_tokens <- dfm(Survey_tokens_stopwords)

# identify 10 most common words:

topfeatures(Survey_dfm, 10)

# create a wordcloud:

textplot_wordcloud(Survey_dfm, min_count = 5, max_words = 50, random_order = FALSE)
```

## Co-Occurrence Matrix:

Next, I will generate a feature co-occurrence matrix to determine which terms, if any, are more likely to occur more frequently together than would be expected by chance.

```{r}
# create a document-feature matrix that is limited to words that appear frequently (more than 5% of responses).

Survey_dfm_freq <- dfm_trim(Survey_dfm, min_termfreq = 5)
Survey_dfm_freq <- dfm_trim(Survey_dfm_freq, min_docfreq = .05, docfreq_type = "prop")

# use dfm to create a feature co-occurring matrix:

Survey_fcm <- fcm(Survey_dfm_freq)

# check dimensions:

dim(Survey_fcm)

# check features:

Survey_fcm@Dimnames[["features"]]

# view co-occurrence matrix:

print(Survey_fcm)
```

We can see from selecting the co-occurrence matrix, whose dimensions are 7x7, that the co-occurring features are "house," "gun," "street," "pulled," "outside," "home," and "car." This provides limited information, but it may be interesting when considered within the context of my research question.

I would like to revisit my minimum term frequency, as well, to determine whether I should adjust it based on varied outputs.

In the interim, I will generate a word network that uses a feature co-occurrence matrix capturing terms occurring a minimum of 10 times.

```{r}
library(quanteda.textplots)

Survey_fcm_small <- fcm(Survey_dfm_freq, min_count = 10, max_words = 10)

# generate text plot:

textplot_network(Survey_fcm_small)
```

## Correlated Topic Modeling:

Next, I will apply the correlated topic modeling approach to my data set. I will also select a covariate based on closed-ended variables included in my data set.

```{r}
# install packages:

library(stm)
library(quanteda)

# generate data frame:

Survey_dfm_clean <- dfm(Survey_dfm)
             tolower = TRUE
             remove = stopwords("en")
             remove_punct = TRUE
dim(Survey_dfm_clean)
```

```{r}
# generate correlated topic model:

cor_topic_model <- stm(Survey_dfm, K = 5,
                       verbose = FALSE, init.type = "Spectral")
cor_topic_model
summary(cor_topic_model)

# label topics:
# frex = words that are both frequent and exclusive to topic.
# lift = calculated by dividing the topic-word distribution by the empirical word count probability distribution.

labelTopics(cor_topic_model)
```

I now have a topic model with 5 topics, 805 documents, and a 1224-word dictionary.

```{r}
# identify document most frequently associated with the five topics:

findThoughts(cor_topic_model,
             texts = Survey_dfm$Open_Ended_Responses,
             topics = c(1:5),
             n = 1)

```

I am having a little trouble with this code; my understanding is that it's intended to yield the document most frequently associated with the five topics, but I am seeing a different, less informative output. I will work more on this.

## Structure Topic Modeling:

Try structural topic modeling (without predictor). My thought is that these results may look similar to the correlation topic model since I am not adding a predictor.

```{r}
# choose the number of topics:

k <- 5

# specify model:

Survey_STM <- stm(Survey_dfm,
               K = k,
               data = Survey_dfm$Open_Ended_Responses,
               max.em.its = 1224,
               seed = 1234,
               init.type = "Spectral")


labelTopics(Survey_STM)
```

Try some visualization to capture the estimated frequency of words across topics:

```{r}
plot(Survey_STM, type = "summary")
```

Next, I want to try extracting topics and assigning them to the vector of document proportions. I will extract the top words (as identified by 'frex'), collapse the strings, and separate the tokens:

```{r}
# extract the words:

Survey_words <- labelTopics(Survey_STM, n=5)$frex

# set up an empty vector:

Survey_topic_labels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single label:

for(i in 1:k){Survey_topic_labels[i] <- paste(Survey_words[i,], collapse = "_")}

# print the labels:

Survey_topic_labels
```

We now can see that the top words within each topic.

Without a predictor or covariate, I am unable to measure the effect on topic distributions. Identifying a way to incorporate a covariate or predictor will be my next task.

## Next steps:

## K-Means Cluster Analysis:

I will next try a K-Means Cluster Analysis to see if any observations can be clustered into groups. I need to review this code chunk, as I am having trouble identifying clusters and generating visualizations. For future reference, I have left sample code in the code chunk.

```{r}
library(tidyverse)
library(cluster)

# start with k=10

kmeans <- kmeans(Survey_dfm_clean, centers = 10, nstart = 1)
kmeans

# text mining and weight by term frequency (inverse document frequency):

Survey_tm <- convert(Survey_dfm_clean, to = "tm")
Survey_wtfidf <- tm::weightTfIdf(Survey_tm)

# remove sparse terms:

Survey_wtfidf_sparse <- tm::removeSparseTerms(Survey_wtfidf, 0.999)

# create matrix:

wrfidf_matrix <- as.matrix(Survey_wtfidf_sparse)

# k-means cluster:

Survey_kmeans <- kmeans(wrfidf_matrix, centers = 10)
names(Survey_kmeans)

# plot clusters:

clusplot(wrfidf_matrix)

# SAMPLE CODE: 

# generate Confusion Matrix:

Survey_vector <- as.vector(Survey_dfm_clean)

Survey_CM <- table(Survey_vector, kmeans$cluster)
Survey_CM
  
# Model Evaluation and visualization
plot(iris_1[c("Sepal.Length", "Sepal.Width")])
plot(iris_1[c("Sepal.Length", "Sepal.Width")], 
     col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")], 
     col = kmeans.re$cluster, 
     main = "K-means with 3 clusters")
  
## Plotting cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]
  
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")], 
       col = 1:3, pch = 8, cex = 3) 
  
## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster iris"),
         xlab = 'Sepal.Length',
         ylab = 'Sepal.Width')
```

***QUESTION: I do not have access to the closed-ended questions in this survey; they are documented separately from the open-ended responses, and they require software that I do not have access to.***

***I am unclear as to what the codes in the open-ended responses are referring. I tried to identify patterns based on survey questions, it seems that codes are applied across different branches of the survey (1 means x, y, or z depending on the respondents' previous answers). Do I need to pivot the data frame so that each code is its own variable? Would I then apply each code as its own covariate? To me, this seems very convoluted, and I anticipate there being a certain amount of speculation involved.***
