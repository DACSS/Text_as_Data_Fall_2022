---
title: "Blog Post Three"
author: "Molly Hackbarth"
desription: "Working with the data"
date: "10/15/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw3
---

```{r}
#| label: setup
#| warning: false

library(tidyverse)
library(cld3)
library(dplyr)
library(stringi)
library(stringr)
library(here)
library(devtools)
library(tidytext)
library(plyr)
library(quanteda)
library(preText)
library(quanteda.textstats)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)

knitr::opts_chunk$set(echo = TRUE)
```

# Research Question

**My current research question:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?

# Working with the Data

In my previous blog post I had started to clean the data using two different methods. After discussing I decided I'll remove the "textclean" package in case of issues. I will also do some back tracking to have both twitter and reddit in separate databases.

### Adding Data in Separately

```{r reading in the data}
reddit_data <- read.csv(here::here("posts", "_data", "loveisblindjapan.csv"))

twitter1 <- read.csv(here::here("posts", "_data", "tweets.csv"))

twitter2 <- read.csv(here::here("posts", "_data", "tweets#.csv"))

```

```{r cleaning reddit data}

reddit <- subset(reddit_data, select = c("body", "created_utc")) 

reddit$created_utc <- as.Date.POSIXct(reddit$created_utc)

reddit <- reddit %>% 
  select(text = body, 
            date = created_utc)
# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])
reddit <- reddit %>% 
  filter(!text == '[deleted]') %>% 
  filter(!text == '[removed]')

```

```{r cleaning and combining twitter data}
#remove counting column
twitter1 <- twitter1 %>% select(!c(X, User))
twitter2 <- twitter2 %>% select(!c(X, User))

twitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)
write.csv(twitter, here::here("posts", "_data", "twitter.csv") , all(T) )

names(twitter) <- tolower(names(twitter))
twitter <- twitter %>% 
  rename_at('tweet', ~ 'text', 
            'Date' ~ 'date')
twitter$date <- strftime(twitter$date, format="%Y-%m-%d")

# remove duplicate tweets
twitter <- twitter %>% distinct(text, date, .keep_all = TRUE)

#check for duplicate tweets
twitter %in% unique(twitter[ duplicated(twitter)]) 
```

# Ways that Didn't Work Completely - Word Cloud

Below you will find ways that didn't seem to work for me,

### Cleaning the Data and Creating Tokens

To see if cleaning the data works I'll go ahead and test first with Reddit data.

```{r cleaning data reddit}

reddit_corpus <- corpus(reddit, docid_field = "doc_id", text_field = "text")
redditsummary <- summary(reddit_corpus)

#remove non english languages 
reddit_corpus <- subset(reddit_corpus, detect_language(reddit_corpus) == "en") 

#remove htmls
reddit_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", reddit_corpus)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
reddit_corpus <- reddit_corpus %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#remove htmls
reddit_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]

```

```{r  reddit corpus tokens}

reddit_corpus_tokens <- tokens(reddit_corpus, 
    remove_punct = T,
    remove_numbers = T)

reddit_corpus_tokens <- tokens_tolower(reddit_corpus_tokens)

reddit_corpus_tokens <- tokens_select(reddit_corpus_tokens, pattern = stopwords("en"), selection = "remove")

reddit_lemmitized <- tokens_replace(reddit_corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)
```

### Trying to create a DTM

```{r creating a dtm}
dtm <- TermDocumentMatrix(reddit_lemmitized) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

```

```{r}
# quanteda comes with a corpus of presidential inaugural speeches
# this first line subsets that corpus to speeches later than 1953
dfm_inaug <- corpus_subset(reddit_corpus_tokens) %>%
    # notice we are using the piping operator again.
    # this time, we pipe the corpus to DFM, which creates a document-feature matrix
    # in creating it, we remove stop words ("a", "it", "they") and punctuation
    dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
    # then we trim words that appear fewer than 10 times
    dfm_trim(min_termfreq = 10, verbose = FALSE)
```

While this works for creating tokens, unfortunately it has removed the date column. While this is fine to gather an overall sentiment, it would be nice to keep the other column there.

Additionally trying to create a dtm from this doesn't seem to work.

### Trying a Different Way

Below you will see me attempt to have a corpus vector date (where it says large simple corpus). This is because I would like the date columns. I will once again try with Reddit first.

```{r trying a different solution}
test <- corpus(reddit)

#remove non english languages 
test$text <- subset(test$text, detect_language(test) == "en") 

#remove htmls
test$text <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", test$text)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
test$text <- test$text %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

test <- Corpus(VectorSource(reddit))

test <- test %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>% # remove stop words
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, stopwords("english")) # remove stop words


```

#### Trying to Create Tokens

```{r trying to tokenize}
test_tokens <- tokens(test, 
    remove_punct = T,
    remove_numbers = T)

test_tokens <- tokens_tolower(test_tokens)

lemmitized <- tokens_replace(test_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)

```

While this has created a large simple corpus which has included the two columns I mentioned, I can't seem to create "tokens" this way which could make researching difficult.

#### DTM & Word Cloud

```{r creating a dtm}
dtm <- TermDocumentMatrix(test2) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

```

```{r wordcloud}
library(wordcloud)
library(RColorBrewer)
set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 50,           max.words=100, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

Here we can see the word cloud worked!

I also noticed despite removing emojis and other languages this way it seems to still be in the matrix. While it only shows up once and awhile it is sort of strange.

### Additional TM code

To try to help to remove more I will use addition tm code.

```{r addint tospace}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
test <- test %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>% 
  tm_map(toSpace, "/") %>% 
  tm_map(toSpace, "@") %>% 
  tm_map(toSpace, "\\|")

```

### Dealing with some issues

I realized my current removing non English is not working completely. Thus I'm trying a different formula in addition to make it work.

I also realized I was using tm_map to remove numbers which may have caused an issue.

```{r}
test2 <- corpus(reddit)

test2 <- subset(test2, detect_language(test3) == "en") 

test2 <- stringi::stri_trans_general(test2, "latin-ascii")

#remove htmls
test2 <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", test3)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
test2 <- test2 %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#change non letters to spaces
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

test2 <- test %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>% # remove stop words
  tm_map(toSpace, "/") %>% 
  tm_map(toSpace, "@") %>% 
  tm_map(toSpace, "\\|") %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, stopwords("english")) # remove stop words
```

#### Trying a DTM & Word Cloud Again

```{r updated dtm}
dtm <- TermDocumentMatrix(test2) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

```

```{r updated wordcloud}
library(wordcloud)
set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 10,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))

```

This seems to have kept both the dates and if it's twitter or reddit which does help a lot! However I would like those removed for the word cloud still.

# Way that Worked - Word Cloud

### Another way for Word Clouds

I was able to find another way to do text plots that don't involve using the "tm" package to create a corpus thanks to the class blog! Below you will see the word cloud for Reddit Post.

```{r word cloud}
library(quanteda.textplots)
reddit_corpus_edit <- reddit_corpus %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>% dfm_trim(min_termfreq = 10, verbose = FALSE)
textplot_wordcloud(reddit_corpus_edit, max_words=100, color="red")


```

#### Twitter Word Cloud

Below you will see the word cloud for Twitter posts.

```{r cleaning tweets}

twitter_corpus <- corpus(twitter)
twittersummary <- summary(twitter_corpus)

#remove non english languages 
#twitter_corpus <- subset(twitter_corpus, detect_language(twitter_corpus) == "en") 

twitter_corpus <- stringi::stri_trans_general(twitter_corpus, "latin-ascii")

#remove htmls
twitter_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", twitter_corpus)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
twitter_corpus <- twitter_corpus %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#remove htmls
twitter_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]

library(quanteda.textplots)

twitter_corpus_edit <- twitter_corpus %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>% dfm_trim(min_termfreq = 10, verbose = FALSE)
textplot_wordcloud(twitter_corpus_edit, max_words=20, color="blue")

```

Something I noticed was that when I tried to subset with English, the amount of tweets in the Twitter corpus went down by almost half. This seemed to be an error issue as when I looked at my data it shouldn't have gone down more than 1,000. This has lead me to remove the detect language function.

## Code For New Word Clouds

```{r reddit}
reddit_corpus <- corpus(reddit, docid_field = "doc_id", text_field = "text")
redditsummary <- summary(reddit_corpus)

#remove non english languages 
reddit_corpus <- stringi::stri_trans_general(reddit_corpus, "latin-ascii")

#remove htmls
reddit_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", reddit_corpus)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
reddit_corpus <- reddit_corpus %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#remove htmls
reddit_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", reddit_corpus)
reddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]

reddit_corpus_edit <- reddit_corpus %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>% dfm_trim(min_termfreq = 10, verbose = FALSE)
textplot_wordcloud(reddit_corpus_edit, max_words=100, color="red")
```

```{r twitter}
twitter_corpus <- corpus(twitter)
twittersummary <- summary(twitter_corpus)

#remove non english languages 
twitter_corpus <- stringi::stri_trans_general(twitter_corpus, "latin-ascii")

#remove htmls
twitter_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", twitter_corpus)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
twitter_corpus <- twitter_corpus %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#remove htmls
twitter_corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", twitter_corpus)
twitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]

twitter_corpus_edit <- twitter_corpus %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>% dfm_trim(min_termfreq = 10, verbose = FALSE)
textplot_wordcloud(twitter_corpus_edit, max_words=20, color="blue")
```
