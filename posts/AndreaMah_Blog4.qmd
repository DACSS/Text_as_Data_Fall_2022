---
title: "Blog Post 4: Sentiment analysis"
author: "Andrea Mah"
desription: "Continued data exploration and analysis"
date: "11/10/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - BlogPost4
  - Andrea Mah

---

```{r}
#loading in nececssary libraries
library(quanteda.sentiment)
library(quanteda)
library(tidyr)
library(dplyr)
library(ggplot2)
library(devtools)
library(RCurl)
#devtools::install_github("kbenoit/quanteda.dictionaries")
library(quanteda.dictionaries)
#remotes::install_github("quanteda/quanteda.sentiment")
library(quanteda.sentiment)

```

For this blog, I wanted to report on my progress trying out sentiment analysis on my data. In my project, my corpus is made up of speeches given by world leaders at the UN climate change conferences (COP1 - COP15). First, I prepared the data, and ran sentiment analysis. 

```{r}
#loading in the processed dataset. 
x <- getURL('https://raw.githubusercontent.com/andrea-yj-mah/DatafilesTAD/main/FINAL_combined_meta-text_dataset.csv')
speechdf <- read.csv(text = x)

#convert the data to a corpus
speech_corpus <- corpus(speechdf)

#run sentiment analysis
speech_nrc_sentiment <- liwcalike(speech_corpus, data_dictionary_NRC)
#check output
head(speech_nrc_sentiment)

# converting the corpus to dfm using the dictionary
speech_nrc <- tokens(speech_corpus,
                             include_docvars = TRUE) %>%
  dfm() %>%
  dfm_lookup(data_dictionary_NRC)

#checking the conversion and outputs
dim(speech_nrc)
head(speech_nrc, 10)
class(speech_nrc)
```
After running the inital sentiment analysis, I had a dfm, but the dfm was missing my metadata, and I couldn't use the dfm to run calculations that I was interested in. I calculated polarity by document. Next, I converted the dfm to a data frame object and merged it with my metadata.

```{r}
#converting the data to a dataframe to be able to perform calculations. 
df_nrc <- convert(speech_nrc, to = "data.frame")

#calculate polarity like we learned in class
df_nrc$polarity <- (df_nrc$positive - df_nrc$negative)/(df_nrc$positive + df_nrc$negative)

df_nrc$polarity[(df_nrc$positive + df_nrc$negative) == 0] <- 0

#now I want to merge it back with the dataframe that has all the metadata
names(speechdf)
names(df_nrc)

#creating a variable that can help to match data
speechdf$doc_id <- paste("text",speechdf$textnum, sep = "")

#joining the data
df_nrc_meta <- left_join(speechdf, df_nrc, by = "doc_id")

```

Then I just wanted to look at some descriptive information about the results of the sentiment analysis...I noticed that the mean 'positive' scores were
a lot higher than those for 'negative.' I was curious if the difference was significant and ran some t.tests to see. The speeches contained significantly more 
positive sentiment than negative. Perhaps this isn't surprising given that the context where speeches are delivered is one of global cooperation, and probably
leaders don't want to project too much negativity? 

```{r}
summary(df_nrc_meta)

t.test(df_nrc_meta$positive, df_nrc_meta$negative, paired = TRUE)

```

Moving on to some other analyses with my metadata...

For now, I was interested in two meta-data variables and their relationship to
sentiment in the text. These are the year the speech was delivered and the
climate risk index (CRI) associated with the country of the speaker. 
The CRI is based on experienced weather-related impacts
on regions/countries like heat-waves, flooding, or storms. Countries with higher CRI
are objectively more impacted by climate change. My prediction was that countries with
higher CRI may feel more urgency about addressing climate change, and perhaps
speeches delivered by members of these countries would contain more negative sentiments,
fear, or anger... 


Regarding the year speeches were delivered, my thought was that while speeches from the beginning of
these conferences would contain more positive sentiments, but that perhaps these decreased
over time as climate change impacts became more frequent/severe, and as progress continued to be
slow.

For each of these variables, I looked first at correlations and then regressions for
any of the observed sigificant correlations. I looked at their relationship to polarity, and
then to each of the emotions. 

```{r}
#first, is polarity of speeches associated with year? 
cor.test(df_nrc_meta$year, df_nrc_meta$polarity, method = "pearson")

#second, was polarity of speeches associated with CRI? 
cor.test(df_nrc_meta$CRI, df_nrc_meta$polarity, method = "pearson")

```

Both CRI and year were significantly and negatively correlated with polarity.

For year*polarity: From the beginning of COP conferences --> COP 15, later speeches tended to be more negative
For CRI*polarity: Countries who experienced more impacts from climate change tended to give more negative speeches. 

Next, I was interested in looking at other emotions. First, I looked at correlations. 

```{r}
cor.emo.year <- function(b){
  cor.test(df_nrc_meta$year, b, method = "pearson")
}
cor.emo.year(df_nrc_meta$fear)
cor.emo.year(df_nrc_meta$negative)
cor.emo.year(df_nrc_meta$positive)
cor.emo.year(df_nrc_meta$anger)
cor.emo.year(df_nrc_meta$disgust)
cor.emo.year(df_nrc_meta$surprise)
cor.emo.year(df_nrc_meta$trust)
cor.emo.year(df_nrc_meta$anticipation)
cor.emo.year(df_nrc_meta$sadness)
cor.emo.year(df_nrc_meta$joy)

```
Only fear, anger and trust sentiment had significant results. 
Increased year was associated with increased fear/anger. 
Increased year was associated with lower trust..

Then, I wanted to regress year on these emotions and polarity, just to see how much variance was explained by time.

```{r}
reg.emo <- function(b){
 emomodel <- lm(b ~ year, data = df_nrc_meta)

 emoplot <- ggplot(data = df_nrc_meta, aes(x = year, y = b)) +
   geom_point() +
   geom_smooth(method = 'lm', formula = y ~ x)
 
 print(summary(emomodel))
 emoplot
   
}

reg.emo(df_nrc_meta$polarity)
reg.emo(df_nrc_meta$fear)
reg.emo(df_nrc_meta$anger)
reg.emo(df_nrc_meta$trust)
```

The relationships certainly look small, even if they were significant. For each dependent variable, year explained less than 1% of the variance. 

Next, I followed the same procedure examining climate risk in relation to outcomes

```{r}
cor.emo.cri <- function(b){
  cor.test(df_nrc_meta$CRI, b, method = "pearson")
}

cor.emo.cri(df_nrc_meta$anger)
cor.emo.cri(df_nrc_meta$anticipation)
cor.emo.cri(df_nrc_meta$sadness)
cor.emo.cri(df_nrc_meta$disgust)
cor.emo.cri(df_nrc_meta$trust)
cor.emo.cri(df_nrc_meta$surprise)
cor.emo.cri(df_nrc_meta$joy)
cor.emo.cri(df_nrc_meta$fear) #weirdly, higher CRI associated with less fearful? 
cor.emo.cri(df_nrc_meta$negative) #significant, negative relationship...
cor.emo.cri(df_nrc_meta$positive) # significant, negative relationship...

```

Besides polarity, CRI was also significantly associated with sentiments of fear, positivity, and negativity. 
These relationships were not in the direction I expected. Higher CRI was associated with...
-less fear
-less negative sentiment
-less positive sentiment

Next I looked at regressions for these.

```{r}
#now, regressions. 
reg.emo.cri <- function(b){
  emomodel <- lm(b ~ CRI, data = df_nrc_meta)
  
  emoplot <- ggplot(data = df_nrc_meta, aes(x = CRI, y = b)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = y ~ x)
  
  print(summary(emomodel))
  emoplot
  
}

reg.emo.cri(df_nrc_meta$fear)
reg.emo.cri(df_nrc_meta$negative)
reg.emo.cri(df_nrc_meta$positive)
reg.emo.cri(df_nrc_meta$polarity)
```

Again, the relationships though significant were very small, with tiny effect sizes. I will think more about
whether or not sentiment analysis is useful to me in this context...I also only looked at results from using one dictionary. 
In the future, I might see how/whether using different dictionaries might impact my results.

For the next step in my project, I am interested in testing out some unsupervised methods. Topic modeling might help me to identify interesting things. 
