---
title: "Blog Post Three"
author: "Molly Hackbarth"
desription: "Focusing on downloading data"
date: "10/15/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - blog posts
  - hw3
---

```{r}
#| label: setup
#| warning: false

library(tidyverse)
library(cld3)
library(dplyr)
library(textclean)
library(stringi)
library(stringr)
library(here)
library(devtools)
library(tidytext)
library(plyr)
library(quanteda)
library(preText)
library(quanteda.textstats)
library(tm)

knitr::opts_chunk$set(echo = TRUE)
```

## Research Question

**My current research question I'm leaning towards is:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?

## Cleaning the Data

In my previous blog post I had started to clean the data using two different methods. Today I hope to combine them both together to make an easy to read list.

```{r cleaning data}

data <- read.csv(here::here("posts", "_data", "loveisblind_socialmedia.csv"))

#change the date to a character vector
data$date <- as.character(data$date)

corpus <- corpus(data)
corpussummary <- summary(corpus)


#remove non english languages 
corpus <- subset(corpus, detect_language(corpus) == "en") 

#remove htmls
corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", corpus)

#remove emojis
only_ascii_regexp <- '[^\u0001-\u007F]+|<U\\+\\w+>'
corpus <- corpus %>% 
 str_replace_all(regex(only_ascii_regexp), "") 

#remove htmls
corpus <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", corpus)
corpus <- corpus[!is.na(corpus)]

```

```{r text clean}

<<<<<<< HEAD
#corpus <- replace_internet_slang(corpus)

#corpus <- replace_word_elongation(corpus)

#corpus <- replace_kern(corpus)
```

```{r corpus tokens}
corpus <- corpus[!is.na(corpus)]
corpus2 <- corpus
corpus_tokens <- tokens(corpus, 
    remove_punct = T,
    remove_numbers = T)

corpus_tokens <- tokens_tolower(corpus_tokens)

corpus_tokens <- tokens_select(corpus_tokens, pattern = stopwords("en"), selection = "remove")

lemmitized <- tokens_replace(corpus_tokens, 
                             pattern = lexicon::hash_lemmas$token, 
                             replacement = lexicon::hash_lemmas$lemma)
```

```{r new code}

```

## Code

```{r full current code}

```
