---
title: "Blog Post 3"
author: "Mani Shanker Kamarapu"
desription: "Acquiring Data and Tidying data"
date: "11/5/2022"
format:
  html:
    df-print: paged
    css: styles.css
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - Post3
  - ManiShankerKamarapu
  - Amazon Review analysis
---
## Introduction

In the last post, I have acquired the data for one product in Amazon and did the analysis and converted into corpus and done a wordcloud. In this blog I plan to scrape more reviews on different products and do preprocessing of data.

## Loading the libraries

```{r}
library(polite)
library(rvest)
library(tidyverse)
library(stringr)
library(quanteda)
library(tidyr)
library(RColorBrewer)
library(quanteda.textplots)
library(wordcloud)
library(wordcloud2)
library(devtools)
library(quanteda.dictionaries)
library(quanteda.sentiment)

knitr::opts_chunk$set(echo = TRUE)
```

```{r}
scrape_amazon <- function(ASIN, page_num){
  
  url_reviews <- paste0("https://www.amazon.com/product-reviews/",ASIN,"/?pageNumber=",page_num)
  
  doc <- read_html(url_reviews) # Assign results to `doc`
  
  # Review Title
  doc %>% 
    html_nodes("[class='a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold']") %>%
    html_text() -> review_title
  
  # Review Text
  doc %>% 
    html_nodes("[class='a-size-base review-text review-text-content']") %>%
    html_text() -> review_text
  
  # Number of stars in review
  doc %>%
    html_nodes("[data-hook='review-star-rating']") %>%
    html_text() -> review_star
  
  # Return a tibble
  tibble(review_title,
         review_text,
         review_star,
         page = page_num,
         ASIN) %>% return()
}
```

<!-- ```{r} -->
<!-- out <- scrape_amazon("B0001DBI1Q", 1) -->
<!-- for (i in 2:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B0001DBI1Q", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B0001MC01Y", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B00026WUZU", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B07ZN4WM13", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B005C7QVUE", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B000BO2D64", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 336:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B000I2JFQU", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 311:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B000UW50LW", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B001FD6RLM", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B07HHJ7669", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B07T6BQV2L", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- for (i in 1:400) { -->
<!--   out <- bind_rows(out, scrape_amazon("B07T43YYRY", i)) -->
<!--   if((i %% 3) == 0){ -->
<!--     Sys.sleep(2) # Take an additional two second break -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- write.csv(out, "amazonreview.csv") -->
<!-- ``` -->

```{r}
reviews <- read_csv("amazonreview.csv")
reviews
```

```{r}
clean_text <- function (text) {
  str_remove_all(text," ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>% 
    # Remove mentions
    str_remove_all("@[[:alnum:]_]*") %>% 
    # Replace "&" character reference with "and"
    str_replace_all("&amp;", "and") %>%
    # Remove punctuation, using a standard character class
    str_remove_all("[[:punct:]]") %>%
    # remove digits
    str_remove_all("[[:digit:]]") %>%
    # Replace any newline characters with a space
    str_replace_all("\\\n|\\\r", " ") %>%
    # remove strings like "<U+0001F9F5>"
    str_remove_all("<.*?>") %>% 
    # Make everything lowercase
    str_to_lower() %>%
    # Remove any trailing white space around the text and inside a string
    str_squish()
}
```

```{r}
reviews$clean_text <- clean_text(reviews$review_text)
reviews
```

```{r}
text <- corpus(c(reviews$clean_text))
text <- dfm(tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%
             tokens_select(pattern=stopwords("en"),
                            selection="remove"))
text
```

```{r}
sum(ntoken(text))
```

```{r}
summary(corpus(c(reviews$clean_text)))
```

```{r}
word_counts <- as.data.frame(sort(colSums(text),dec=T))
colnames(word_counts) <- c("Frequency")
word_counts$word <- row.names(word_counts)
word_counts$Rank <- c(1:ncol(text))
word_counts
```

```{r}
text_dfm <- dfm_trim(text, min_termfreq = 50, docfreq_type = "prop")
# create fcm from dfm
text_fcm <- fcm(text_dfm)
text_fcm
```

```{r}
# pull the top features
top_features <- names(topfeatures(text_fcm, 50))
# retain only those top features as part of our matrix
even_text_fcm <- fcm_select(text_fcm, pattern = top_features, selection = "keep")
# check dimensions
dim(even_text_fcm)
# compute size weight for vertices in network
size <- log(colSums(even_text_fcm))
# create plot
textplot_network(even_text_fcm, vertex_size = size / max(size) * 2)
```

```{r}
textplot_wordcloud(text, min_count = 100, max_words = 200, random_order = TRUE)
```

## Further study

I will be adding more reviews and doing pre-processing and also plot some analysis plots and if possible also do some sentiment analysis.
