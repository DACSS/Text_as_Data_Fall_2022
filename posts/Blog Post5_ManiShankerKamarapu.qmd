---
title: "Blog Post 5"
author: "Mani Shanker Kamarapu"
desription: "Topic modelling"
date: "11/16/2022"
format:
  html:
    df-print: paged
    css: styles.css
    toc: true
    code-fold: false
    code-copy: true
    code-tools: true
categories:
  - Post5
  - ManiShankerKamarapu
  - Amazon Review analysis
---
## Introduction

In the last post, I have done sentimental analysis. In this blog I plan to do topic modelling of data.

## Loading the libraries

```{r}
library(polite)
library(rvest)
library(ggplot2)
library(plotly)
library(tidyverse)
library(SnowballC)
library(stringr)
library(ldatuning)
library(quanteda)
library(tidyr)
library(reshape2)
library(RColorBrewer)
library(tidytext)
library(quanteda.textplots)
library(wordcloud)
library(textdata)
library(gridExtra)
library(stm)
library(wordcloud2)
library(devtools)
library(quanteda.dictionaries)
library(quanteda.sentiment)

knitr::opts_chunk$set(echo = TRUE)
```

## Reading the data

```{r}
reviews <- read_csv("amazonreview.csv")
reviews
```

## Pre-processing function

I have cleaned the text of the reviews by removing punctuations, numbers, UTF symbols, &amp, digits, new line characters, single length words, Pascal case words were removed from the tweets text using the `stringr` library departments communicate information to alleviate specific public functions. Stopwords are removed. The stopwords collection is taken from `stopwords-iso` and `SMART`. Stemming is not preferred here as the meaning of the word is important for analysis. And then categorized the data based on the book title and series title. The analysis done in the project is based on the categorized series titles to compare sentiments and topics on basis of series.

```{r}
clean_text <- function (text) {
  str_remove_all(text," ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>% 
    # Remove mentions
    str_remove_all("@[[:alnum:]_]*") %>% 
    # Replace "&" character reference with "and"
    str_replace_all("&amp;", "and") %>%
    # Remove punctuation
    str_remove_all("[[:punct:]]") %>%
    # remove digits
    str_remove_all("[[:digit:]]") %>%
    # Replace any newline characters with a space
    str_replace_all("\\\n|\\\r", " ") %>%
    # remove strings like "<U+0001F9F5>"
    str_remove_all("<.*?>") %>% 
    # Make everything lowercase
    str_to_lower() %>%
    # Remove any trailing white space around the text and inside a string
    str_squish()
}
```

## Tidying the data

Dropping the NA in the cleaned text.

```{r}
reviews$clean_text <- clean_text(reviews$review_text) 
reviews <- reviews %>%
  drop_na(clean_text)
reviews
```

Removing unnecessary columns

```{r}
reviews <- reviews %>%
  select(-c(...1, page, review_text))
reviews
```

Pre-processing the title variable

```{r}
reviews$review_title <- reviews$review_title %>%
  str_remove_all("\n")
reviews
```

Re-coding star of reviews from character to numeric 

```{r}
reviews$review_star <- substr(reviews$review_star, 1, 3) %>%
  as.numeric()
  reviews
```

Adding new variable book title to the reviews

```{r}
reviews <- reviews %>%
  mutate(book_title = case_when(ASIN == "B0001DBI1Q" ~ "A Game of Thrones: A Song of Ice and Fire, Book 1", 
                                ASIN == "B0001MC01Y" ~ "A Clash of Kings: A Song of Ice and Fire, Book 2", 
                                ASIN == "B00026WUZU" ~ "A Storm of Swords: A Song of Ice and Fire, Book 3", 
                                ASIN == "B07ZN4WM13" ~ "A Feast for Crows: A Song of Ice and Fire, Book 4", 
                                ASIN == "B005C7QVUE" ~ "A Dance with Dragons: A Song of Ice and Fire, Book 5", 
                                ASIN == "B000BO2D64" ~ "Twilight: The Twilight Saga, Book 1", 
                                ASIN == "B000I2JFQU" ~ "New Moon: The Twilight Saga, Book 2", 
                                ASIN == "B000UW50LW" ~ "Eclipse: The Twilight Saga, Book 3", 
                                ASIN == "B001FD6RLM" ~ "Breaking Dawn: The Twilight Saga, Book 4 ", 
                                ASIN == "B07HHJ7669" ~ "The Hunger Games", 
                                ASIN == "B07T6BQV2L" ~ "Catching Fire: The Hunger Games", 
                                ASIN == "B07T43YYRY" ~ "Mockingjay: The Hunger Games, Book 3"))
reviews
```

Using the ASIN given to each product by amazon converted the ASIN into the book titles and then I will further add the series titles to categorize them into three series.

Adding new variable series title to the reviews

```{r}
reviews <- reviews %>%
  mutate(series_title = case_when(ASIN == "B0001DBI1Q" ~ "A Song of Ice and Fire", 
                                ASIN == "B0001MC01Y" ~ "A Song of Ice and Fire", 
                                ASIN == "B00026WUZU" ~ "A Song of Ice and Fire", 
                                ASIN == "B07ZN4WM13" ~ "A Song of Ice and Fire", 
                                ASIN == "B005C7QVUE" ~ "A Song of Ice and Fire", 
                                ASIN == "B000BO2D64" ~ "The Twilight Saga", 
                                ASIN == "B000I2JFQU" ~ "The Twilight Saga", 
                                ASIN == "B000UW50LW" ~ "The Twilight Saga", 
                                ASIN == "B001FD6RLM" ~ "The Twilight Saga", 
                                ASIN == "B07HHJ7669" ~ "The Hunger Games", 
                                ASIN == "B07T6BQV2L" ~ "The Hunger Games", 
                                ASIN == "B07T43YYRY" ~ "The Hunger Games"))
reviews
```

## Tokenization of data

I have split the data set into 3 parts based on the series title to do sentiment analysis on each series separately.

```{r}
reviews1 <- reviews %>%
  filter(series_title == "A Song of Ice and Fire")
reviews2 <- reviews %>%
  filter(series_title == "The Twilight Saga")
reviews3 <- reviews %>%
  filter(series_title == "The Hunger Games")
```

Converting the 3 spliced data into corpus and then tokenizing them and creating a document feature matrix.

```{r}
# Converting the text into corpus
text_corpus1 <- corpus(c(reviews1$clean_text))
# Converting the text into tokens
text_token1 <- tokens(text_corpus1, remove_punct=TRUE, remove_numbers = TRUE, remove_separators = TRUE, remove_symbols = TRUE) %>% 
  tokens_select(pattern=c(stopwords("en"), "im", "didnt", "couldnt","wasnt", "id", "ive", "isnt", "dont", "wont", "shes", "doesnt"), selection="remove") %>%
  tokens_select(pattern=stopwords("SMART"), 
                selection="remove") 
# Converting tokens into Document feature matrix
text_dfm1 <- dfm(text_token1) %>%
  dfm_trim(min_docfreq = 10, min_termfreq = 10) %>%
  dfm_subset(ntoken(.) > 0)
text_dfm1
```

```{r}
# Converting the text into corpus
text_corpus2 <- corpus(c(reviews2$clean_text))
# Converting the text into tokens
text_token2 <- tokens(text_corpus2, remove_punct=TRUE, remove_numbers = TRUE, remove_separators = TRUE, remove_symbols = TRUE) %>% 
  tokens_select(pattern=c(stopwords("en"), "im", "didnt", "couldnt","wasnt", "id", "ive", "isnt", "dont", "wont", "shes", "doesnt"), selection="remove") %>%
  tokens_select(pattern=stopwords("SMART"), 
                selection="remove") 
# Converting tokens into Document feature matrix
text_dfm2 <- dfm(text_token2) %>%
  dfm_trim(min_docfreq = 10, min_termfreq = 10) %>%
  dfm_subset(ntoken(.) > 0)
text_dfm2
```

```{r}
# Converting the text into corpus
text_corpus3 <- corpus(c(reviews3$clean_text))
# Converting the text into tokens
text_token3 <- tokens(text_corpus3, remove_punct=TRUE, remove_numbers = TRUE, remove_separators = TRUE, remove_symbols = TRUE) %>% 
  tokens_select(pattern=c(stopwords("en"), "im", "didnt", "couldnt","wasnt", "id", "ive", "isnt", "dont", "wont", "shes", "doesnt"), selection="remove") %>%
  tokens_select(pattern=stopwords("SMART"), 
                selection="remove") 
# Converting tokens into Document feature matrix
text_dfm3 <- dfm(text_token3) %>%
  dfm_trim(min_docfreq = 10, min_termfreq = 10) %>%
  dfm_subset(ntoken(.) > 0)
text_dfm3
```

## Topic modelling

### Structural Topic Modelling

```{r}
topic_model <- stm(text_dfm1, K = 15, init.type = 'Spectral')
```

```{r}
summary(topic_model)
```

Ultimately, I decided to discard the structural topic modelling for the following reasons:

- The model is computationally expensive to run, because of the size of the table.
- I am not adding any covariate, so it makes more sense to run correlated topic models, which I will attempt in the next section.

### Correlated Topic Modelling: The Song of Ice and Fire

I have used searchK() function to find the right K value to fit the model. 

```{r}
model1_stm <- searchK(text_dfm1, K = c(5, 10, 15), N = floor(0.1 * nrow(reviews1)), data = reviews1, max.em.its = 100, init.type = "Spectral")
plot(model1_stm)
```

The held-out likelihood is highest at 15 topics, and the residuals are lowest around 15 topics, so perhaps a good number of topics would be around there. Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff.

```{r}
model1 <- convert(text_dfm1, to = "stm")
model1 <- stm(model1$documents, model1$vocab, K = 15, verbose = FALSE, max.em.its = 100, seed = 1357, init.type = "Spectral")
```

```{r}
summary(model1)
```

The model have 4757 word dictionaries. There are many interesting words in different topics which will help to understand different emotions we saw in sentimental analysis. Topics 1-4 and 15 discuss on different characters and there roles and how are they. Topic 5 discuss about types of books in amazon. Topic 6 has words like supernatural, fanatsy and political which discuss different genres of the series. Topic9, 10 and 14 discuss about emotions in the series.

```{r}
plot(model1)
```

The above plot shows the top topics with their expected topic proportions. Topic 3, topic 9 and topic 1 have the highest proportion and Topic 11 has the least proportion.

```{r}
tidy_beta <- tidy(model1)
tidy_beta %>% group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill=topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales="free") +
  coord_flip() 
```

The above plot shows the top bag of words in each topic.

```{r}
#How the topics are linked?
topicQuality(model1, text_dfm1)
```

I would choose Topics 9, 12 and 15 as being of good quality (good balance between exclusivity and semantic coherence).

### Correlated Topic Modelling: The Twilight Saga

I have used searchK() function to find the right K value to fit the model.

```{r}
model2_stm <- searchK(text_dfm2, K = c(5, 10, 15), N = floor(0.1 * nrow(reviews2)), data = reviews2, max.em.its = 100, init.type = "Spectral")
plot(model2_stm)
```

The held-out likelihood is highest at 15 topics, and the residuals are lowest around 15 topics, so perhaps a good number of topics would be around there. Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff.

```{r}
model2 <- convert(text_dfm2, to = "stm")
model2 <- stm(model2$documents, model2$vocab, K = 15, verbose = FALSE, max.em.its = 100, seed = 1357, init.type = "Spectral")
```

```{r}
summary(model2)
```

The model have 5030 word dictionaries. There are many interesting words in different topics which will help to understand different emotions we saw in sentimental analysis. Topics 1, 5, 11 and 13 discuss on different characters and there roles and how are they. Topic 3, 8 and 10 have words like fantastic, amazing, excitement, enjoyed, happy which discuss different sentiments of the series. Topic 8 also have words like thriller, action and suspense which represent the genre of the series.

```{r}
plot(model2)
```

The above plot shows the top topics with their expected topic proportions. Topic 12, 14 and 6 have the highest proportion and Topic 13 has the least proportion.

```{r}
tidy_beta <- tidy(model2)
tidy_beta %>% group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill=topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales="free") +
  coord_flip() 
```

The above plot shows the top bag of words in each topic.

```{r}
#How the topics are linked?
topicQuality(model2, text_dfm2)
```

I would choose Topics 12, 8 and 7 as being of good quality (good balance between exclusivity and semantic coherence).

### Correlated Topic Modelling: The Hunger Games

I have used searchK() function to find the right K value to fit the model.

```{r}
model3_stm <- searchK(text_dfm3, K = c(5, 10, 15), N = floor(0.1 * nrow(reviews3)), data = reviews3, max.em.its = 100, init.type = "Spectral")
plot(model3_stm)
```

The held-out likelihood is highest at 15 topics, and the residuals are lowest around 15 topics, so perhaps a good number of topics would be around there. Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff.

```{r}
model3 <- convert(text_dfm3, to = "stm")
model3 <- stm(model3$documents, model3$vocab, K = 15, verbose = FALSE, max.em.its = 100, seed = 1357, init.type = "Spectral")
```

```{r}
summary(model3)
```

The model have 3953 word dictionaries. There are many interesting words in different topics which will help to understand different emotions we saw in sentimental analysis. Topics 1 and 2 discuss on different characters and there roles and how are they. Topic 14 discuss about types of books in amazon. Topic 2 and 4 has words like fiction, scifi and violence which discuss different genres of the series. Topics 5 and 8 amazing, love, interesting discuss about emotions in the series.

```{r}
plot(model3)
```

The above plot shows the top topics with their expected topic proportions. Topics 12, 7 and 13 have the highest proportion and Topic 1 has the least proportion.

```{r}
tidy_beta <- tidy(model3)
tidy_beta %>% group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill=topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales="free") +
  coord_flip() 
```

The above plot shows the top bag of words in each topic.

```{r}
#How the topics are linked?
topicQuality(model3, text_dfm3)
```

I would choose Topics 12, 13 and 15 as being of good quality (good balance between exclusivity and semantic coherence).

## Further study

I will do evaluate the blogs and try to conclude the blog.
