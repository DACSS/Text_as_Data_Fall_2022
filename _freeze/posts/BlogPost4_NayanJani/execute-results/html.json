{
  "hash": "13dd29c06eb3fa52725444542cce6aaa",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 4\"\nauthor: \"Nayan Jani\"\ndescription: \"Sentiment Analysis\"\ndate: \"11/07/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Nayan Jani\n  - Blog4\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textmodels)\nlibrary(ggplot2)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.sentiment)\nlibrary(syuzhet)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n\n\n# My Data \n\nUsing Youtube API, I was able to extract comments from multiple videos covering the world cup in Qatar. One of the Videos is from Nov 13, 2021 and has 421,163 views and 1,862 comments. The reason that I chose this video is because it is the most interacted video on  youtube that talks about some of the controversies that surround this years World Cup I Qatar. I realize that this video is not current but my goal does not include comparison of language and thoughts over a period of time. The Qatar World cup has been a controversy for years and the discussion leading up to it has not changed because of the possible human rights violations. The comments I have scraped are the top 100 most relevant comments and the top 100 most recent comments. This only leaves me with 400 comments to deal with at the moment . For now I will only keep it at 400 so that I can figure out my thought process for analysis but for my next blog post I will scrape more comments and videos that discuss the controversies of the World Cup.\n\n\n# Key Questions\n\nMy Goal is to:\n\n  - Find the overall Sentiment of the comments (Positive and Negative, Other emotions)\n  \n  - What is the main focus in the comments (what topic is most important to the people in the comments)\n  \n  - Based on the most important topics and the sentiment of those comments, are those comments classified correctly positive or negative? If yes, what are the comments POV? Are those comments \"socially correct\"? (logical/acceptable POV vs Stereotyped/Stigmatized POV)\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_bbc<- read_csv(\"_data/comments_bbc.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 100 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): i\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_bbc<- df_bbc%>% \n  rename(text = \"i\")\n\ndf_bbc<- df_bbc %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbc <- df_bbc %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbc <- corpus(df_bbc)\ncorpus_bbc_summary <- summary(corpus_bbc)\n\ncorpus_bbc_summary$video <- \"BBC\"\ndocvars(corpus_bbc) <- corpus_bbc_summary\n\ndf_q<- read_csv(\"_data/comments_q.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_q<- df_q %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\n\ndf_q <- df_q %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_q <- corpus(df_q)\ncorpus_q_summary <- summary(corpus_q)\ncorpus_q_summary$video <- \"Maqwell\"\ndocvars(corpus_q) <- corpus_q_summary\n\ndf_qRev<- read_csv(\"_data/comments_qRev.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_qRev <- df_qRev%>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_qRev<- df_qRev %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_qRev <- df_qRev %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_qRev <- corpus(df_qRev)\ncorpus_qRev_summary <- summary(corpus_qRev)\ncorpus_qRev_summary$video <- \"MaqwellRev\"\ndocvars(corpus_qRev) <- corpus_qRev_summary\n\ndf_sky<- read_csv(\"_data/comments_sky.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): i\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_sky<- df_sky%>% \n  rename(text = \"i\")\n\ndf_sky<- df_sky %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_sky <- df_sky %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_sky <- corpus(df_sky)\ncorpus_sky_summary <- summary(corpus_sky)\ncorpus_sky_summary$video <- \"sky\"\ndocvars(corpus_sky) <- corpus_sky_summary\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_bbc_summary1<- as.tibble(corpus_bbc_summary)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n```\n:::\n\n```{.r .cell-code}\ncorpus_bbc_summary1<- corpus_bbc_summary1 %>% \n  mutate(Text =str_replace_all(Text,\"text\", \"bbc-text\"))\n\ncorpus_q_summary1<- as.tibble(corpus_q_summary)\n\ncorpus_q_summary1<- corpus_q_summary1 %>% \n  mutate(Text =str_replace_all(Text,\"text\", \"Maqwell-text\"))\n\n\ncorpus_qRev_summary1<- as.tibble(corpus_qRev_summary)\n\ncorpus_qRev_summary1<- corpus_qRev_summary1 %>% \n  mutate(Text =str_replace_all(Text,\"text\", \"MaqwellRev-text\"))\n\n\ncorpus_sky_summary1<- as.tibble(corpus_sky_summary)\n\ncorpus_sky_summary1<- corpus_sky_summary1 %>% \n  mutate(Text =str_replace_all(Text,\"text\", \"sky-text\"))\n\n\n\n\nfull <- rbind(corpus_bbc_summary1,corpus_q_summary1,corpus_qRev_summary1,corpus_sky_summary1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_df1 <- rbind(df_bbc,df_q,df_qRev,df_sky)\n\nhead(full_df1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 1\n  text                                                                          \n  <chr>                                                                         \n1 Looking forward to it if you just act normal respect the culture and country …\n2 Honestly every country in the world has done bad. May Allah bless these worke…\n3 if you don't like it stay home and the last people talking about human rights…\n4 So we can’t boycott a football game ( a trivial matter)  to protest a repress…\n5 I see everyone kept their mouth shut with Russia&#;s world cup                \n6 Did Qatar invade any country and kill millions? Who are robbing Africa? Weste…\n```\n:::\n\n```{.r .cell-code}\nfull_corpus <- corpus(full_df1)\nfull_corpus_summary<- summary(full_corpus)\n\n\n\nhead(full_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 6 documents.\ntext1 :\n\"Looking forward to it if you just act normal respect the cul...\"\n\ntext2 :\n\"Honestly every country in the world has done bad. May Allah ...\"\n\ntext3 :\n\"if you don't like it stay home and the last people talking a...\"\n\ntext4 :\n\"So we can’t boycott a football game ( a trivial matter)  to ...\"\n\ntext5 :\n\"I see everyone kept their mouth shut with Russia&#;s world c...\"\n\ntext6 :\n\"Did Qatar invade any country and kill millions? Who are robb...\"\n```\n:::\n:::\n\n\nFor this blog post I will only be analyzing the full corpus.\n\n\n# WordClouds and TF-IDF\n\nMy first step is to discover which words are the most frequent and important in my corpus. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_tokens <- tokens(full_corpus,\n    remove_numbers = T,\n    remove_punct = T,\n    remove_symbols = T)\nfull_tokens <-tokens_tolower(full_tokens)\nfull_tokens <- tokens_select(full_tokens, \n                              pattern = c(stopwords(\"en\"),\"[a-z]\",\"t\",\"m\",\"=\",\"ve\",\"don\",\"s\"),\n                              selection = \"remove\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1245)\n\n\nfull_dfm <- dfm(full_tokens)\n\n\ntextplot_wordcloud(full_dfm, min_count = 10, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThis word clouds depicts the frequency of each word in the corpus. Words like workers, culture, respect, western, rights and human shown in the word cloud imply that the common discussion within these comments could be about the cultural difference between visitors of the world cup and the people who live in Qatar. The word workers being large suggests that the discussion of how workers were treated during the build up of the World Cup is common in the corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_dfm_tfidf <- dfm_tfidf(full_dfm)\n\ntopfeatures(full_dfm_tfidf,50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    world     qatar      quot       cup    people   country      just      like \n 92.14272  84.35479  78.10155  77.33323  71.51482  70.43851  61.26745  60.19609 \n  workers       can countries       one   million      laws     video   respect \n 59.77415  57.09799  55.98868  54.05803  53.04173  51.21717  47.95445  45.45570 \n   rights  football      must     human   culture        go      know      time \n 43.76203  42.69246  41.57572  40.91013  40.85994  40.38009  38.00387  37.63505 \n      war      fifa      even      many      href        🇺🇦       see      work \n 36.76417  35.89342  35.87482  35.64100  35.07385  34.46641  33.62381  33.58795 \n      say       get    russia      west      also   migrant    really      want \n 33.40069  32.66556  32.44401  32.07742  31.96306  31.91179  31.40920  31.22211 \n     make   western       law      much        us     going      good       god \n 30.63771  30.63771  30.39713  29.84849  29.84849  29.04081  28.89646  28.87258 \n    think     great \n 28.55073  28.55073 \n```\n:::\n:::\n\n\nBased on TF_IDF ranking, I pulled the top 50 most important terms from my corpus. Numerically, I can see words like workers, laws, respect, rights, human and culture are ranked highly in my corpus. The wordcloud will show a better depiction.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_wordcloud(full_dfm_tfidf, min_count = 10, random_order = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in dfm_trim.dfm(x, min_termfreq = min_count): dfm has been previously\nweighted\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nincluding could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nhomophobic could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nconstruction could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nbeliefs could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : since\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : major\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : point\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : times\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ndeaths could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nsecond could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ndegrees could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ngermany could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nsecurity could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nsponsor could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nchristianity could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nunited could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nalcohol could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : needs\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nrussian could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : event\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : party\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nbloody could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : happy\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nexcept could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nperfect could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nenglish could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : views\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nsimply could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nhappening could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\npaying could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : south\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : shame\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : topic\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nmigrants could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : day\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nhomosexuality could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nviolations could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : lived\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nabuses could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nislamic could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ncommon could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nnational could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nlittle could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nthough could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : power\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ndeserve could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : two\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ninteresting could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nmentioned could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nthroughout could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ndefinitely could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nimportant could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ninformative could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nvisiting could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : enjoy\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nopportunity could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : gulf\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : face\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ndifferences could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : light\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : woman\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\naccording could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nagency could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : drunk\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, : guy\ncould not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nfuture could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\ncontinued could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(x, min_size, max_size, min_count, max_words, color, :\nopportunities could not be fit on page. It will not be plotted.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe word cloud gives me a better look for which words stand out the most. Still I can see that the main discussion in the corpus is about the human rights violation and respecting the host country based on the top words. \n\n\n# Sentiment Analysis\n\nFor now, the two dictionaries that I am using are NRC and General Inquirer. I chose these because they both evaluate positive and negative sentiment values. In the my next post I will add at least on more dictionary to compare with these two. The dictionary I will use to detect emotions is NRC.\n\n\nHere is used the liwcalike() function, which takes a corpus or character vector and carries out an analysis--based on the provided dictionary (NRC). The LIWC software calculates the percentage of the document that reflects a host of different characteristics. My focus is on positive and negative language. I can evaluate that language using polarity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviewSentiment_nrc <- liwcalike(full_corpus, data_dictionary_NRC)\n\nreviewSentiment_nrc$polarity <- reviewSentiment_nrc$positive - reviewSentiment_nrc$negative\n\nggplot(reviewSentiment_nrc) +\n  geom_histogram(aes(polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfull_corpus[which(reviewSentiment_nrc$polarity < -25)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 3 documents.\ntext20 :\n\"Shame on qatar killing 100000 workers\"\n\ntext329 :\n\"Ruined it\"\n\ntext387 :\n\"Boycott\"\n```\n:::\n:::\n\n\n\nTo check for correctness I looked at the lowest polarity comments. Based on the comments the NRC analysis is classifying the comments correctly for now. These comments have such low polarity because of how short they are. If the sentence is only 3-4 words long containing negative words, the polarity will be very low. I will take this into consideration.\n\n\nUsing Dictionaries with DFMs, I implement the same method but now the values are counts rather than percentages. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nFullDfm_nrc <- tokens(full_corpus,\n                         remove_punct = TRUE,\n                         remove_symbols = TRUE,\n                         remove_numbers = TRUE,\n                         remove_url = TRUE,\n                         split_hyphens = FALSE) %>%\n  tokens_tolower() %>%\n  dfm() %>%\n  dfm_lookup(data_dictionary_NRC)\n\ndf_nrc <- convert(FullDfm_nrc, to = \"data.frame\")\n\ndf_nrc$polarity <- (df_nrc$positive - df_nrc$negative)/(df_nrc$positive + df_nrc$negative)\n\ndf_nrc$polarity[(df_nrc$positive + df_nrc$negative) == 0] <- 0\n\nggplot(df_nrc) +\n  geom_histogram(aes(x=polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwriteLines(head(full_corpus[which(df_nrc$polarity == 1)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLooking forward to it if you just act normal respect the culture and country and you will be ok.  No one needs to know someone is gay just like no one needs to know if you hetero.  Just save that for behind closed doors and make sure it&#;s between consenting adults.  Children and animals are not consenting adults btw you perverts.\nJust respect other countries laws and beliefs.\nWell hope  qp kp  ero  3 code  brIncome  you I&#;m  3 layar brQp   KP   iERO EEE/ibrYOU I&#;M AZ ORGANIZATION\nAn apology won’t bring those dead labour  back alive\nWhen is the west going to respecting other countries without pushing their agenda, I mean we all know this ain’t about the workers sharing rooms or the stadium being moveable ( it’s about the west pushing so called LGBQT) no wonder why only the west condemned Russia and the world didn’t\nArab using western accent .......he even hates hiz own culture 😂😂\n```\n:::\n:::\n\n\nBased on the results of the most positive comments, I can see a mix of comments that are positive and some that are negative. The POV of most of these comments come from people who are from Qatar. They are defending their countries culture whilst calling out western culture. Some of the comments are stigmatized based on some of the language and assumptions drawn.\n\nHere I look at the NRC emotions of the full corpus. The graph below shows words from the corpus that contribute to each sentiment. The graph displays the most common words for each sentiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidytext' was built under R version 4.2.1\n```\n:::\n\n```{.r .cell-code}\nlibrary(textdata)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'textdata' was built under R version 4.2.1\n```\n:::\n\n```{.r .cell-code}\npost_clean <- full_df1 %>%\n  select(text) %>%\n  unnest_tokens(word, text)\n\nsentiment_word_counts <- post_clean %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\nsentiment_word_counts<- sentiment_word_counts %>% \n  filter(word != \"don\")\n\n\nsentiment_word_counts %>%\n  group_by(sentiment) %>%\n  top_n(9) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(title = \"Sentiment terms\",\n       y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe graph below shows the percentage of each emotion in the text plotted as a bar graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns_v <- get_sentences(full_df1$text)\n\nnrc_data <- get_nrc_sentiment(s_v)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `spread_()` was deprecated in tidyr 1.2.0.\nℹ Please use `spread()` instead.\nℹ The deprecated feature was likely used in the syuzhet package.\n  Please report the issue to the authors.\n```\n:::\n\n```{.r .cell-code}\nvalence <- (nrc_data[, 9]*-1) + nrc_data[, 10]\n\nbarplot(\n  sort(colSums(prop.table(nrc_data[, 1:8]))), \n  horiz = TRUE, \n  cex.names = 0.7, \n  las = 1, \n  main = \"Emotions in full corpus\", xlab=\"Percentage\"\n  )\n```\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nTrust and anticipation are the highest values because of the word \"respect\". This could imply that the overarching theme of the comments is trying to convey respect between cultures. \n\nHere I use the dictionary Genreal Inquirer to evalute the polarity of the comments in the corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviewSentiment_genq <- liwcalike(full_corpus, data_dictionary_geninqposneg)\n\nreviewSentiment_genq$polarity <- reviewSentiment_genq$positive - reviewSentiment_genq$negative\n\nggplot(reviewSentiment_genq) +\n  geom_histogram(aes(polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfull_corpus[which(reviewSentiment_genq$polarity < -20)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 4 documents.\ntext6 :\n\"Did Qatar invade any country and kill millions? Who are robb...\"\n\ntext34 :\n\"Radical Country n Radical People\"\n\ntext343 :\n\"hypocrisy and audacity of western civilization is unbelievab...\"\n\ntext356 :\n\"Load of bolllox\"\n```\n:::\n:::\n\n\nHere I see the correct classification of negative comments. I can also see that most of the comment are charged with emotions rather than logical thought. \n\n\nNext, I performed a DFM dictionary analysis using GenInq.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFullDfm_genq <- tokens(full_corpus,\n                         remove_punct = TRUE,\n                         remove_symbols = TRUE,\n                         remove_numbers = TRUE,\n                         remove_url = TRUE,\n                         split_hyphens = FALSE) %>%\n  tokens_tolower() %>%\n  dfm() %>%\n  dfm_lookup(data_dictionary_geninqposneg)\n\ndf_genq <- convert(FullDfm_genq, to = \"data.frame\")\n\ndf_genq$polarity <- (df_genq$positive - df_genq$negative)/(df_genq$positive + df_genq$negative)\n\ndf_genq$polarity[(df_genq$positive + df_genq$negative) == 0] <- 0\n\nggplot(df_genq) +\n  geom_histogram(aes(x=polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwriteLines(head(full_corpus[which(df_genq$polarity == 1)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nif you don't like it stay home and the last people talking about human rights is the west.\nwell................\nJust respect other countries laws and beliefs.\nWell hope  qp kp  ero  3 code  brIncome  you I&#;m  3 layar brQp   KP   iERO EEE/ibrYOU I&#;M AZ ORGANIZATION\nwhy don&#;t the rich arab take ther middeleast people to Labour  .. same culture..\nPeople talking about Qatar bribing FIFA forget that in 1990s USA hosted the world cup. USA will host again next world cup. This is in a country where  Football is not a major sport and is played by kids and women. How do you think US got to host it? 😂😂😂😂😂\n```\n:::\n:::\n\n\nFor the most positive comments I see a mix of positive and negative comments. I also see a lot of comments that are baised against the USA and their beliefs.\n\n# Comparing Dictionaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(df_nrc) <- paste(\"nrc\", colnames(df_nrc), sep = \"_\")\ncolnames(df_genq) <- paste(\"genq\", colnames(df_genq), sep = \"_\")\n\nsent_df <- merge(df_nrc, df_genq, by.x = \"nrc_doc_id\", by.y = \"genq_doc_id\")\n\ncor(sent_df$nrc_polarity, sent_df$genq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4817672\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sent_df, mapping = aes(x = nrc_polarity,\n                              y = genq_polarity)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nHere I compare the similarity between NRC and General Inquirer dictionaries. As the plots make clear, while the measures are  correlated, they are decidedly not identical to one another. We can observe really significant variance across each in the estimates of polarity.\n\n\n# Semantic Network\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_fcm <- fcm(full_dfm)\n\n# keep only top features.\nsmall_fcm <- fcm_select(full_fcm, pattern = names(topfeatures(full_fcm, 53)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_fcm))\n\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_NayanJani_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\nHere I created a Semantic Network of the top 53 terms in the corpus. I see the heart of the network revolves around politcal terms. Thus could imply that some of the policies that are followed or have been created for the world cup have been at the heart of discussion. I also see a sub network in the bottom left that link travelers to law and laws. Thuis implies that people visiting the world cup must follow the laws in place. \n\n# Next Post\n\nFor my next post I want to use topic modeling. I also would like to implement Custom Sentiment Lexicons using key words in context to see which important words that signify political correctness. I also would like to use mixed messages to identify areas of the text where there is emotional ambiguity. I will keep adding comments to my data. \n",
    "supporting": [
      "BlogPost4_NayanJani_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}