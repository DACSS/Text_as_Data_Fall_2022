{
  "hash": "06a8b62ed3d06b457a94e6ef9937883e",
  "result": {
    "markdown": "---\ntitle: \"Post 4\"\nauthor: \"Saaradhaa M\"\ndescription: \"Cleaning Reddit Data; Analysing PDFs and Reddit Data\"\ndate: \"10/29/2022\"\neditor: visual\nformat:\n    html:\n        df-print: paged\n        toc: true\n        code-copy: true\n        code-tools: true\n        css: \"styles.css\"\ncategories: \n  - post 4\n  - saaradhaa\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load libraries.\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(readtext)\nlibrary(phrasemachine)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(phrasemachine): there is no package called 'phrasemachine'\n```\n:::\n\n```{.r .cell-code}\nlibrary(RedditExtractoR)\nlibrary(readr)\n```\n:::\n\n\n## Introduction\n\nIn the first part of the post, I will revise some of the processing and preliminary analysis for the oral histories, which I worked on in the previous blog post. I will then work on generating and cleaning the Reddit data.\n\n## Oral Histories: Cleaning Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# list out files.\nfile_list <- dir(\"~/Desktop/2022_Fall/GitHub/Text_as_Data_Fall_2022/posts/Transcripts\", \n             full.names = TRUE, pattern = \"*.txt\")\n\n# create list of text files.\ntranscripts <- readtext(paste0(file_list), docvarnames = c(\"transcript\", \"text\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in list_files(file, ignore_missing_files, FALSE, cache, verbosity): File '' does not exist.\n```\n:::\n\n```{.r .cell-code}\n# remove references to 'interviewer:' and 'interviewee:', as well as line breaks, '\\n'. I found this website really helpful in testing out regex: https://spannbaueradam.shinyapps.io/r_regex_tester/\ntranscripts$text <- str_replace_all(transcripts$text, \"[a-zA-Z]+:\", \"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\ntranscripts$text <- str_replace_all(transcripts$text, \"\\n\", \"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\n# create 'quanteda' corpus. \noh_corpus <- corpus(transcripts$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(transcripts$text): object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\n# create my own list of stopwords, based on qualitative reading of the first transcript.\nto_keep <- c(\"do\", \"does\", \"did\", \"would\", \"should\", \"could\", \"ought\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"because\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"over\", \"under\", \"no\", \"nor\", \"not\")\nStopwords <- stopwords(\"en\")\nStopwords <- Stopwords[!(Stopwords %in% to_keep)]\n\n# create tokens, remove punctuation, numbers, symbols and stopwords, then convert to lowercase.\noh_tokens <- tokens(oh_corpus, remove_punct = T, remove_numbers = T, remove_symbols = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(oh_corpus, remove_punct = T, remove_numbers = T, remove_symbols = T): object 'oh_corpus' not found\n```\n:::\n\n```{.r .cell-code}\noh_tokens <- tokens_select(oh_tokens, pattern = Stopwords, selection = \"remove\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(oh_tokens, pattern = Stopwords, selection = \"remove\"): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\noh_tokens <- tokens_tolower(oh_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(oh_tokens): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n# remove tokens that appeared in previous wordcloud that are not too meaningful.\noh_tokens <- tokens_remove(oh_tokens, c(\"know\", \"even\", \"used\", \"thing\", \"lot\", \"yes\", \"say\", \"going\", \"went\", \"yeah\", \"come\", \"actually\", \"mean\", \"like\", \"think\", \"get\", \"go\", \"said\", \"see\", \"things\", \"really\", \"well\", \"still\", \"little\", \"got\", \"right\", \"can\", \"came\", \"um\", \"quite\", \"bit\", \"every\", \"oh\", \"many\", \"always\", \"one\", \"two\", \"just\", \"much\", \"want\", \"wanted\", \"okay\", \"part\", \"also\", \"just\", \"us\", \"never\", \"many\", \"something\", \"want\", \"wanted\", \"uh\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"remove\"): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n# get summary of corpus.\noh_summary <- summary(oh_corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(oh_corpus): object 'oh_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# add metadata on interview phase and interviewee gender.\noh_summary$phase <- c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in oh_summary$phase <- c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, : object 'oh_summary' not found\n```\n:::\n\n```{.r .cell-code}\noh_summary$gender <- c(1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 3, 1, 2, 2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in oh_summary$gender <- c(1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, : object 'oh_summary' not found\n```\n:::\n\n```{.r .cell-code}\n# create metadata.\ndocvars(oh_corpus) <- oh_summary\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'oh_summary' not found\n```\n:::\n:::\n\n\n## Oral Histories: Text Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# re-create wordcloud with modified data.\ndfm <- oh_tokens %>% dfm() %>% dfm_trim(min_termfreq = 10, verbose = FALSE, min_docfreq = .1, docfreq_type = \"prop\") %>% dfm_remove(stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(.): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm, max_words=50, color=\"purple\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: textplot_wordcloud() only works on dfm, keyness objects.\n```\n:::\n:::\n\n\nI changed up the wordcloud by removing some tokens that were not as meaningful, and this seems to paint a much clearer picture of what the interviews were about - work, music, college, family, friends, and moving to a new country.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create fcm.\nfcm <- fcm(dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: fcm() only works on character, corpus, dfm, tokens objects.\n```\n:::\n\n```{.r .cell-code}\n# keep only top features.\nsmall_fcm <- fcm_select(fcm, pattern = names(topfeatures(fcm, 50)), selection = \"keep\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: fcm_select() only works on fcm objects.\n```\n:::\n\n```{.r .cell-code}\n# compute weights.\nsize <- log(colSums(small_fcm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'colSums': object 'small_fcm' not found\n```\n:::\n\n```{.r .cell-code}\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_network(small_fcm, vertex_size = size/max(size) * 4): object 'small_fcm' not found\n```\n:::\n:::\n\n\nThe network plot also looks a lot better after removing symbols; it's much more inter-connected. It looks like many interviews referenced Seattle, which makes sense, since the interviews were conducted by UW. Lots of references to work, school and university, which might reflect (1) South Asian cultural focus on these themes, or (2) possibly many of the migrants moved to the US for these purposes.\n\n## Oral Histories: N-Grams\n\nSince the oral histories were collected over a number of years, there did not seem to be a uniform structure that they followed. Hence, I will randomly select a subset of transcripts, and then try to identify common phrases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# randomly choose 10 out of 41 transcripts.\nset.seed(1)\nnumbers <- sample(1:41, 10, replace=FALSE)\ndocuments <- transcripts$text[numbers]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\n# generate phrases.\nphrases <- phrasemachine(documents, minimum_ngram_length = 2, maximum_ngram_length = 4, return_phrase_vectors = TRUE, return_tag_sequences = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in phrasemachine(documents, minimum_ngram_length = 2, maximum_ngram_length = 4, : could not find function \"phrasemachine\"\n```\n:::\n\n```{.r .cell-code}\nphrases[[1]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[2]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[3]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[4]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[5]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[6]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[7]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[8]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[9]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n\n```{.r .cell-code}\nphrases[[10]]$phrases[300:320]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n:::\n\n\nFrom the small selection of n-grams above, it seems like some topics that might arise are memories from South Asia, what prompted the move to the US (education, work), travel, experiences in the US, etc.\n\n**How do I preserve sequences of two or four numbers in my tokens? These might reference years in history, which might be important to understand some topics that come up.**\n\n## Reddit: Generating Data\n\nThe previous time I tried to load the Reddit data, it just didn't work. I'm going to try to leave RStudio alone for a longer time this time around and see if that might do the trick instead. Much of the code below is commented out, since it need not be re-run when rendering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate top 1000 posts.\n# content_top <- find_thread_urls(subreddit=\"ABCDesis\",period=\"all\",sort_by=\"top\")\n# content_top <- content_top[-1,]\n# content_top$type <- \"top\"\n# saveRDS(content_top, \"content_top.rds\")\n# content_top <- read_rds(\"content_top.rds\")\n\n# attempting to generate comments alone first - this took ~ 1hr.\n# url_content <- get_thread_content(content$url[1:1000])$comments$comment\n# saveRDS(url_content, \"url_content.rds\")\n# url_content_top <- read_rds(\"url_content.rds\")\n\n# i decided to try to get additional info on the comments as well.\n# url_content_info <- get_thread_content(content$url[1:1000])$comments\n# saveRDS(url_content_info, \"url_content_info.rds\")\n# url_content_info_top <- read_rds(\"url_content_info.rds\")\n# url_content_info_top$type <- \"top\"\n# saveRDS(url_content_info_top, \"url_content_info_top.rds\")\n# comments_top <- read_rds(\"url_content_info_top.rds\")\n\n# dr. song also wanted me to try generating more than 1000 posts, so i'm gonna try to create tables for 'hot' and 'rising' as well, not just 'top'.\n# content_hot <- find_thread_urls(subreddit=\"ABCDesis\",period=\"all\",sort_by=\"hot\")\n# content_hot$id <- 1:991\n\n# check duplicates with top posts, then remove them.\n# duplicates <- inner_join(content_hot, content_top)\n# remove_list <- duplicates$id\n# content_hot <- content_hot %>% slice(-c(remove_list))\n\n# save object.\n# content_hot$type <- \"hot\"\n# saveRDS(content_hot, \"content_hot.rds\")\n# content_hot <- read_rds(\"content_hot.rds\")\n\n# generate comments for 'hot' posts.\n# comments_hot <- get_thread_content(content_hot$url[1:936])$comments\n# comments_hot$type <- \"hot\"\n# saveRDS(comments_hot, \"comments_hot.rds\")\n# comments_hot <- read_rds(\"comments_hot.rds\")\n\n# ok, i'm not going to add rising' or 'new', because 'rising' only had 24 posts, and 'new' had many duplicates with 'hot'. so i'll just be using 'hot' and 'top' posts.\n\n# combine 'hot' and 'top' posts into 1 dataframe.\n# content_hot <- content_hot[,-8]\n# reddit_posts <- rbind(content_hot, content_top)\n# saveRDS(reddit_posts, \"reddit_posts.rds\")\nreddit_posts <- read_rds(\"reddit_posts.rds\")\n\n# combine 'hot' and 'top' comments into 1 dataframe.\n# reddit_comments <- rbind(comments_hot, comments_top)\n# saveRDS(reddit_comments, \"reddit_comments.rds\")\nreddit_comments <- read_rds(\"reddit_comments.rds\")\n```\n:::\n\n\nOk, this is a quick summary of what I've done above - I generated both the 'top' and 'hot' posts of all time from the subreddit, as well as all the comments on the posts, and some associated information. I then combined them into two dataframes - one with all the posts (*N* = 1,936) and another with all the comments (*N* = 110,967). I'm going to clean up the two dataframes in the following code chunk.\n\n## Reddit: Cleaning Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check dataframe of comments.\nglimpse(reddit_comments)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 110,967\nColumns: 11\n$ url        <chr> \"https://www.reddit.com/r/ABCDesis/comments/x6oivc/what_are…\n$ author     <chr> \"jubeer\", \"day1222\", \"ninithebeanie\", \"skinnybrownhippie\", …\n$ date       <chr> \"2022-09-06\", \"2022-09-06\", \"2022-09-06\", \"2022-09-06\", \"20…\n$ timestamp  <dbl> 1662424047, 1662427654, 1662445516, 1662479140, 1662475081,…\n$ score      <dbl> 1, 1, 1, 1, 1, 1, 5, 1, -4, 1, 1, 1, 1, -1, 0, 29, 15, 10, …\n$ upvotes    <dbl> 1, 1, 1, 1, 1, 1, 5, 1, -4, 1, 1, 1, 1, -1, 0, 29, 15, 10, …\n$ downvotes  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"The Baljeet meme cut\", \"Honestly works for a lot of brown …\n$ comment_id <chr> \"1\", \"1_1\", \"1_2\", \"2\", \"1\", \"2\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ type       <chr> \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"ho…\n```\n:::\n\n```{.r .cell-code}\n# remove 'upvotes' and 'downvotes' as they haves no useful info.\nreddit_comments <- reddit_comments %>% select (-c(upvotes, downvotes))\n\n# remove 'deleted' / 'removed' comments.\nreddit_comments$comment <- str_replace_all(reddit_comments$comment, \"\\\\[deleted\\\\]\", \"\")\nreddit_comments$comment <- str_replace_all(reddit_comments$comment, \"\\\\[ Removed by Reddit \\\\]\", \"\")\nreddit_comments <- reddit_comments[!(reddit_comments$comment==\"\"), ]\n\n# create a vectorized function to remove html elements from comments.\n# vectorized_htmldecode <- Vectorize(HTMLdecode)\n\n# run the function - this is commented out because it took a really long time. \n# reddit_comments$comment <- vectorized_htmldecode(reddit_comments$comment)\n\n# i'm going to save the object so i don't have to re-do this.\n# saveRDS(reddit_comments, \"reddit_comments_final.rds\")\n\n# re-load the object.\nreddit_comments_final <- read_rds(\"reddit_comments_final.rds\")\n\n# ok, now check the dataframe of posts.\nglimpse(reddit_posts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,936\nColumns: 8\n$ date_utc  <chr> \"2022-09-05\", \"2022-09-06\", \"2022-09-05\", \"2022-09-05\", \"202…\n$ timestamp <dbl> 1662405203, 1662431521, 1662394101, 1662350088, 1662419582, …\n$ title     <chr> \"What are some good hairstyles for guy that does have his be…\n$ text      <chr> \"\", \"\", \"\\n\\n[View Poll](https://www.reddit.com/poll/x6jxfg)…\n$ subreddit <chr> \"ABCDesis\", \"ABCDesis\", \"ABCDesis\", \"ABCDesis\", \"ABCDesis\", …\n$ comments  <dbl> 4, 2, 9, 25, 12, 16, 4, 3, 0, 0, 4, 80, 9, 0, 85, 15, 24, 11…\n$ url       <chr> \"https://www.reddit.com/r/ABCDesis/comments/x6oivc/what_are_…\n$ type      <chr> \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot…\n```\n:::\n\n```{.r .cell-code}\n# remove 'subreddit' column.\nreddit_posts <- reddit_posts %>% select (-c(subreddit))\n\n# remove html elements from 'title' column.\n# reddit_posts$title <- vectorized_htmldecode(reddit_posts$title)\n\n# remove html elements from 'text' column.\n# reddit_posts$text <- vectorized_htmldecode(reddit_posts$text)\n\n# save the finalised posts object.\n# saveRDS(reddit_posts, \"reddit_posts_final.rds\")\n\n# re-load the object.\nreddit_posts_final <- read_rds(\"reddit_posts_final.rds\")\n```\n:::\n\n\n## Reddit: Text Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create 'quanteda' corpus. \nreddit_corpus <- corpus(reddit_comments_final$comment)\n\n# use same list of stopwords as oral histories.\nto_keep <- c(\"do\", \"does\", \"did\", \"would\", \"should\", \"could\", \"ought\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"because\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"over\", \"under\", \"no\", \"nor\", \"not\")\nStopwords <- stopwords(\"en\")\nStopwords <- Stopwords[!(Stopwords %in% to_keep)]\n\n# create tokens, remove punctuation, numbers, symbols and stopwords, then convert to lowercase.\nreddit_tokens <- tokens(reddit_corpus, remove_punct = T, remove_numbers = T, remove_symbols = T)\nreddit_tokens <- tokens_select(reddit_tokens, pattern = Stopwords, selection = \"remove\")\nreddit_tokens <- tokens_tolower(reddit_tokens)\n\n# remove tokens that appear in wordcloud that are not too meaningful.\nreddit_tokens <- tokens_remove(reddit_tokens, c(\"m\", \"s\", \"say\", \"see\", \"make\", \"way\", \"well\", \"many\", \"someone\", \"never\", \"still\", \"now\", \"go\", \"thing\", \"things\", \"know\", \"think\", \"also\", \"said\", \"going\", \"want\", \"t\", \"one\", \"lot\", \"much\", \"even\", \"really\", \"sure\", \"yeah\", \"look\", \"always\", \"something\", \"re\", \"actually\", \"get\", \"got\", \"though\", \"take\", \"etc\", \"can\", \"like\", \"don\", \"saying\"))\n\n# get summary of corpus.\nreddit_summary <- summary(reddit_corpus)\n\n# create wordcloud with modified data.\nreddit_dfm <- reddit_tokens %>% dfm() %>% dfm_remove(stopwords(\"en\"))\ntextplot_wordcloud(reddit_dfm, max_words = 50, color=\"purple\")\n```\n\n::: {.cell-output-display}\n![](Post4_Saaradhaa_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm.\nreddit_fcm <- fcm(reddit_dfm)\n\n# keep only top features.\nsmall_reddit_fcm <- fcm_select(reddit_fcm, pattern = names(topfeatures(reddit_fcm, 50)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_reddit_fcm))\n\n# create network.\ntextplot_network(small_reddit_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output-display}\n![](Post4_Saaradhaa_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\nThe wordcloud and network plot allow me to confirm that this subreddit indeed discusses South Asian culture, hence making it a good comparison with the oral histories data. We can also see that the younger generation has concerns specific to their age group - such as with dating and relationships, a wider range of ethnic and national groups (Asians, Black people, White people, British, etc), racism (which was not in the text plots for the older generation) and even caste concerns. This gives me some idea of how the values and concerns of South Asians have changed over time, which is my first research question.\n\n## Comparing the Two Datasets\n\nI will attempt to use str_match() to compare how many times 'culture' appears in the oral histories vs. Reddit data (as a proportion of the total number of tokens).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute proportions for reddit data.\nsum(str_count(reddit_tokens, \"[cC]ultur\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8047\n```\n:::\n\n```{.r .cell-code}\nsum(ntoken(reddit_tokens))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2029966\n```\n:::\n\n```{.r .cell-code}\n# compute proportions for oral histories.\nsum(str_count(oh_tokens, \"[cC]ultur\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_count_regex(string, pattern, opts_regex = opts(pattern)): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\nsum(ntoken(oh_tokens))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ntoken(oh_tokens): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n# test proportions.\nprop.test(x = c(8047, 502), n = c(2029966, 206015), p = NULL, alternative = \"two.sided\", correct = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  c(8047, 502) out of c(2029966, 206015)\nX-squared = 114.16, df = 1, p-value < 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.00129494 0.00175984\nsample estimates:\n     prop 1      prop 2 \n0.003964106 0.002436716 \n```\n:::\n:::\n\n\nInteresting - the younger generation referenced culture significantly more than the older generation did.\n\n## Moving Forward\n\nThis is my tentative plan for the next two posts, which I might revise depending on time constraints.\n\n-   Post 5: try Personal Values Dictionary on oral histories and Reddit data.\n\n-   Post 6: do topic modelling on oral histories and Reddit data.\n",
    "supporting": [
      "Post4_Saaradhaa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}