{
  "hash": "1c088fdaf3d42c0988852643a0cd31c2",
  "result": {
    "markdown": "---\ntitle: \"CaitlinRowley - Blog Post 5\"\nauthor: \"Caitlin Rowley\"\neditor: visual\n---\n\n\nI have switched data sets for future blog posts. I will be using open-ended survey responses from a 1996 study titled *Survey of Gun Owners in the United States*.\n\nFor the study, respondents were asked six qualifying questions related to: (1) gun ownership, (2) gun-carrying practices, (3) gun display against the respondent, (4) gun use in self-defense against animals, (5) gun use in self-defense against people, and (6) other weapons used in self-defense. A \"yes\" response to a qualifying question led to a series of additional questions on the same topic as the qualifying question.\n\nThe open-ended responses include descriptions specifically related to the following questions: (1) where the respondent was when he or she displayed a gun (in self-defense or otherwise), (2) specific reasons why the respondent displayed a gun, (3) how the other individual reacted when the respondent displayed the gun, (4) how the individual knew the respondent had a gun, (5) whether the police were contacted for specific self-defense events, and (6) if not, why not.\n\nI will focus on the following research question: Can we identify the most common circumstance in which respondents displayed a gun? I am hoping that \"circumstance\" can include references to both the catalyst in the situation and the environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages:\n\ninstall.packages(\"RColorBrewer\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstalling package into 'C:/Users/caitr/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n```\n:::\n\n```{.r .cell-code}\ninstall.packages(\"stopwords\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstalling package into 'C:/Users/caitr/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n```\n:::\n\n```{.r .cell-code}\n# load libraries: \n\nlibrary(readr)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyverse' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"mMatrix\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"replValueSp\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 4 of 4 threads used.\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n:::\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\nlibrary(wordcloud)\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidytext' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stopwords)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stopwords' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta<-\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\nAttaching package: 'tm'\n\nThe following object is masked from 'package:stopwords':\n\n    stopwords\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nlibrary(tokenizers)\n\n# read in data:\n\nSurvey <- read_csv(\"C:\\\\Users\\\\caitr\\\\OneDrive\\\\Documents\\\\DACSS\\\\DACSS 679\\\\DACSS 697\\\\DACSS 697 - Survey.csv\", col_names = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 814 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Q#, VERBATIM RESPONSE\ndbl (2): ID#, CODE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n#rename columns:\n\nnames(Survey) <- c('Respondent_ID','Survey_Question', 'Code', 'Open_Ended_Response')\nprint(Survey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 814 × 4\n   Respondent_ID Survey_Question  Code Open_Ended_Response                      \n           <dbl> <chr>           <dbl> <chr>                                    \n 1           250 12                  0 \"I had it in my vehicle to go hunting, n…\n 2           248 28                  0 \"None of the above.  I was an employee w…\n 3           250 6                   1 \"Four of them.\"                          \n 4           250 6                   1 \"Four of them.\"                          \n 5           250 7                   1 \"My shotgun.\"                            \n 6           250 7                   1 \"My shotgun.\"                            \n 7           428 7                   1 \"{He said \\\"no\\\" to fully automatic weap…\n 8           409 8                   1 \"If they were used incorrectly.\"         \n 9          1165 9                   1 \"Well, yeah, community property.\"        \n10           250 12                  1 \"I had it in my vehicle to go hunting, n…\n# … with 804 more rows\n```\n:::\n\n```{.r .cell-code}\n# remove duplicate observations: \n\nSurvey_unique <- distinct(Survey)\n\nSurvey_unique %>% \n    separate_rows(Open_Ended_Response) %>% \n    distinct() %>%\n    nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8134\n```\n:::\n:::\n\n\nThere are 812 rows in this data frame and 4 columns. The four columns represent the following variables: survey respondent ID ('Respondent_ID), survey question ('Survey_Question'), response code ('Code'), and verbatim open-ended responses ('Open_Ended_Responses'). Each of the 812 rows now represents a unique observation. Additionally, there are 8134 unique words across all open-ended responses.\n\nI will next create a corpus and remove both capitalization and punctuation from all open-ended responses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a corpus:\n\nSurvey_corpus <- corpus(Survey_unique$Open_Ended_Response)\nhead(Survey_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 6 documents.\ntext1 :\n\"I had it in my vehicle to go hunting, not on my person.\"\n\ntext2 :\n\"None of the above.  I was an employee working in a convenien...\"\n\ntext3 :\n\"Four of them.\"\n\ntext4 :\n\"My shotgun.\"\n\ntext5 :\n\"{He said \"no\" to fully automatic weapons but \"yes\" to semi-a...\"\n\ntext6 :\n\"If they were used incorrectly.\"\n```\n:::\n\n```{.r .cell-code}\n# tokenize, remove capitalization and punctuation:\n\nSurvey_tokens <- tokens(Survey_corpus, \n    remove_punct = T)\nSurvey_tokens <- tokens_tolower(Survey_tokens)\nhead(Survey_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 6 documents.\ntext1 :\n [1] \"i\"       \"had\"     \"it\"      \"in\"      \"my\"      \"vehicle\" \"to\"     \n [8] \"go\"      \"hunting\" \"not\"     \"on\"      \"my\"     \n[ ... and 1 more ]\n\ntext2 :\n [1] \"none\"        \"of\"          \"the\"         \"above\"       \"i\"          \n [6] \"was\"         \"an\"          \"employee\"    \"working\"     \"in\"         \n[11] \"a\"           \"convenience\"\n[ ... and 1 more ]\n\ntext3 :\n[1] \"four\" \"of\"   \"them\"\n\ntext4 :\n[1] \"my\"      \"shotgun\"\n\ntext5 :\n [1] \"he\"             \"said\"           \"no\"             \"to\"            \n [5] \"fully\"          \"automatic\"      \"weapons\"        \"but\"           \n [9] \"yes\"            \"to\"             \"semi-automatic\" \"he\"            \n[ ... and 28 more ]\n\ntext6 :\n[1] \"if\"          \"they\"        \"were\"        \"used\"        \"incorrectly\"\n```\n:::\n\n```{.r .cell-code}\n# remove stopwords separately: \n\nSurvey_tokens_stopwords <- tokens_select(Survey_tokens,\n                                       pattern = stopwords(\"en\"),\n                                       selection = \"remove\")\n```\n:::\n\n\nI will next extract features from my corpus by creating a document feature matrix. From there, I will be able to generate data visualizations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create document feature matrix:\n\nSurvey_tokens <- dfm(Survey_tokens_stopwords)\n\n# identify 10 most common words:\n\ntopfeatures(Survey_dfm, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(Survey_dfm, 10): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\n# create a wordcloud:\n\ntextplot_wordcloud(Survey_dfm, min_count = 5, max_words = 50, random_order = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud(Survey_dfm, min_count = 5, max_words = 50, : object 'Survey_dfm' not found\n```\n:::\n:::\n\n\nNext, I will generate a feature co-occurrence matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a dfm that is limited to words that appear frequently (more than 30% of responses).\n\nSurvey_dfm_freq <- dfm_trim(Survey_dfm, min_termfreq = 30)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(Survey_dfm, min_termfreq = 30): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nSurvey_dfm_freq <- dfm_trim(Survey_dfm_freq, min_docfreq = .3, docfreq_type = \"prop\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(Survey_dfm_freq, min_docfreq = 0.3, docfreq_type = \"prop\"): object 'Survey_dfm_freq' not found\n```\n:::\n\n```{.r .cell-code}\n# use dfm to create a feature co-occurring matrix:\n\nSurvey_fcm <- fcm(Survey_dfm_freq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm(Survey_dfm_freq): object 'Survey_dfm_freq' not found\n```\n:::\n\n```{.r .cell-code}\n# check dimensions:\n\ndim(Survey_fcm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_fcm' not found\n```\n:::\n:::\n\n\nQUESTION: The first time I ran this chunk of code, the dimensions of the feature co-occurring matrix were shown to be 395 rows by 395 columns. After re-running the codes after reopening R, the dimensions are now showing as 0 rows by 0 columns.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nNext, I will apply the correlated topic modeling approach to my data set, since I do not have a covariate such as sentiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages:\n\nlibrary(stm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stm' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nstm v1.3.6 successfully loaded. See ?stm for help. \n Papers, resources, and other materials at structuraltopicmodel.com\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n\n# generate data frame:\n\nSurvey_dfm_clean <- dfm(Survey_dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(Survey_dfm): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\n             tolower = TRUE\n             remove = stopwords(\"en\")\n             remove_punct = TRUE\ndim(Survey_dfm_clean)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_dfm_clean' not found\n```\n:::\n:::\n\n\nThe dimensions of my data frame are 812 rows and 1224 columns. Next, I will generate\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate correlated topic model:\n\ncor_topic_model <- stm(Survey_dfm, K = 5,\n                       verbose = FALSE, init.type = \"Spectral\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in asSTMCorpus(documents, vocab, data): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\ncor_topic_model\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'cor_topic_model' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(cor_topic_model)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(cor_topic_model): object 'cor_topic_model' not found\n```\n:::\n\n```{.r .cell-code}\n# label topics:\n# frex = words that are both frequent and exclusive to topic.\n# lift = calculated by dividing the topic-word distribution by the empirical word count probability distribution.\n\nlabelTopics(cor_topic_model)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(cor_topic_model): object 'cor_topic_model' not found\n```\n:::\n:::\n\n\nI now have a topic model with 5 topics, 805 documents, and a 1224-word dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify document most frequently associated with the five topics:\n\nfindThoughts(cor_topic_model,\n             texts = Survey_dfm$Open_Ended_Responses,\n             topics = c(1:5),\n             n = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findThoughts(cor_topic_model, texts = Survey_dfm$Open_Ended_Responses, : object 'cor_topic_model' not found\n```\n:::\n:::\n\n\nI am having a little trouble with this code; my understanding is that it's intended to yield the document most frequently associated with the five topics, but I am seeing a different, less informative output. I will work more on this.\n\nQUESTION: Do I need to identify question numbers associated with location/reason for drawing a weapon and apply this as a covariate? Do I need a covariate? If so, how do I identify one when all open-ended responses are aggregated? I.e., the 'Survey_Question' variable is the only indicator for response categories, so there isn't necessarily another variable to choose from in terms of identifying a predictor.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nTry structural topic modeling (without predictor). My thought is that these results may look similar to the correlation topic model since I am not adding a predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# choose the number of topics:\n\nk <- 5\n\n# specify model:\n\nSurvey_STM <- stm(Survey_dfm,\n               K = k,\n               data = Survey_dfm$Open_Ended_Responses,\n               max.em.its = 1224,\n               seed = 1234,\n               init.type = \"Spectral\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in asSTMCorpus(documents, vocab, data): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nlabelTopics(Survey_STM)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(Survey_STM): object 'Survey_STM' not found\n```\n:::\n:::\n\n\nTry some visualization to capture the estimated frequency of words across topics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Survey_STM, type = \"summary\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(Survey_STM, type = \"summary\"): object 'Survey_STM' not found\n```\n:::\n:::\n\n\nNext, I want to try extracting topics and assigning them to the vector of document proportions. Extract the top words (as identified by 'frex'), collapse the strings, and separate the tokens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the words:\n\nSurvey_words <- labelTopics(Survey_STM, n=5)$frex\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(Survey_STM, n = 5): object 'Survey_STM' not found\n```\n:::\n\n```{.r .cell-code}\n# set up an empty vector:\n\nSurvey_topic_labels <- rep(NA, k)\n\n# set up a loop to go through the topics and collapse the words to a single label:\n\nfor(i in 1:k){Survey_topic_labels[i] <- paste(Survey_words[i,], collapse = \"_\")}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in paste(Survey_words[i, ], collapse = \"_\"): object 'Survey_words' not found\n```\n:::\n\n```{.r .cell-code}\n# print the labels:\n\nSurvey_topic_labels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA NA NA NA NA\n```\n:::\n:::\n\n\nWithout a predictor or covariate, I am unable to measure the effect on topic distributions. Identifying a way to incorporate a covariate or predictor will be my next task.\n\nI may also try a K-Means analysis.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}