{
  "hash": "4521f501e42207d288c67b15d547e691",
  "result": {
    "markdown": "---\ntitle: \"Blog Post Template\"\nauthor: \"Your Name\"\ndesription: \"Something to describe what I did\"\ndate: \"08/02/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw1\n  - challenge1\n  - my name\n  - dataset\n  - ggplot2\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\nlibrary(stm)\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tm)\nlibrary(tidytext)\nlibrary(ggplot2)\nknitr::opts_chunk$set(echo=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- read_csv('./100FamousPeople_new.csv')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 100 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): people_names, peoples_title, content, from, to, profession, country... dbl\n(2): ...1, ...2\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n• `...1` -> `...2`\n```\n:::\n\n```{.r .cell-code}\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n   ...1  ...2 people_names    peopl…¹ content from  to    profe…² country gender\n  <dbl> <dbl> <chr>           <chr>   <chr>   <chr> <chr> <chr>   <chr>   <chr> \n1     1     1 Abraham Lincoln us pre… “With … 1809   1865 politi… america male  \n2     2     2 Adolf Hitler    leader… Adolf … 1889   1945 politi… germany male  \n3     3     3 Albert Einstein german… Born i… 1879   1955 academ… germany male  \n4     4     4 Alfred Hitchco… englis… Sir Al… 4      40   entert… america male  \n5     5     5 Amelia Earhart… aviator Amelia… 1897  1937  others  others  female\n6     6     6 Angelina Jolie  actres… Angeli… 1975  2022  entert… others  female\n# … with abbreviated variable names ¹​peoples_title, ²​profession\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(dataset$profession)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     academia        artist   businessman entertainment  humanitarian \n           11            14             7             7             7 \n       others    politician       royalty     spiritual        sports \n            8            28             5             3            10 \n```\n:::\n\n```{.r .cell-code}\nartist_df <- dataset %>% filter(profession == 'artist') %>% \n  select(people_names, content, country, gender)\n```\n:::\n\n\n\n# Pre processing\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_processed <- textProcessor(documents = artist_df$content, \n                           metadata = artist_df,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_prepped <- prepDocuments(documents = artist_processed$documents,\n                         vocab = artist_processed$vocab,\n                         meta = artist_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 1909 of 2455 terms (2328 of 4978 tokens) due to frequency \nYour corpus now has 14 documents, 546 terms and 2650 tokens.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_prepped$vocab[1:30]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"abl\"      \"accept\"   \"acclaim\"  \"act\"      \"admir\"    \"adopt\"   \n [7] \"affair\"   \"age\"      \"aid\"      \"album\"    \"also\"     \"although\"\n[13] \"alway\"    \"america\"  \"american\" \"anoth\"    \"anyth\"    \"appear\"  \n[19] \"around\"   \"art\"      \"arthur\"   \"artist\"   \"aspect\"   \"attempt\" \n[25] \"attend\"   \"attent\"   \"attitud\"  \"august\"   \"award\"    \"away\"    \n```\n:::\n:::\n\n\n# Estimate basic model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_basicmodel <- stm(documents = artist_prepped$documents,\n                         vocab = artist_prepped$vocab,\n                         data = artist_prepped$meta,\n                         K=8,\n                         verbose = F)\n```\n:::\n\n\n# Inspect topic words\n\n::: {.cell}\n\n```{.r .cell-code}\nlabelTopics(artist_basicmodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTopic 1 Top Words:\n \t Highest Prob: beatl, paul, record, lennon, new, music, becam \n \t FREX: paul, beatl, club, break, new, lennon, record \n \t Lift: paul, recognis, club, split, better, chariti, support \n \t Score: paul, beatl, lennon, john, record, club, band \nTopic 2 Top Words:\n \t Highest Prob: music, record, sing, also, perform, american, becam \n \t FREX: sing, american, voic, standard, pop, roll, perform \n \t Lift: standard, roll, depend, civil, voic, impress, sing \n \t Score: standard, pop, american, voic, record, song, sing \nTopic 3 Top Words:\n \t Highest Prob: fashion, war, women, pari, life, becom, world \n \t FREX: fashion, women, pari, war, post, dress, enabl \n \t Lift: fashion, women, pari, post, wealthi, war, three \n \t Score: fashion, women, war, pari, post, wealthi, german \nTopic 4 Top Words:\n \t Highest Prob: world, money, concert, lead, becam, live, music \n \t FREX: money, concert, lead, organis, world, rais, rock \n \t Lift: money, lead, aid, organis, concert, invit, thousand \n \t Score: money, organis, intern, lead, aid, concert, rais \nTopic 5 Top Words:\n \t Highest Prob: one, french, took, first, live, act, german \n \t FREX: took, offic, act, french, role, fame, german \n \t Lift: offic, paid, coupl, role, sensual, fame, effort \n \t Score: offic, french, german, media, fame, danc, paid \nTopic 6 Top Words:\n \t Highest Prob: music, album, record, career, peopl, perform, time \n \t FREX: album, member, number, want, solo, produc, adopt \n \t Lift: four, member, album, launch, number, produc, varieti \n \t Score: member, album, solo, number, produc, record, hit \nTopic 7 Top Words:\n \t Highest Prob: great, work, life, one, music, also, artist \n \t FREX: lisa, great, art, view, religi, will, univers \n \t Lift: lisa, note, reflect, understand, view, univers, greatest \n \t Score: lisa, artist, god, view, religi, greatest, art \nTopic 8 Top Words:\n \t Highest Prob: lennon, beatl, john, music, peac, also, year \n \t FREX: lennon, john, peac, beatl, god, love, solo \n \t Lift: peac, john, lennon, beatl, potenti, son, enter \n \t Score: peac, lennon, john, beatl, solo, god, band \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.STM(artist_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n# word clouds\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_basicmodel,\n      topic=8,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n# understanding topics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindThoughts(model = artist_basicmodel,\n             texts = artist_prepped$meta$description,\n             topics = 5,\n             n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Topic 5: \n \t \n```\n:::\n:::\n\n\n\n# Politicians\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_df <- dataset %>% filter(profession == 'politician') %>% \n  select(people_names,content)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_processed <- textProcessor(documents = politician_df$content, \n                           metadata = politician_df,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\npolitician_prepped <- prepDocuments(documents = politician_processed$documents,\n                         vocab = politician_processed$vocab,\n                         meta = politician_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 3111 of 4423 terms (3788 of 12445 tokens) due to frequency \nYour corpus now has 28 documents, 1312 terms and 8657 tokens.\n```\n:::\n\n```{.r .cell-code}\npolitician_basicmodel <- stm(documents = politician_prepped$documents,\n                         vocab = politician_prepped$vocab,\n                         data = politician_prepped$meta,\n                         K=8,\n                         verbose = F)\n\n\nplot.STM(politician_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncloud(stmobj = politician_basicmodel,\n      topic=8,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfindThoughts(model = politician_basicmodel,\n             texts = politician_basicmodel$meta,\n             topics = 5,\n             n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Topic 5: \n \t \n```\n:::\n\n```{.r .cell-code}\nfindingK <- searchK(politician_prepped$documents,\n                    politician_prepped$vocab,\n                    K=c(2,5,10,15),\n                    data=politician_prepped$meta,\n                    verbose=FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in stm(documents = heldout$documents, vocab = heldout$vocab, K = k, :\nK=2 is equivalent to a unidimensional scaling model which you may prefer.\n```\n:::\n\n```{.r .cell-code}\nplot(findingK)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n\n```{.r .cell-code}\npolitician_prepped_corr <- topicCorr(politician_basicmodel)$cor\npolitician_prepped_corr[politician_prepped_corr < 0] <- 0\n\npolitician_prepped_igraph <- graph.adjacency(politician_prepped_corr,\n                                             mode=\"undirected\",\n                                             weighted=T,\n                                             diag=F)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in graph.adjacency(politician_prepped_corr, mode = \"undirected\", : could not find function \"graph.adjacency\"\n```\n:::\n\n```{.r .cell-code}\npolitician_prepped_labels <- apply(labelTopics(politician_basicmodel)$prob, 1,paste, collapse=\"\\n\")\n\nplot.igraph(politician_prepped_igraph, \n            vertex.label = politician_prepped_labels,\n            vertex.size = colMeans(politician_basicmodel$theta)*200,\n            edge.width = E(politician_prepped_igraph)$weight *50)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot.igraph(politician_prepped_igraph, vertex.label = politician_prepped_labels, : could not find function \"plot.igraph\"\n```\n:::\n:::\n\n\n# British Academia\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n   ...1  ...2 people_names    peopl…¹ content from  to    profe…² country gender\n  <dbl> <dbl> <chr>           <chr>   <chr>   <chr> <chr> <chr>   <chr>   <chr> \n1     1     1 Abraham Lincoln us pre… “With … 1809   1865 politi… america male  \n2     2     2 Adolf Hitler    leader… Adolf … 1889   1945 politi… germany male  \n3     3     3 Albert Einstein german… Born i… 1879   1955 academ… germany male  \n4     4     4 Alfred Hitchco… englis… Sir Al… 4      40   entert… america male  \n5     5     5 Amelia Earhart… aviator Amelia… 1897  1937  others  others  female\n6     6     6 Angelina Jolie  actres… Angeli… 1975  2022  entert… others  female\n# … with abbreviated variable names ¹​peoples_title, ²​profession\n```\n:::\n\n```{.r .cell-code}\nacademia_df <- dataset %>%  filter(`profession` == 'academia' & `country` == 'british')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nacademia_processed <- textProcessor(documents = academia_df$content, \n                           metadata = academia_df,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nacademia_prepped <- prepDocuments(documents = academia_processed$documents,\n                         vocab = academia_processed$vocab,\n                         meta = academia_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 1402 of 1615 terms (1683 of 2498 tokens) due to frequency \nYour corpus now has 6 documents, 213 terms and 815 tokens.\n```\n:::\n\n```{.r .cell-code}\nacademia_basicmodel <- stm(documents = academia_prepped$documents,\n                         vocab = academia_prepped$vocab,\n                         data = academia_prepped$meta,\n                         K=8,\n                         verbose = F)\n\n\nplot.STM(academia_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(topicCorr(academia_basicmodel))\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = academia_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>%  filter(profession == 'politician') %>% filter(country == 'india' | country == 'russia' | country == 'america')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 × 10\n    ...1  ...2 people_names   peopl…¹ content from  to    profe…² country gender\n   <dbl> <dbl> <chr>          <chr>   <chr>   <chr> <chr> <chr>   <chr>   <chr> \n 1     1     1 Abraham Linco… us pre… \"“With… 1809   1865 politi… america male  \n 2     9     9 Barack Obama   us pre… \"Barac… 1961  2008  politi… america male  \n 3    29    29 Franklin D. R… us pre… \"Frank… 1882  1945  politi… america male  \n 4    35    35 Indira Gandhi  prime … \"Indir… 1917  1984  politi… india   female\n 5    39    39 Jacqueline Ke… americ… \"Jacqu… 1961  2022  politi… america female\n 6    40    40 Jawaharlal Ne… indian… \"Jawah… 1889   1964 politi… india   male  \n 7    42    42 Joe Biden      us pre… \"\\n\\r\\… 1915  1959  politi… america male  \n 8    43    43 John F. Kenne… us pre… \" John… 1917   1963 politi… america male  \n 9    46    46 Joseph Stalin  leader… \"Josep… 1879   1953 politi… russia  male  \n10    55    55 Lyndon Johnson us pre… \"Lyndo… 1908  1973  politi… america male  \n11    57    57 Mahatma Gandhi leader… \"Mahat… 1869   1948 politi… india   male  \n12    59    59 Malcolm X      americ… \"Malco… 1925  1965  politi… america male  \n13    68    68 Mikhail Gorba… leader… \"Mikha… 1931  1985  politi… russia  male  \n14    87    87 Ronald Reagan  us pre… \"Ronal… 1955   2012 politi… america male  \n15    96    96 Vladimir Lenin leader… \"Vladi… 1870   1924 politi… russia  male  \n16    97    97 Vladimir Putin russia… \"Vladi… 1952  2022  politi… russia  male  \n17   100   100 Woodrow Wilson us pre… \"Woodr… 1856  1939  politi… america male  \n# … with abbreviated variable names ¹​peoples_title, ²​profession\n```\n:::\n\n```{.r .cell-code}\npolitician_ind <- dataset %>%  filter(profession == 'politician') %>% filter(country == 'india')\n\npolitician_america <- dataset %>%  filter(profession == 'politician') %>% filter(country == 'america')\n\npolitician_russia <- dataset %>%  filter(profession == 'politician') %>% filter(country == 'russia')\n```\n:::\n\n# training indian politicians\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_ind_processed <- textProcessor(documents = politician_ind$content, \n                           metadata = politician_ind,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\npolitician_ind_prepped <- prepDocuments(documents = politician_ind_processed$documents,\n                         vocab = politician_ind_processed$vocab,\n                         meta = politician_ind_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 813 of 858 terms (968 of 1103 tokens) due to frequency \nYour corpus now has 3 documents, 45 terms and 135 tokens.\n```\n:::\n\n```{.r .cell-code}\npolitician_ind_basicmodel <- stm(documents = politician_ind_prepped$documents,\n                         vocab = politician_ind_prepped$vocab,\n                         data = politician_ind_prepped$meta,\n                         K=5,\n                         verbose = F)\n\n\nplot.STM(politician_ind_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(topicCorr(politician_ind_basicmodel))\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = politician_ind_basicmodel,\n      topic=2,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n# russian politicans\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_russia_processed <- textProcessor(documents = politician_russia$content, \n                           metadata = politician_russia,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\npolitician_russia_prepped <- prepDocuments(documents = politician_russia_processed$documents,\n                         vocab = politician_russia_processed$vocab,\n                         meta = politician_russia_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 1012 of 1125 terms (1208 of 1572 tokens) due to frequency \nYour corpus now has 4 documents, 113 terms and 364 tokens.\n```\n:::\n\n```{.r .cell-code}\npolitician_russia_basicmodel <- stm(documents = politician_russia_prepped$documents,\n                         vocab = politician_russia_prepped$vocab,\n                         data = politician_russia_prepped$meta,\n                         K=4,\n                         verbose = F)\n\n\nplot.STM(politician_russia_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = politician_russia_basicmodel,\n      topic=3,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n# america politician\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_america_processed <- textProcessor(documents = politician_america$content, \n                           metadata = politician_america,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\npolitician_america_prepped <- prepDocuments(documents = politician_america_processed$documents,\n                         vocab = politician_america_processed$vocab,\n                         meta = politician_america_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 2445 of 3068 terms (2988 of 5749 tokens) due to frequency \nYour corpus now has 10 documents, 623 terms and 2761 tokens.\n```\n:::\n\n```{.r .cell-code}\npolitician_america_basicmodel <- stm(documents = politician_america_prepped$documents,\n                         vocab = politician_america_prepped$vocab,\n                         data = politician_america_prepped$meta,\n                         K=4,\n                         verbose = F)\n\n\nplot.STM(politician_america_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = politician_america_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npolitician_america_prepped_corr <-\n  topicCorr(politician_america_basicmodel)$cor\npolitician_america_prepped_corr[politician_america_prepped_corr < 0] <- 0\n\npolitician_america_prepped_igraph <-\n  graph.adjacency(\n    politician_america_prepped_corr,\n    mode = \"undirected\",\n    weighted = T,\n    diag = F\n  )\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in graph.adjacency(politician_america_prepped_corr, mode = \"undirected\", : could not find function \"graph.adjacency\"\n```\n:::\n\n```{.r .cell-code}\npolitician_america_prepped_labels <-\n  apply(labelTopics(politician_america_basicmodel)$prob,\n        1,\n        paste,\n        collapse = \"\\n\")\n\nplot.igraph(\n  politician_america_prepped_igraph,\n  vertex.label = politician_america_prepped_labels,\n  vertex.size = colMeans(politician_america_basicmodel$theta) * 300,\n  edge.width = E(politician_america_prepped_igraph)$weight * 100\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot.igraph(politician_america_prepped_igraph, vertex.label = politician_america_prepped_labels, : could not find function \"plot.igraph\"\n```\n:::\n:::\n\n\n\n# Artists\n\n::: {.cell}\n\n```{.r .cell-code}\nx<-dataset %>% select(country, people_names, profession) %>% filter(profession == 'artist')\n\ntable(x$country)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\namerica british  europe  france germany \n      5       3       4       1       1 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_america <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'america') \n\nartist_british <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'british') \n\nartist_europe <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'europe') \n```\n:::\n\n\n# america artist\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_america_processed <- textProcessor(documents = artist_america$content, \n                           metadata = artist_america,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_america_prepped <- prepDocuments(documents = artist_america_processed$documents,\n                         vocab = artist_america_processed$vocab,\n                         meta = artist_america_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 993 of 1131 terms (1193 of 1674 tokens) due to frequency \nYour corpus now has 5 documents, 138 terms and 481 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_america_basicmodel <- stm(documents = artist_america_prepped$documents,\n                         vocab = artist_america_prepped$vocab,\n                         data = artist_america_prepped$meta,\n                         K=6,\n                         verbose = F)\n\n\nplot.STM(artist_america_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# cloud(stmobj = artist_america_basicmodel,\n#       topic=4,\n#       random.order=F,\n#       rot.per=0)\n\nplot(topicCorr(artist_america_basicmodel))\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-28-2.png){width=672}\n:::\n:::\n\n\n# british artist\n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_british_processed <- textProcessor(documents = artist_british$content, \n                           metadata = artist_british,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_british_prepped <- prepDocuments(documents = artist_british_processed$documents,\n                         vocab = artist_british_processed$vocab,\n                         meta = artist_british_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 698 of 740 terms (841 of 967 tokens) due to frequency \nYour corpus now has 3 documents, 42 terms and 126 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_british_basicmodel <- stm(documents = artist_british_prepped$documents,\n                         vocab = artist_british_prepped$vocab,\n                         data = artist_british_prepped$meta,\n                         K=3,\n                         verbose = F)\n\n\nplot.STM(artist_british_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncloud(stmobj = artist_british_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-29-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(topicCorr(artist_british_basicmodel))\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-29-3.png){width=672}\n:::\n:::\n\n\n\n# Europe artist\n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_europe_processed <- textProcessor(documents = artist_europe$content, \n                           metadata = artist_europe,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_europe_prepped <- prepDocuments(documents = artist_europe_processed$documents,\n                         vocab = artist_europe_processed$vocab,\n                         meta = artist_europe_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 1065 of 1137 terms (1234 of 1463 tokens) due to frequency \nYour corpus now has 4 documents, 72 terms and 229 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_europe_basicmodel <- stm(documents = artist_europe_prepped$documents,\n                         vocab = artist_europe_prepped$vocab,\n                         data = artist_europe_prepped$meta,\n                         K=3,\n                         verbose = F)\n\n\nplot.STM(artist_europe_basicmodel)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncloud(stmobj = artist_europe_basicmodel,\n      topic=2,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-30-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(topicCorr(artist_europe_basicmodel))\n```\n\n::: {.cell-output-display}\n![](topicModeling_files/figure-html/unnamed-chunk-30-3.png){width=672}\n:::\n:::\n",
    "supporting": [
      "topicModeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}