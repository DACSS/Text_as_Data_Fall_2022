{
  "hash": "15fb727ca4141d19de083968f641b4cc",
  "result": {
    "markdown": "---\ntitle: \"CaitlinRowley - Blog Post 6\"\nauthor: \"Caitlin Rowley\"\ndate: 11/27/2022\neditor: visual\n---\n\n\n## Data Summary:\n\nI have switched data sets for future blog posts. I will be using open-ended survey responses from a 1996 study titled *Survey of Gun Owners in the United States*.\n\nFor the study, respondents were asked six qualifying questions related to: (1) gun ownership, (2) gun-carrying practices, (3) gun display against the respondent, (4) gun use in self-defense against animals, (5) gun use in self-defense against people, and (6) other weapons used in self-defense. A \"yes\" response to a qualifying question led to a series of additional questions on the same topic as the qualifying question.\n\nThe open-ended responses include descriptions specifically related to the following questions: (1) where the respondent was when he or she displayed a gun (in self-defense or otherwise), (2) specific reasons why the respondent displayed a gun, (3) how the other individual reacted when the respondent displayed the gun, (4) how the individual knew the respondent had a gun, (5) whether the police were contacted for specific self-defense events, and (6) if not, why not.\n\nI will focus on the following research question: Can we identify the most common circumstance in which respondents displayed a gun? I am hoping that \"circumstance\" can include references to both the catalyst in the situation and the environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages:\n\ninstall.packages(\"RColorBrewer\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstalling package into 'C:/Users/caitr/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n```\n:::\n\n```{.r .cell-code}\ninstall.packages(\"stopwords\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstalling package into 'C:/Users/caitr/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n```\n:::\n\n```{.r .cell-code}\n# load libraries: \n\nlibrary(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyverse' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"mMatrix\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"replValueSp\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 4 of 4 threads used.\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n:::\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\nlibrary(wordcloud)\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidytext' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stopwords)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stopwords' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta<-\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\nAttaching package: 'tm'\n\nThe following object is masked from 'package:stopwords':\n\n    stopwords\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nlibrary(tokenizers)\n\n# read in data:\n\nSurvey <- read_csv(\"C:\\\\Users\\\\caitr\\\\OneDrive\\\\Documents\\\\DACSS\\\\DACSS 679\\\\DACSS 697\\\\DACSS 697 - Survey.csv\", col_names = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 814 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Q#, VERBATIM RESPONSE\ndbl (2): ID#, CODE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n#rename columns:\n\nnames(Survey) <- c('Respondent_ID','Survey_Question', 'Code', 'Open_Ended_Response')\nprint(Survey)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 814 × 4\n   Respondent_ID Survey_Question  Code Open_Ended_Response                      \n           <dbl> <chr>           <dbl> <chr>                                    \n 1           250 12                  0 \"I had it in my vehicle to go hunting, n…\n 2           248 28                  0 \"None of the above.  I was an employee w…\n 3           250 6                   1 \"Four of them.\"                          \n 4           250 6                   1 \"Four of them.\"                          \n 5           250 7                   1 \"My shotgun.\"                            \n 6           250 7                   1 \"My shotgun.\"                            \n 7           428 7                   1 \"{He said \\\"no\\\" to fully automatic weap…\n 8           409 8                   1 \"If they were used incorrectly.\"         \n 9          1165 9                   1 \"Well, yeah, community property.\"        \n10           250 12                  1 \"I had it in my vehicle to go hunting, n…\n# … with 804 more rows\n```\n:::\n\n```{.r .cell-code}\n# remove duplicate observations: \n\nSurvey_unique <- distinct(Survey)\n\nSurvey_unique %>% \n    separate_rows(Open_Ended_Response) %>% \n    distinct() %>%\n    nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8134\n```\n:::\n:::\n\n\nThere are 812 rows in this data frame and 4 columns. The four columns represent the following variables: survey respondent ID ('Respondent_ID), survey question ('Survey_Question'), response code ('Code'), and verbatim open-ended responses ('Open_Ended_Responses'). Each of the 812 rows now represents a unique observation. Additionally, there are 8134 unique words across all open-ended responses.\n\n## Pre-Processing:\n\nI will next create a corpus and remove both capitalization, punctuation, and stopwords from all open-ended responses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a corpus:\n\nSurvey_corpus <- corpus(Survey_unique$Open_Ended_Response)\nhead(Survey_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 6 documents.\ntext1 :\n\"I had it in my vehicle to go hunting, not on my person.\"\n\ntext2 :\n\"None of the above.  I was an employee working in a convenien...\"\n\ntext3 :\n\"Four of them.\"\n\ntext4 :\n\"My shotgun.\"\n\ntext5 :\n\"{He said \"no\" to fully automatic weapons but \"yes\" to semi-a...\"\n\ntext6 :\n\"If they were used incorrectly.\"\n```\n:::\n\n```{.r .cell-code}\n# tokenize, remove capitalization and punctuation:\n\nSurvey_tokens <- tokens(Survey_corpus, \n    remove_punct = T)\nSurvey_tokens <- tokens_tolower(Survey_tokens)\nhead(Survey_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 6 documents.\ntext1 :\n [1] \"i\"       \"had\"     \"it\"      \"in\"      \"my\"      \"vehicle\" \"to\"     \n [8] \"go\"      \"hunting\" \"not\"     \"on\"      \"my\"     \n[ ... and 1 more ]\n\ntext2 :\n [1] \"none\"        \"of\"          \"the\"         \"above\"       \"i\"          \n [6] \"was\"         \"an\"          \"employee\"    \"working\"     \"in\"         \n[11] \"a\"           \"convenience\"\n[ ... and 1 more ]\n\ntext3 :\n[1] \"four\" \"of\"   \"them\"\n\ntext4 :\n[1] \"my\"      \"shotgun\"\n\ntext5 :\n [1] \"he\"             \"said\"           \"no\"             \"to\"            \n [5] \"fully\"          \"automatic\"      \"weapons\"        \"but\"           \n [9] \"yes\"            \"to\"             \"semi-automatic\" \"he\"            \n[ ... and 28 more ]\n\ntext6 :\n[1] \"if\"          \"they\"        \"were\"        \"used\"        \"incorrectly\"\n```\n:::\n\n```{.r .cell-code}\n# remove stopwords separately: \n\nSurvey_tokens_stopwords <- tokens_select(Survey_tokens,\n                                       pattern = stopwords(\"en\"),\n                                       selection = \"remove\")\n```\n:::\n\n\n## Data Visualization:\n\nI will next extract features from my corpus by creating a document feature matrix. From there, I will be able to generate data visualizations.\n\nFor my first data visualization, I will generate a word cloud that focuses on the 50 most frequently-occurring words that appear a minimum of five times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create document feature matrix:\n\nSurvey_tokens <- dfm(Survey_tokens_stopwords)\n\n# identify 10 most common words:\n\ntopfeatures(Survey_dfm, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(Survey_dfm, 10): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\n# create a wordcloud:\n\ntextplot_wordcloud(Survey_dfm, min_count = 5, max_words = 50, random_order = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud(Survey_dfm, min_count = 5, max_words = 50, : object 'Survey_dfm' not found\n```\n:::\n:::\n\n\n## Co-Occurrence Matrix:\n\nNext, I will generate a feature co-occurrence matrix to determine which terms, if any, are more likely to occur more frequently together than would be expected by chance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a document-feature matrix that is limited to words that appear frequently (more than 5% of responses).\n\nSurvey_dfm_freq <- dfm_trim(Survey_dfm, min_termfreq = 5)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(Survey_dfm, min_termfreq = 5): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nSurvey_dfm_freq <- dfm_trim(Survey_dfm_freq, min_docfreq = .05, docfreq_type = \"prop\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(Survey_dfm_freq, min_docfreq = 0.05, docfreq_type = \"prop\"): object 'Survey_dfm_freq' not found\n```\n:::\n\n```{.r .cell-code}\n# use dfm to create a feature co-occurring matrix:\n\nSurvey_fcm <- fcm(Survey_dfm_freq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm(Survey_dfm_freq): object 'Survey_dfm_freq' not found\n```\n:::\n\n```{.r .cell-code}\n# check dimensions:\n\ndim(Survey_fcm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_fcm' not found\n```\n:::\n\n```{.r .cell-code}\n# check features:\n\nSurvey_fcm@Dimnames[[\"features\"]]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_fcm' not found\n```\n:::\n\n```{.r .cell-code}\n# view co-occurrence matrix:\n\nprint(Survey_fcm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'print': object 'Survey_fcm' not found\n```\n:::\n:::\n\n\nWe can see from selecting the co-occurrence matrix, whose dimensions are 7x7, that the co-occurring features are \"house,\" \"gun,\" \"street,\" \"pulled,\" \"outside,\" \"home,\" and \"car.\" This provides limited information, but it may be interesting when considered within the context of my research question.\n\nI would like to revisit my minimum term frequency, as well, to determine whether I should adjust it based on varied outputs.\n\nIn the interim, I will generate a word network that uses a feature co-occurrence matrix capturing terms occurring a minimum of 10 times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\n\nSurvey_fcm_small <- fcm(Survey_dfm_freq, min_count = 10, max_words = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm(Survey_dfm_freq, min_count = 10, max_words = 10): object 'Survey_dfm_freq' not found\n```\n:::\n\n```{.r .cell-code}\n# generate text plot:\n\ntextplot_network(Survey_fcm_small)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_network(Survey_fcm_small): object 'Survey_fcm_small' not found\n```\n:::\n:::\n\n\n## Correlated Topic Modeling:\n\nNext, I will apply the correlated topic modeling approach to my data set. I will also select a covariate based on closed-ended variables included in my data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages:\n\nlibrary(stm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stm' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nstm v1.3.6 successfully loaded. See ?stm for help. \n Papers, resources, and other materials at structuraltopicmodel.com\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n\n# generate data frame:\n\nSurvey_dfm_clean <- dfm(Survey_dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(Survey_dfm): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\n             tolower = TRUE\n             remove = stopwords(\"en\")\n             remove_punct = TRUE\ndim(Survey_dfm_clean)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_dfm_clean' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate correlated topic model:\n\ncor_topic_model <- stm(Survey_dfm, K = 5,\n                       verbose = FALSE, init.type = \"Spectral\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in asSTMCorpus(documents, vocab, data): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\ncor_topic_model\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'cor_topic_model' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(cor_topic_model)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(cor_topic_model): object 'cor_topic_model' not found\n```\n:::\n\n```{.r .cell-code}\n# label topics:\n# frex = words that are both frequent and exclusive to topic.\n# lift = calculated by dividing the topic-word distribution by the empirical word count probability distribution.\n\nlabelTopics(cor_topic_model)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(cor_topic_model): object 'cor_topic_model' not found\n```\n:::\n:::\n\n\nI now have a topic model with 5 topics, 805 documents, and a 1224-word dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify document most frequently associated with the five topics:\n\nfindThoughts(cor_topic_model,\n             texts = Survey_dfm$Open_Ended_Responses,\n             topics = c(1:5),\n             n = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findThoughts(cor_topic_model, texts = Survey_dfm$Open_Ended_Responses, : object 'cor_topic_model' not found\n```\n:::\n:::\n\n\nI am having a little trouble with this code; my understanding is that it's intended to yield the document most frequently associated with the five topics, but I am seeing a different, less informative output. I will work more on this.\n\n## Structure Topic Modeling:\n\nTry structural topic modeling (without predictor). My thought is that these results may look similar to the correlation topic model since I am not adding a predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# choose the number of topics:\n\nk <- 5\n\n# specify model:\n\nSurvey_STM <- stm(Survey_dfm,\n               K = k,\n               data = Survey_dfm$Open_Ended_Responses,\n               max.em.its = 1224,\n               seed = 1234,\n               init.type = \"Spectral\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in asSTMCorpus(documents, vocab, data): object 'Survey_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nlabelTopics(Survey_STM)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(Survey_STM): object 'Survey_STM' not found\n```\n:::\n:::\n\n\nTry some visualization to capture the estimated frequency of words across topics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Survey_STM, type = \"summary\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(Survey_STM, type = \"summary\"): object 'Survey_STM' not found\n```\n:::\n:::\n\n\nNext, I want to try extracting topics and assigning them to the vector of document proportions. I will extract the top words (as identified by 'frex'), collapse the strings, and separate the tokens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the words:\n\nSurvey_words <- labelTopics(Survey_STM, n=5)$frex\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in labelTopics(Survey_STM, n = 5): object 'Survey_STM' not found\n```\n:::\n\n```{.r .cell-code}\n# set up an empty vector:\n\nSurvey_topic_labels <- rep(NA, k)\n\n# set up a loop to go through the topics and collapse the words to a single label:\n\nfor(i in 1:k){Survey_topic_labels[i] <- paste(Survey_words[i,], collapse = \"_\")}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in paste(Survey_words[i, ], collapse = \"_\"): object 'Survey_words' not found\n```\n:::\n\n```{.r .cell-code}\n# print the labels:\n\nSurvey_topic_labels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA NA NA NA NA\n```\n:::\n:::\n\n\nWe now can see that the top words within each topic.\n\nWithout a predictor or covariate, I am unable to measure the effect on topic distributions. Identifying a way to incorporate a covariate or predictor will be my next task.\n\n## Next steps:\n\n## K-Means Cluster Analysis:\n\nI will next try a K-Means Cluster Analysis to see if any observations can be clustered into groups. I need to review this code chunk, as I am having trouble identifying clusters and generating visualizations. For future reference, I have left sample code in the code chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cluster)\n\n# start with k=10\n\nkmeans <- kmeans(Survey_dfm_clean, centers = 10, nstart = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(x): object 'Survey_dfm_clean' not found\n```\n:::\n\n```{.r .cell-code}\nkmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (x, centers, iter.max = 10L, nstart = 1L, algorithm = c(\"Hartigan-Wong\", \n    \"Lloyd\", \"Forgy\", \"MacQueen\"), trace = FALSE) \n{\n    .Mimax <- .Machine$integer.max\n    do_one <- function(nmeth) {\n        switch(nmeth, {\n            isteps.Qtran <- as.integer(min(.Mimax, 50 * m))\n            iTran <- c(isteps.Qtran, integer(k))\n            Z <- .Fortran(C_kmns, x, m, p, centers = centers, \n                as.integer(k), c1 = integer(m), c2 = integer(m), \n                nc = integer(k), double(k), double(k), ncp = integer(k), \n                D = double(m), iTran = iTran, live = integer(k), \n                iter = iter.max, wss = double(k), ifault = as.integer(trace))\n            switch(Z$ifault, stop(\"empty cluster: try a better set of initial centers\", \n                call. = FALSE), Z$iter <- max(Z$iter, iter.max + \n                1L), stop(\"number of cluster centres must lie between 1 and nrow(x)\", \n                call. = FALSE), warning(gettextf(\"Quick-TRANSfer stage steps exceeded maximum (= %d)\", \n                isteps.Qtran), call. = FALSE))\n        }, {\n            Z <- .C(C_kmeans_Lloyd, x, m, p, centers = centers, \n                k, c1 = integer(m), iter = iter.max, nc = integer(k), \n                wss = double(k))\n        }, {\n            Z <- .C(C_kmeans_MacQueen, x, m, p, centers = as.double(centers), \n                k, c1 = integer(m), iter = iter.max, nc = integer(k), \n                wss = double(k))\n        })\n        if (m23 <- any(nmeth == c(2L, 3L))) {\n            if (any(Z$nc == 0)) \n                warning(\"empty cluster: try a better set of initial centers\", \n                  call. = FALSE)\n        }\n        if (Z$iter > iter.max) {\n            warning(sprintf(ngettext(iter.max, \"did not converge in %d iteration\", \n                \"did not converge in %d iterations\"), iter.max), \n                call. = FALSE, domain = NA)\n            if (m23) \n                Z$ifault <- 2L\n        }\n        if (nmeth %in% c(2L, 3L)) {\n            if (any(Z$nc == 0)) \n                warning(\"empty cluster: try a better set of initial centers\", \n                  call. = FALSE)\n        }\n        Z\n    }\n    x <- as.matrix(x)\n    m <- as.integer(nrow(x))\n    if (is.na(m)) \n        stop(\"invalid nrow(x)\")\n    p <- as.integer(ncol(x))\n    if (is.na(p)) \n        stop(\"invalid ncol(x)\")\n    if (missing(centers)) \n        stop(\"'centers' must be a number or a matrix\")\n    nmeth <- switch(match.arg(algorithm), `Hartigan-Wong` = 1L, \n        Lloyd = 2L, Forgy = 2L, MacQueen = 3L)\n    storage.mode(x) <- \"double\"\n    if (length(centers) == 1L) {\n        k <- centers\n        if (nstart == 1L) \n            centers <- x[sample.int(m, k), , drop = FALSE]\n        if (nstart >= 2L || any(duplicated(centers))) {\n            cn <- unique(x)\n            mm <- nrow(cn)\n            if (mm < k) \n                stop(\"more cluster centers than distinct data points.\")\n            centers <- cn[sample.int(mm, k), , drop = FALSE]\n        }\n    }\n    else {\n        centers <- as.matrix(centers)\n        if (any(duplicated(centers))) \n            stop(\"initial centers are not distinct\")\n        cn <- NULL\n        k <- nrow(centers)\n        if (m < k) \n            stop(\"more cluster centers than data points\")\n    }\n    k <- as.integer(k)\n    if (is.na(k)) \n        stop(gettextf(\"invalid value of %s\", \"'k'\"), domain = NA)\n    if (k == 1L) \n        nmeth <- 3L\n    iter.max <- as.integer(iter.max)\n    if (is.na(iter.max) || iter.max < 1L) \n        stop(\"'iter.max' must be positive\")\n    if (ncol(x) != ncol(centers)) \n        stop(\"must have same number of columns in 'x' and 'centers'\")\n    storage.mode(centers) <- \"double\"\n    Z <- do_one(nmeth)\n    best <- sum(Z$wss)\n    if (nstart >= 2L && !is.null(cn)) \n        for (i in 2:nstart) {\n            centers <- cn[sample.int(mm, k), , drop = FALSE]\n            ZZ <- do_one(nmeth)\n            if ((z <- sum(ZZ$wss)) < best) {\n                Z <- ZZ\n                best <- z\n            }\n        }\n    centers <- matrix(Z$centers, k)\n    dimnames(centers) <- list(1L:k, dimnames(x)[[2L]])\n    cluster <- Z$c1\n    if (!is.null(rn <- rownames(x))) \n        names(cluster) <- rn\n    totss <- sum(scale(x, scale = FALSE)^2)\n    structure(list(cluster = cluster, centers = centers, totss = totss, \n        withinss = Z$wss, tot.withinss = best, betweenss = totss - \n            best, size = Z$nc, iter = Z$iter, ifault = Z$ifault), \n        class = \"kmeans\")\n}\n<bytecode: 0x000001f357309b88>\n<environment: namespace:stats>\n```\n:::\n\n```{.r .cell-code}\n# text mining and weight by term frequency (inverse document frequency):\n\nSurvey_tm <- convert(Survey_dfm_clean, to = \"tm\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in convert(Survey_dfm_clean, to = \"tm\"): object 'Survey_dfm_clean' not found\n```\n:::\n\n```{.r .cell-code}\nSurvey_wtfidf <- tm::weightTfIdf(Survey_tm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm::weightTfIdf(Survey_tm): object 'Survey_tm' not found\n```\n:::\n\n```{.r .cell-code}\n# remove sparse terms:\n\nSurvey_wtfidf_sparse <- tm::removeSparseTerms(Survey_wtfidf, 0.999)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, c(\"DocumentTermMatrix\", \"TermDocumentMatrix\")), : object 'Survey_wtfidf' not found\n```\n:::\n\n```{.r .cell-code}\n# create matrix:\n\nwrfidf_matrix <- as.matrix(Survey_wtfidf_sparse)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(Survey_wtfidf_sparse): object 'Survey_wtfidf_sparse' not found\n```\n:::\n\n```{.r .cell-code}\n# k-means cluster:\n\nSurvey_kmeans <- kmeans(wrfidf_matrix, centers = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(x): object 'wrfidf_matrix' not found\n```\n:::\n\n```{.r .cell-code}\nnames(Survey_kmeans)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_kmeans' not found\n```\n:::\n\n```{.r .cell-code}\n# plot clusters:\n\nclusplot(wrfidf_matrix)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in clusplot(wrfidf_matrix): object 'wrfidf_matrix' not found\n```\n:::\n\n```{.r .cell-code}\n# SAMPLE CODE: \n\n# generate Confusion Matrix:\n\nSurvey_vector <- as.vector(Survey_dfm_clean)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.vector(Survey_dfm_clean): object 'Survey_dfm_clean' not found\n```\n:::\n\n```{.r .cell-code}\nSurvey_CM <- table(Survey_vector, kmeans$cluster)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in table(Survey_vector, kmeans$cluster): object 'Survey_vector' not found\n```\n:::\n\n```{.r .cell-code}\nSurvey_CM\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Survey_CM' not found\n```\n:::\n\n```{.r .cell-code}\n# Model Evaluation and visualization\nplot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")]): object 'iris_1' not found\n```\n:::\n\n```{.r .cell-code}\nplot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")], \n     col = kmeans.re$cluster)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")], col = kmeans.re$cluster): object 'iris_1' not found\n```\n:::\n\n```{.r .cell-code}\nplot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")], \n     col = kmeans.re$cluster, \n     main = \"K-means with 3 clusters\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(iris_1[c(\"Sepal.Length\", \"Sepal.Width\")], col = kmeans.re$cluster, : object 'iris_1' not found\n```\n:::\n\n```{.r .cell-code}\n## Plotting cluster centers\nkmeans.re$centers\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'kmeans.re' not found\n```\n:::\n\n```{.r .cell-code}\nkmeans.re$centers[, c(\"Sepal.Length\", \"Sepal.Width\")]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'kmeans.re' not found\n```\n:::\n\n```{.r .cell-code}\n# cex is font size, pch is symbol\npoints(kmeans.re$centers[, c(\"Sepal.Length\", \"Sepal.Width\")], \n       col = 1:3, pch = 8, cex = 3) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in points(kmeans.re$centers[, c(\"Sepal.Length\", \"Sepal.Width\")], : object 'kmeans.re' not found\n```\n:::\n\n```{.r .cell-code}\n## Visualizing clusters\ny_kmeans <- kmeans.re$cluster\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'kmeans.re' not found\n```\n:::\n\n```{.r .cell-code}\nclusplot(iris_1[, c(\"Sepal.Length\", \"Sepal.Width\")],\n         y_kmeans,\n         lines = 0,\n         shade = TRUE,\n         color = TRUE,\n         labels = 2,\n         plotchar = FALSE,\n         span = TRUE,\n         main = paste(\"Cluster iris\"),\n         xlab = 'Sepal.Length',\n         ylab = 'Sepal.Width')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in clusplot(iris_1[, c(\"Sepal.Length\", \"Sepal.Width\")], y_kmeans, : object 'iris_1' not found\n```\n:::\n:::\n\n\n***QUESTION: I do not have access to the closed-ended questions in this survey; they are documented separately from the open-ended responses, and they require software that I do not have access to.***\n\n***I am unclear as to what the codes in the open-ended responses are referring. I tried to identify patterns based on survey questions, it seems that codes are applied across different branches of the survey (1 means x, y, or z depending on the respondents' previous answers). Do I need to pivot the data frame so that each code is its own variable? Would I then apply each code as its own covariate? To me, this seems very convoluted, and I anticipate there being a certain amount of speculation involved.***\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}