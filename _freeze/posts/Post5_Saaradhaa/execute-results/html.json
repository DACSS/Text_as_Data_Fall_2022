{
  "hash": "89c633b524ae620cfa8596e0b29220db",
  "result": {
    "markdown": "---\ntitle: \"Post 5\"\nauthor: \"Saaradhaa M\"\ndescription: \"KWIC and Dictionary Analysis\"\ndate: \"11/13/2022\"\neditor: visual\nformat:\n    html:\n        df-print: paged\n        toc: true\n        code-copy: true\n        code-tools: true\n        css: \"styles.css\"\ncategories: \n  - post 5\n  - saaradhaa\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.dictionaries)\nlibrary(utils)\n\nload(\"post5_saaradhaa.rdata\")\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n\n## KWIC Analysis\n\nI will first work on the comments I received for Post 4. I compared the appearance of the term 'culture' in the oral histories vs. Reddit comments, but received feedback to try a kwic analysis instead (to check which terms are more likely to be used with 'culture').\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load saved R object with oral histories.\n# transcripts <- read_rds(\"transcripts.rds\")\n\n# create 'quanteda' corpus. \noh_corpus <- corpus(transcripts$text)\n\n# create my own list of stopwords, based on qualitative reading of the first transcript.\nto_keep <- c(\"do\", \"does\", \"did\", \"would\", \"should\", \"could\", \"ought\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"because\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"over\", \"under\", \"no\", \"nor\", \"not\")\nStopwords <- stopwords(\"en\")\nStopwords <- Stopwords[!(Stopwords %in% to_keep)]\n\n# create tokens, remove punctuation, numbers, symbols and stopwords, then convert to lowercase.\noh_tokens <- tokens(oh_corpus, remove_punct = T, remove_numbers = T, remove_symbols = T)\noh_tokens <- tokens_select(oh_tokens, pattern = Stopwords, selection = \"remove\")\noh_tokens <- tokens_tolower(oh_tokens)\n\n# remove tokens that appeared in previous wordcloud that are not too meaningful.\noh_tokens <- tokens_remove(oh_tokens, c(\"know\", \"even\", \"used\", \"thing\", \"lot\", \"yes\", \"say\", \"going\", \"went\", \"yeah\", \"come\", \"actually\", \"mean\", \"like\", \"think\", \"get\", \"go\", \"said\", \"see\", \"things\", \"really\", \"well\", \"still\", \"little\", \"got\", \"right\", \"can\", \"came\", \"um\", \"quite\", \"bit\", \"every\", \"oh\", \"many\", \"always\", \"one\", \"two\", \"just\", \"much\", \"want\", \"wanted\", \"okay\", \"part\", \"also\", \"just\", \"us\", \"never\", \"many\", \"something\", \"want\", \"wanted\", \"uh\"))\n\n# look at kwic for oral histories.\ndata <- kwic(oh_tokens, pattern = c(\"[cC]ultur\"), window=15, valuetype = \"regex\")\ntextplot_xray(data)\n```\n\n::: {.cell-output-display}\n![](Post5_Saaradhaa_files/figure-html/kwic OH-1.png){width=672}\n:::\n:::\n\n\nI am not too sure how to interpret this plot and might need some help with this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load saved R object with reddit data and create corpus. remove punctuation/symbols, lowercase.\n# reddit_comments_final <- read_rds(\"reddit_comments_final.rds\")\nreddit_corpus <- corpus(reddit_comments_final$comment)\n\n# use same list of stopwords as oral histories.\nto_keep <- c(\"do\", \"does\", \"did\", \"would\", \"should\", \"could\", \"ought\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"because\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"over\", \"under\", \"no\", \"nor\", \"not\")\nStopwords <- stopwords(\"en\")\nStopwords <- Stopwords[!(Stopwords %in% to_keep)]\n\n# create tokens, remove punctuation, numbers, symbols and stopwords, then convert to lowercase.\nreddit_tokens <- tokens(reddit_corpus, remove_punct = T, remove_numbers = T, remove_symbols = T)\nreddit_tokens <- tokens_select(reddit_tokens, pattern = Stopwords, selection = \"remove\")\nreddit_tokens <- tokens_tolower(reddit_tokens)\n\n# remove tokens that appear in wordcloud that are not too meaningful.\nreddit_tokens <- tokens_remove(reddit_tokens, c(\"m\", \"s\", \"say\", \"see\", \"make\", \"way\", \"well\", \"many\", \"someone\", \"never\", \"still\", \"now\", \"go\", \"thing\", \"things\", \"know\", \"think\", \"also\", \"said\", \"going\", \"want\", \"t\", \"one\", \"lot\", \"much\", \"even\", \"really\", \"sure\", \"yeah\", \"look\", \"always\", \"something\", \"re\", \"actually\", \"get\", \"got\", \"though\", \"take\", \"etc\", \"can\", \"like\", \"don\", \"saying\"))\n\n# look at kwic for reddit data.\ndata2 <- kwic(reddit_tokens, pattern = c(\"[cC]ultur\"), window=15, valuetype = \"regex\")\n# textplot_xray(data2)\n```\n:::\n\n\nUnfortunately, RStudio crashes/hangs when I try to load the plot for the Reddit comments, as there are too many matches for the pattern (*N* = 8,047). How can I fix this?\n\n## Personal Values Dictionary\n\nThe main part of this post focuses on implementing the Personal Values Dictionary (PVD) on the oral histories and Reddit data. This dictionary was created by Ponizovskiy et al. (2020), based on the 10 fundamental values theorized by Schwartz (1992). These are shown in the figure below, which was taken from their paper:\n\n![*Figure Credit: Ponizovskiy et al. (2020)*](images/Screenshot%202022-11-09%20at%202.07.12%20PM.png){fig-align=\"center\" width=\"366\"}\n\nI created the table below to briefly summarize what each value means, adapting the descriptions from Schwartz (2012):\n\n| Value          | Need For? (adapted from Schwartz, 2012) |\n|----------------|-----------------------------------------|\n| Self-Direction | Independence                            |\n| Stimulation    | Excitement                              |\n| Hedonism       | Enjoyment                               |\n| Achievement    | Success                                 |\n| Power          | Authority                               |\n| Security       | Safety                                  |\n| Conformity     | Harmony                                 |\n| Tradition      | Maintaining customs                     |\n| Universalism   | Peace                                   |\n| Benevolence    | Group harmony                           |\n\nThe PVD was validated using both short-form text (e.g., social media status updates) and long-form text (e.g., essays), making it a good fit to extract values from both the oral histories and Reddit data. There were also consistent relationships between the personal values generated by the dictionary and those actually reported by participants.\n\n# Analyse Oral Histories\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load dictionary.\ndic <- read.delim(\"Refined_dictionary.txt\", col.names=c(\"word\",\"sentiment\"))\n\n# remove legend.\ndic <- dic[12:1079, ]\n\n# recode values for 'sentiment' column.\ndic$sentiment <- recode(dic$sentiment, \"1\" = \"SE\", \"2\" = \"CO\", \"3\" = \"TR\", \"4\" = \"BE\", \"5\" = \"UN\", \"6\" = \"SD\", \"7\" = \"ST\", \"8\" = \"HE\", \"9\" = \"AC\", \"10\" = \"PO\")\n\n# change to dictionary format.\ndic <- as.dictionary(dic)\n\n# run analysis.\noh_values <- liwcalike(oh_corpus, dic)\n\n# remove 'segment', which is redundant since 'docname' supplies the same info.\noh_values <- oh_values[,-2]\n\n# obtain mean values across texts.\ngraph_values <- oh_values %>% select(6:15) %>% colMeans()\ngraph_values <- as_tibble(graph_values)\ngraph_values$label <- c(\"ac\", \"be\", \"co\", \"he\", \"po\", \"sd\", \"se\", \"st\", \"tr\", \"un\")\n\n# plot graph.\nggplot(graph_values, aes(x= reorder(label,value), y= value)) +\n  geom_point(size = 1.5, color = \"purple\", fill = alpha(\"pink\", 0.4), alpha = 0.8, shape = 21, stroke = 2) +\n  geom_segment(aes(x = label, xend = label, y = 0, yend = value)) + \n   theme_minimal() + \n  labs(title = \"Personal Values Extracted From Oral Histories\", x = \"Value\", y = \"Mean Percentage\") +\n  geom_text(aes(label=sprintf(\"%0.2f\", ..y..)), position=position_dodge(width=0.9), vjust=-0.93, size=3)\n```\n\n::: {.cell-output-display}\n![](Post5_Saaradhaa_files/figure-html/dic OH-1.png){width=672}\n:::\n:::\n\n\nThe top 3 values observed were **self-direction**, **benevolence** and **achievement**.\n\n# Analyse Reddit Comments\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run analysis.\nreddit_values <- liwcalike(reddit_corpus, dic)\n\n# remove 'segment', which is redundant since 'docname' supplies the same info.\nreddit_values <- reddit_values[,-2]\n\n# obtain mean values across texts.\nvalues2 <- reddit_values %>% select(6:15) %>% colMeans()\nvalues2 <- as_tibble(values2)\nvalues2$label <- c(\"ac\", \"be\", \"co\", \"he\", \"po\", \"sd\", \"se\", \"st\", \"tr\", \"un\")\n\n# plot graph.\nggplot(values2, aes(x= reorder(label,value), y= value)) +\n  geom_point(size = 1.5, color = \"purple\", fill = alpha(\"pink\", 0.4), alpha = 0.8, shape = 21, stroke = 2) +\n  geom_segment(aes(x = label, xend = label, y = 0, yend = value)) + \n   theme_minimal() + \n  labs(title = \"Personal Values Extracted From Reddit\", x = \"Value\", y = \"Mean Percentage\") +\n  geom_text(aes(label=sprintf(\"%0.2f\", ..y..)), position=position_dodge(width=0.9), vjust=-0.93, size=3)\n```\n\n::: {.cell-output-display}\n![](Post5_Saaradhaa_files/figure-html/dic reddit-1.png){width=672}\n:::\n:::\n\n\nThe top 3 values observed were **benevolence**, **self-direction** and **universalism**.\n\n# Impact on RQs\n\nTo recap, these are my RQs:\n\n::: callout-tip\n## Research Questions\n\n1.  How have the values and concerns of South Asian Americans changed over time?\n2.  Do older South Asian Americans align more with the honour culture prevalent in their home countries? Do younger South Asian Americans align more with the dignity culture prevalent in the USA?\n:::\n\n**Self-direction** and **benevolence** was common to both groups of South Asian Americans. This can be interpreted as representing dual needs for exploring the world independently, as well as protecting the well-being of close ones (Schwartz, 2012). These intuitively might not seem like they go together, but perhaps could be expected in migrants: leaving home to pursue new opportunities and adapt to a new culture, and caring for the small family unit brought over.\n\nHowever, older South Asian Americans valued **achievement** more. Quoting directly from Schwartz (2012), p. 5: \"personal success through demonstrating competence according to social standards...thereby obtaining social approval\". Assuming older South Asian Americans have largely been socialized to South Asian cultural values, we can interpret this as a need for success *according to the societies they came from*, so as to maintain their reputation. This is a key facet of honour cultures, as I described in Blog Post 1.\n\nWe can qualitatively skim the interview with the highest achievement score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noh_values %>% select(docname,ac) %>% arrange(desc(ac)) %>% head(1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"docname\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ac\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"text36\",\"2\":\"0.85\",\"_rn_\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# save.image(\"post5_saaradhaa.RData\")\n```\n:::\n\n\nOpening Text 36 on my computer, the interviewee discusses her grueling training in classical dance, and how it was challenging to keep up with especially when she migrated to the US. This is a classic example of achievement.\n\nMeanwhile, younger South Asian Americans valued **universalism** more. This is interesting, as Schwartz describes this as *opposite* to benevolence, which focuses just on loved ones: \"people do not recognize these needs until they encounter others beyond the extended primary group...two subtypes of concern...welfare of those in the larger society and world and for nature\". This could reflect several things:\n\n-   Younger South Asian Americans' socialization to both South Asian and American cultures (regard for both the society at large [and]{.underline} their close loved ones, reflected in endorsing supposedly opposing values);\n\n-   Acknowledgment of growing social problems such as class inequality and global warming;\n\n-   Greater socialization to the dignity culture in the US than their elders, a key facet of which entails recognizing the worth of every individual.\n\nTo caution, the above is just one potential interpretation of the results that makes sense to me. However, it's important to note that honour and dignity are group-level values, while the PVD describes individual-level values. It is possible that there may be some disparities unaccounted for because of this.\n\nOverall, the dictionary analysis allows us to partially answer the RQs:\n\n1.  Values of South Asian Americans that remain unchanged are **self-direction** and **benevolence**, although the older generation gives more importance to the former and the younger generation gives more importance to the latter. Values that have changed: older South Asian Americans value **achievement** more, while younger South Asian Americans value **universalism** more.\n2.  There is some potential evidence that older South Asian Americans align more with South Asian honour cultures, given the importance of achievement as endorsed by society; and that younger South Asian Americans align more with the US' dignity culture, given the importance of universalism. The caveat is that both groups endorsed both values, but just at different degrees.\n\n# Bibliography\n\nPonizovskiy, V., Ardag, M., Grigoryan, L., Boyd, R., Dobewall, H., & Holtz, P. (2020). Development and Validation of the Personal Values Dictionary: A Theory--Driven Tool for Investigating References to Basic Human Values in Text. *European Journal of Personality*, *34*(5), 885--902.\n\nSchwartz, S. H. (1992). Universals in the content and structure of values: Theoretical advances and empirical tests in 20 countries. *Advances in Experimental Social Psychology*, 1--65.\n\nSchwartz, S. H. (2012). An Overview of the Schwartz Theory of Basic Values. *Online Readings in Psychology and Culture*, *2*(1).\n",
    "supporting": [
      "Post5_Saaradhaa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}