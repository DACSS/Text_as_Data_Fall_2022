{
  "hash": "6d8731cb3c2f463bfddaf5ba91ca09d7",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 6\"\nauthor: \"Nayan Jani\"\ndescription: \"Final Post\"\ndate: \"12/15/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Blog6\n  - Nayan Jani\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textmodels)\nlibrary(ggplot2)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.sentiment)\nlibrary(syuzhet)\nlibrary(text2vec)\nlibrary(stm)\nlibrary(LDAvis)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(textdata)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Introduction\n\nThe FIFA 2022 World Cup has captivated almost everyone's attention this year. However, most of that attention has been focused on human rights violations that are present. The Host nation Qatar has been under pressure for these violations because of their treatment of foreigners in their country and their failure to be inclusive. Migrant workers that have helped build stadiums have been mistreated, underpaid, overworked and even killed leading up to the World Cup. The Host nation also has disallowed for the LGBTQ community to represent themselves because of the Host nations beliefs. Soccer Fans from both cultures (Host nation vs Foreigners) have argued over what values to respect on the global level.\n \n# Objectives\n\nFind the overall Sentiment of the comments (Positive and Negative, Other emotions)\n  \nWhat is the main focus of discussion in the comments? What topic is most important to the people in the comments?\n  \nBased on the most important topics and the sentiment of those comments, are those comments classified correctly positive or negative? If yes, what are the comments POV? (western culture vs middle east culture)  Are those comments \"socially correct\"? (logical/acceptable POV vs Stereotyped/Stigmatized POV)\n \n# Read in Data\n\nUsing Youtube API and Python, I was able to extract comments from nine videos covering the world cup in Qatar. The sources of the videos include BBC, Sky Sports News, France 21 and independent content creators. I chose videos based on the the amount of views. The comments I have scraped are the top 100 most relevant comments and the top 100 most recent comments from each video. The  total number of comments I scraped was 1,391 . As a part of pre-processing my data, I removed all comments with less than 3 tokens in them using the tidyverse. I also removed any symbols, punctuation URLs , numbers and stopwords from my data . I tokenized my data using three different libraries, quanteda, tidytext and text2vec.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_bbc<- read_csv(\"_data/comments_bbc.csv\")\n\ndf_bbc<- df_bbc%>% \n  rename(text = \"i\")\n\ndf_bbc<- df_bbc %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbc <- df_bbc %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbc <- corpus(df_bbc)\ncorpus_bbc_summary <- summary(corpus_bbc)\n\ncorpus_bbc_summary$video <- \"BBC\"\ndocvars(corpus_bbc) <- corpus_bbc_summary\n\ndf_q<- read_csv(\"_data/comments_q.csv\")\n\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_q<- df_q %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\n\ndf_q <- df_q %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_q <- corpus(df_q)\ncorpus_q_summary <- summary(corpus_q)\ncorpus_q_summary$video <- \"Maqwell\"\ndocvars(corpus_q) <- corpus_q_summary\n\ndf_qRev<- read_csv(\"_data/comments_qRev.csv\")\n\ndf_qRev <- df_qRev%>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_qRev<- df_qRev %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_qRev <- df_qRev %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_qRev <- corpus(df_qRev)\ncorpus_qRev_summary <- summary(corpus_qRev)\ncorpus_qRev_summary$video <- \"MaqwellRev\"\ndocvars(corpus_qRev) <- corpus_qRev_summary\n\ndf_sky<- read_csv(\"_data/comments_sky.csv\")\n\ndf_sky<- df_sky%>% \n  rename(text = \"i\")\n\ndf_sky<- df_sky %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_sky <- df_sky %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_sky <- corpus(df_sky)\ncorpus_sky_summary <- summary(corpus_sky)\ncorpus_sky_summary$video <- \"sky\"\ndocvars(corpus_sky) <- corpus_sky_summary\n\n\ndf_bbcQ <- read_csv(\"_data/comments_bbcQ.csv\")\n\n\ndf_bbcQ<- df_bbcQ%>% \n  rename(text = \"i\")\n\ndf_bbcQ<- df_bbcQ %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbcQ <- df_bbcQ %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbcQ<- corpus(df_bbcQ)\ncorpus_bbcQ_summary <- summary(corpus_bbcQ)\ncorpus_bbcQ_summary$video <- \"BBC\"\n\ndf_bbcOL <- read_csv(\"_data/comments_bbcOL.csv\")\n\n\ndf_bbcOL<- df_bbcOL%>% \n  rename(text = \"i\")\n\ndf_bbcOL<- df_bbcOL %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbcOL <- df_bbcOL %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbcOL<- corpus(df_bbcOL)\ncorpus_bbcOL_summary <- summary(corpus_bbcOL)\ncorpus_bbcOL_summary$video <- \"BBC\"\n\n\ndf_BI <- read_csv(\"_data/comments_BI.csv\")\n\n\ndf_BI<- df_BI%>% \n  rename(text = \"i\")\n\ndf_BI<- df_BI %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_BI <- df_BI %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_BI<- corpus(df_BI)\ncorpus_BI_summary <- summary(corpus_BI)\ncorpus_BI_summary$video <- \"Business Insider\"\n\ndf_fra <- read_csv(\"_data/comments_fra.csv\")\n\n\ndf_fra<- df_fra%>% \n  rename(text = \"i\")\n\ndf_fra<- df_fra %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_fra <- df_fra %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_fra<- corpus(df_fra)\ncorpus_fra_summary <- summary(corpus_fra)\ncorpus_fra_summary$video <- \"France 21\"\n\ndf_H <- read_csv(\"_data/comments_H.csv\")\n\n\ndf_H<- df_H%>% \n  rename(text = \"i\")\n\ndf_H<- df_H %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_H <- df_H %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_H<- corpus(df_H)\ncorpus_H_summary <- summary(corpus_H)\ncorpus_H_summary$video <- \"Harris\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_df <- rbind(df_bbc,df_q,df_qRev,df_sky,df_bbcQ,df_bbcOL,df_BI,df_fra,df_H)\nfull_df$id <- 1:nrow(full_df)\nfull_df$id <- as.character(full_df$id)\n\nhead(full_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  text                                                                     id   \n  <chr>                                                                    <chr>\n1 Looking forward to it if you just act normal respect the culture and co… 1    \n2 Honestly every country in the world has done bad. May Allah bless these… 2    \n3 if you don't like it stay home and the last people talking about human … 3    \n4 So we can’t boycott a football game ( a trivial matter)  to protest a r… 4    \n5 I see everyone kept their mouth shut with Russia&#;s world cup           5    \n6 Did Qatar invade any country and kill millions? Who are robbing Africa?… 6    \n```\n:::\n\n```{.r .cell-code}\nfull_corpus <- corpus(full_df$text)\nfull_corpus_summary<- summary(full_corpus)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens1 <- tolower(full_df$text)\n\n# performs tokenization\ntokens1 <- word_tokenizer(tokens1,pos_remove = c(\"PUNCT\", \"DET\", \"ADP\", \"SYM\", \"PART\", \"AUX\" ))\n\nhead(tokens1, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] \"looking\"    \"forward\"    \"to\"         \"it\"         \"if\"        \n [6] \"you\"        \"just\"       \"act\"        \"normal\"     \"respect\"   \n[11] \"the\"        \"culture\"    \"and\"        \"country\"    \"and\"       \n[16] \"you\"        \"will\"       \"be\"         \"ok\"         \"no\"        \n[21] \"one\"        \"needs\"      \"to\"         \"know\"       \"someone\"   \n[26] \"is\"         \"gay\"        \"just\"       \"like\"       \"no\"        \n[31] \"one\"        \"needs\"      \"to\"         \"know\"       \"if\"        \n[36] \"you\"        \"hetero\"     \"just\"       \"save\"       \"that\"      \n[41] \"for\"        \"behind\"     \"closed\"     \"doors\"      \"and\"       \n[46] \"make\"       \"sure\"       \"it\"         \"s\"          \"between\"   \n[51] \"consenting\" \"adults\"     \"children\"   \"and\"        \"animals\"   \n[56] \"are\"        \"not\"        \"consenting\" \"adults\"     \"btw\"       \n[61] \"you\"        \"perverts\"  \n\n[[2]]\n [1] \"honestly\"  \"every\"     \"country\"   \"in\"        \"the\"       \"world\"    \n [7] \"has\"       \"done\"      \"bad\"       \"may\"       \"allah\"     \"bless\"    \n[13] \"these\"     \"workers\"   \"who\"       \"have\"      \"done\"      \"so\"       \n[19] \"much\"      \"for\"       \"the\"       \"country\"   \"however\"   \"to\"       \n[25] \"call\"      \"it\"        \"slavery\"   \"is\"        \"too\"       \"far\"      \n[31] \"they\"      \"r\"         \"not\"       \"taking\"    \"them\"      \"without\"  \n[37] \"there\"     \"will\"      \"like\"      \"when\"      \"british\"   \"and\"      \n[43] \"americans\" \"killed\"    \"and\"       \"took\"      \"thousands\" \"of\"       \n[49] \"slaves\"    \"they\"      \"come\"      \"here\"      \"for\"       \"better\"   \n[55] \"options\"   \"of\"        \"course\"    \"it\"        \"can\"       \"be\"       \n[61] \"better\"    \"however\"   \"it\"        \"is\"        \"much\"      \"better\"   \n[67] \"than\"      \"what\"      \"was\"       \"happening\" \"to\"        \"them\"     \n[73] \"in\"        \"their\"     \"own\"       \"countries\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_tokens <- tokens(full_corpus,\n    remove_numbers = T,\n    remove_url = T,\n    remove_punct = T,\n    remove_symbols = T)\nfull_tokens <-tokens_tolower(full_tokens)\nfull_tokens <- tokens_select(full_tokens, \n                              pattern = c(stopwords(\"en\"),\"quot\",\"href\",\"don\"),\n                              selection = \"remove\",\n                              min_nchar = 3)\n\n\nhead(full_tokens, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ntext1 :\n [1] \"looking\" \"forward\" \"just\"    \"act\"     \"normal\"  \"respect\" \"culture\"\n [8] \"country\" \"one\"     \"needs\"   \"know\"    \"someone\"\n[ ... and 22 more ]\n\ntext2 :\n [1] \"honestly\" \"every\"    \"country\"  \"world\"    \"done\"     \"bad\"     \n [7] \"may\"      \"allah\"    \"bless\"    \"workers\"  \"done\"     \"much\"    \n[ ... and 25 more ]\n```\n:::\n:::\n\n\n# WordClouds and TF-IDF\n\nMy first step is to do some exploratory analysis to see the most frequent and important terms in my corpus. TF-IDF is intended to measure how important a word is to a document in a collection (or corpus) of documents. I am looking for high TF-IDF values. Using a word cloud will display the frequency of my terms in my corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1245)\n\n\nfull_dfm <- dfm(full_tokens)\n\n\nsmaller_dfm <- dfm_trim(full_dfm, min_termfreq = 2)\n\n\nfull_dfm_tfidf <- dfm_tfidf(smaller_dfm)\n\n\ntextplot_wordcloud(smaller_dfm, min_count = 40, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(full_dfm_tfidf,50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     qatar      world     people        cup    workers    country  countries \n 282.35169  278.88022  249.95985  218.45006  213.95836  205.22592  200.27005 \n      just       like        can      human        one       work     rights \n 192.69143  189.64580  188.09018  154.31723  154.28810  151.48861  147.48920 \n      even       fifa   football    respect        see        get     really \n 130.13915  128.56978  127.23205  122.62079  121.78159  119.58720  112.11300 \n      good      video    western       know       many    migrant      money \n 109.62160  108.02355  107.75301  107.31365  107.28179  106.03057  105.38019 \n       now      years       make       time       west       also       want \n 103.36550  102.87957  101.11145   99.35274   99.19814   98.19723   97.12021 \n   culture conditions       much      never       well     middle        say \n  96.77193   94.56548   93.12160   92.59565   91.39109   87.47787   87.47787 \n     think       love      still      every    working      going      thing \n  86.93365   86.49862   86.06694   86.02204   85.58083   84.17787   82.79395 \n     india \n  82.36553 \n```\n:::\n:::\n\n\n\nThis word cloud depicts the frequency of words that were counted more than 40 times in the corpus. Words like workers, culture, migrant, western, rights and human shown in the word cloud imply that the common discussion within these comments could be about the clash of cultural difference between visitors of the world cup and the people who live in Qatar. The word workers being large suggests that the discussion of how workers were treated during the build up of the World Cup is common in the corpus. Based on TF_IDF ranking, I pulled the top 50 most important terms from my corpus. Numerically, I can see words like workers, fifa, respect, rights, human and culture are ranked highly in my corpus. \n\n# Sentiment Analysis\n\nFor Sentiment Analysis I used the packages tidytext and sentimentr. Sentimentr attempts to take into account valence shifters (i.e., negators, amplifiers (intensifiers), de-amplifiers (downtoners), and adversative conjunctions). This will give me better results than before. I then pulled the most positive and negative comments from the corpus to analyze.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sentimentr)\n\nmytext <- get_sentences(full_df$text)\nsenti<- sentiment_by(mytext)\n\nsenti<- senti %>% \n  filter(word_count > 4)\n\nqplot(senti$ave_sentiment,   geom=\"histogram\",binwidth=0.1,main=\"Sentiment Histogram\")\n```\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(head(full_corpus[which(senti$ave_sentiment >.5)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na country that has modern day slavery, human rights abuses, sharia law which is extremely discriminatory to women and people of any other religion other than Islam and Christianity, one of the highest death rates of workers, yet it&#;s okay to play football there, amazing isn&#;t it, people say money can&#;t buy you everything, show them this shitshow\nThis Arabic nation is little humanitarian concepts among workers , especially  home maid workers conditions are very brutal no human rights and laws. like fisherman attitude.\nA golden opportunity to combat negative stereotypes... used to make them worse.\nChad Qatar : Bribes Virgin West to submission. Humiliaties the West. Doesn&#;t care about stupid lgbt flag. Profits\nnow do the same when US host a worldwide event lol\nIm glad someone made a vid about this, I lived in Doha for 8 years and recently left back to my country. Whenever I would go home from school you could see workers being forced to work in the summers peak temp hour even though it was made illegal to work from 12pm to 4pm or something like that? Theres also a huge lack of safety, cranes with cargo would be moved OVER MOVING TRAFFIC:brbrTo any adults who plan to go to the world cup I will tell you now that many places in Qatar will not serve alchoholic drinks because they need a license and can only purchase their alchohol from QDC. So if you do want alchohol I&#;d reccommend going to a hotel like the Grand Hyatt or something. So don&#;t get upset if ya cant find a place to get beer. (but in the stadium there  will be alchohol drinking zones).\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(head(full_corpus[which(senti$ave_sentiment < -0.5)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIf this was Russia most teams would have pulled out!\nWhen is the west going to respecting other countries without pushing their agenda, I mean we all know this ain’t about the workers sharing rooms or the stadium being moveable ( it’s about the west pushing so called LGBQT) no wonder why only the west condemned Russia and the world didn’t\nFertilizer I am a Qatari expatriate. There are many workers in Qatar who work for 6/7 years. They work for the old wages. The company does not increase their wages. Many people have been able to change their jobs and have benefited a lot from the introduction of government without noc system. If the Qatari government had given the opportunity to the workers who have been working in Qatar for more than 5 years to change the kafala without noc, the workers in Qatar would have benefited a lot.\nHalf the British clowns are mad because, they don’t get to drink 🤡\nIs the homophobic, slave-using, dictatorship of Qatar`s lack of feeling shame news ?\nQatar should improve migrants workers conditions.  SHAME on you Qatar\n```\n:::\n:::\n\n\nThe overall sentiment of the corpus is skewed right, suggesting that most of the comments are negative. The most positive comments show more western culture beliefs and criticisms of Qatar. The reasoning and language the commenters use are socially acceptable based on their knowledge of the situation in Qatar and their experiences. Some of the comments are jokes but the main points get across about their beliefs. The most negative comments are more hateful in their beliefs about the opposing culture. The commenters are not using any reasonable judgment to make their claims.\n\n\n# Dictionary Analysis\n\n I chose to use the NRC dictionary to visualize the contribution of terms to emotional sentiment.\n \n\n::: {.cell}\n\n```{.r .cell-code}\npost_clean <- full_df %>%\n  select(text) %>%\n  unnest_tokens(word, text) %>% \n  filter(!word %in% stop_words$word) %>% \n  filter(word != \"https\") %>% \n  filter(word != \"href\") %>% \n  filter(word != \"www.youtube.com\")\n\nsentiment_word_counts <- post_clean %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\nsentiment_word_counts<- sentiment_word_counts %>% \n  filter(word != \"don\")\n\n\nsentiment_word_counts %>%\n  group_by(sentiment) %>%\n  top_n(9) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(title = \"Sentiment terms\",\n       y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nI can see that the words \"slavery\", \"government\" , \"Corruption\", and \"treat\" dominate negative emotions. Anticipation and Anger are two emotions I want to look at because they relate well to the lead up of the World Cup. Seeing \"respect\" contribute to the emotion Anticipation could associate that fans are expecting respect of all cultures and people at the World Cup. The emotion Anger reveals the words money and politics are contributing to the emotional distress of fans.\n\n\n\n# Topic Model \n\nFor LDA topic Modeling, I used the package text2vec. I found that the best value for K ranges form 5-10 from the my last blog post. I then extract the top 10 words from my topics. In this example, I set k=7.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nit1 <- itoken(tokens1, ids = full_df$id, progressbar = FALSE)\n\nstop_words1 = c(\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\"a\",\"the\",\"in\",\"as\",\"on\", \"is\",\"it\", \"to\",\"of\",\"are\",\"not\",\"and\",\"quot\",\"don\",\"youtu.be\",\"an\",\"have\",\"this\",\"if\",\"they\",\"v\",\"2\",\"their\",\"can\", \"than\",\"ve\")\nv1 <- create_vocabulary(it1,stopwords = stop_words1)\n\n\n\n#v1 <- prune_vocabulary(v1, term_count_min = 5)\n\n\n\nvectorizer1 <- vocab_vectorizer(v1)\n\ndtm1 <- create_dtm(it1, vectorizer1, type = \"dgTMatrix\")\n\nlda_model1 <- LDA$new(n_topics = 7, doc_topic_prior = 0.1,\n                     topic_word_prior = 0.01)\n\n\ndoc_topic_distr1 <- \n  lda_model1$fit_transform(x = dtm1, n_iter = 1000,\n                          convergence_tol = 0.001, n_check_convergence = 25,\n                          progressbar = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nINFO  [18:39:01.289] early stopping at 150 iteration\nINFO  [18:39:02.087] early stopping at 100 iteration\n```\n:::\n\n```{.r .cell-code}\nlda_model1$get_top_words(n = 10,\n                        lambda = 0.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]              [,2]      [,3]      [,4]    [,5]      [,6]        \n [1,] \"amp\"             \"cup\"     \"respect\" \"was\"   \"india\"   \"south\"     \n [2,] \"https\"           \"world\"   \"culture\" \"did\"   \"much\"    \"u\"         \n [3,] \"then\"            \"2022\"    \"country\" \"his\"   \"migrant\" \"for\"       \n [4,] \"href\"            \"hosting\" \"laws\"    \"again\" \"so\"      \"problem\"   \n [5,] \"www.youtube.com\" \"s\"       \"people\"  \"he\"    \"died\"    \"by\"        \n [6,] \"war\"             \"since\"   \"gay\"     \"being\" \"job\"     \"things\"    \n [7,] \"million\"         \"because\" \"just\"    \"last\"  \"workers\" \"western\"   \n [8,] \"truth\"           \"thing\"   \"lgbt\"    \"were\"  \"heart\"   \"different\" \n [9,] \"women\"           \"same\"    \"happy\"   \"done\"  \"qatari\"  \"her\"       \n[10,] \"usa\"             \"nothing\" \"lgbtq\"   \"now\"   \"those\"   \"conditions\"\n      [,7]       \n [1,] \"these\"    \n [2,] \"workers\"  \n [3,] \"rich\"     \n [4,] \"countries\"\n [5,] \"many\"     \n [6,] \"gulf\"     \n [7,] \"worked\"   \n [8,] \"treated\"  \n [9,] \"am\"       \n[10,] \"system\"   \n```\n:::\n:::\n\n\nThe Topic Model implies that the major topic of discussion surrounds how migrant workers were treated. The other topics in the model suggests respecting the laws of the host country, violation of human rights and government values.The first topic specifically is about where workers came from and how they were treated. Topic 6 specifically highlights the discussion of respect of a countries' laws and culture.\n\n\n# Semantic and Pairwise Correlation Network\n\nThe packages I used for Semantic and Pairwise Analysis include quanteda, widyr and ggraph. Here I wanted to learn a bit more about what features co-occur and correlation among words, by creating networks for both will help me examine this. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_fcm <- fcm(smaller_dfm)\n\n# keep only top features.\nsmall_fcm <- fcm_select(full_fcm, pattern = names(topfeatures(full_fcm, 60)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_fcm))\n\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHere I created a Semantic Network of the top 60 terms in the FCM. I see the heart of the network revolves around political terms. The two terms political and war seem to co occur with a lot of other terms. This could imply that some comments are discussion a war between different political views .I also see a sub network  that links travelers to law,laws and alcohol. This implies that people visiting the world cup must follow the laws in place, especially the no alcohol law.\n\n\n\n\n\n\nHere I want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately. The pairwise_cor() function in widyr lets us find the phi coefficient between words based on how often they appear in the same section. Here I pick particular terms of interest and find the other terms most associated with them and create a visualization of the correlations and clusters of words.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsection_words <- full_df %>%\n  mutate(section = row_number() %/% 10) %>%\n  filter(section > 0) %>%\n  unnest_tokens(word, text) %>%\n  filter(!word %in% stop_words$word) %>% \n  filter(word != \"https\") %>% \n  filter(word != \"href\") %>% \n  filter(word != \"www.youtube.com\") %>%\n  filter(word != \"youtu.be\") %>% \n  filter(word!= \"3\") %>% \n  filter(word!= \"2\") %>% \n  filter(word!= \"1\") %>% \n  filter(word!= \"12\") %>% \n  filter(word!= \"ve\")\n  \n  \n\nlibrary(widyr)\nlibrary(ggraph)\n\n\nword_cors <- section_words %>%\n  group_by(word) %>%\n  filter(n() >= 15) %>%\n  pairwise_cor(word, section, sort = TRUE)\n\n\n\nset.seed(2016)\n\nword_cors %>%\n  filter(correlation > .35) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nword_cors %>%\n  filter(item1 %in% c(\"western\", \"qatar\", \"lgbtq\", \"workers\",\"rights\",\"respect\",\"country\",\"cultures\")) %>%\n  group_by(item1) %>%\n  slice_max(correlation, n = 6) %>%\n  ungroup() %>%\n  mutate(item2 = reorder(item2, correlation)) %>%\n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](BlogPost6_NayanJani_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\nThe Words of interest that I picked include \"western\", \"qatar\", \"lgbtq\", \"workers\",\"rights\",\"respect\",\"country\",\"cultures\". The correlation between the words respect and culture, politics suggests that respecting the culture and politics of a certain region is being discussed when talking about respect. The Visualization shows the correlation between many words. The relationships here are symmetrical, rather than directional. The connections between words help verify what topics are being discussed in all of the comments. For example, the cluster surrounding the word \"worker\" shows that the word is correlated with negative terms that relate to the treatment and condition they received.\n\n\n# Limitations\n\nThe conclusions drawn cannot be fully proven from the current data , but they are enough to encourage further research.The data was only collected in a small scope compared to other projects. This is because of Youtube API did not let me scrape more than 100 comments per video. Many comments had typos in them, which hurts my analysis because some of the misspelled words could of been valuable. The topics of the videos may have skewed the conversation of the comments because the videos come from different sources.\n",
    "supporting": [
      "BlogPost6_NayanJani_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}