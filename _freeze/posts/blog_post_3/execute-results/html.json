{
  "hash": "971c18feeaf0b1bb6003db790f7d8b6a",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3 Text\"\nauthor: \"Quinn He\"\ndesription: \"Research project\"\neditor: visual\ndate: \"11/11/2022\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n#| label: setup\n#| warning: false\n\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RedditExtractoR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'RedditExtractoR' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet)\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rvest' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nlibrary(cleanNLP)\nlibrary(readr)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda.sentiment'\n\nThe following object is masked from 'package:quanteda':\n\n    data_dictionary_LSD2015\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(DT)\nlibrary(quanteda.textstats)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'quanteda.textstats' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Data pull from Reddit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_repub <- find_thread_urls(subreddit = \"republicans\", sort_by = \"top\", period = \"year\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsing URLs on page 1...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in file(con, \"r\"): cannot open URL 'https://www.reddit.com/r/\nrepublicans/top.json?t=year&limit=100': HTTP status was '429 Unknown Error'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in value[[3L]](cond): Cannot read from Reddit, check your inputs or internet connection\n```\n:::\n\n```{.r .cell-code}\ntop_dem <- find_thread_urls(subreddit = \"democrats\", sort_by = \"top\", period = \"year\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsing URLs on page 1...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in file(con, \"r\"): cannot open URL 'https://www.reddit.com/r/democrats/\ntop.json?t=year&limit=100': HTTP status was '429 Unknown Error'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in value[[3L]](cond): Cannot read from Reddit, check your inputs or internet connection\n```\n:::\n:::\n\n\n## Republican subreddit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_repub <- top_repub[-1,]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top_repub' not found\n```\n:::\n\n```{.r .cell-code}\ntop_repub$type <-\"top\"\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in top_repub$type <- \"top\": object 'top_repub' not found\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(top_repub, \"top_repub.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(top_repub, \"top_repub.rds\"): object 'top_repub' not found\n```\n:::\n\n```{.r .cell-code}\ntop_repub <- read_rds(\"top_repub.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file 'top_repub.rds': No\nsuch file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n:::\n\n\n## Democrats subreddit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_dem <- top_dem[-1,]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top_dem' not found\n```\n:::\n\n```{.r .cell-code}\ntop_dem$type <-\"top\"\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in top_dem$type <- \"top\": object 'top_dem' not found\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(top_dem, \"top_dem.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(top_dem, \"top_dem.rds\"): object 'top_dem' not found\n```\n:::\n\n```{.r .cell-code}\ntop_dem <- read_rds(\"top_dem.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file 'top_dem.rds': No\nsuch file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n:::\n\n\nI try to get the comments for both red and blue subreddits. The first one is for the democrat subreddit while the second is for the republican.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndem_url_content <- get_thread_content(top_dem$url[1:500])$comments$comment\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(urls, parse_thread_url): object 'top_dem' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl_content <- get_thread_content(top_repub$url[1:500])$comments$comment\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(urls, parse_thread_url): object 'top_repub' not found\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(url_content, \"url_content.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(url_content, \"url_content.rds\"): object 'url_content' not found\n```\n:::\n\n```{.r .cell-code}\nurl_content_top <- read_rds(\"url_content.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file 'url_content.rds':\nNo such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n:::\n\n\nRun below for democrat subreddit to turn it into a cleanable dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(dem_url_content, \"dem_url_content.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(dem_url_content, \"dem_url_content.rds\"): object 'dem_url_content' not found\n```\n:::\n\n```{.r .cell-code}\ndem_comments <- read_rds(\"dem_url_content.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file\n'dem_url_content.rds': No such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nwrite.csv(dem_comments, \"dem_comments.csv\", col.names = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in write.csv(dem_comments, \"dem_comments.csv\", col.names = T): attempt\nto set 'col.names' ignored\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'dem_comments' not found\n```\n:::\n\n```{.r .cell-code}\ndem_comments <- read_csv(\"dem_comments.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'dem_comments.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/Text_as_Data_Fall_2022/posts').\n```\n:::\n\n```{.r .cell-code}\ndem_content_info <- get_thread_content(top_dem$url[1:500])$comments\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(urls, parse_thread_url): object 'top_dem' not found\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(dem_content_info, \"dem_content_info.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(dem_content_info, \"dem_content_info.rds\"): object 'dem_content_info' not found\n```\n:::\n\n```{.r .cell-code}\ndem_comments_info <- read_rds(\"dem_content_info.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file\n'dem_content_info.rds': No such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nwrite.csv(dem_comments_info, \"dem_comments_info.csv\", col.names = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in write.csv(dem_comments_info, \"dem_comments_info.csv\", col.names = T):\nattempt to set 'col.names' ignored\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'dem_comments_info' not found\n```\n:::\n\n```{.r .cell-code}\ndem_comments_info <- read_csv(\"dem_comments_info.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'dem_comments_info.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/Text_as_Data_Fall_2022/posts').\n```\n:::\n:::\n\n\nI rename the republican subreddit comments to an easier name to follow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrepub_comments <- read_rds(\"url_content.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file 'url_content.rds':\nNo such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n:::\n\n\nI convert the rds to a csv for both the republican and democrat subreddits\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(repub_comments, \"repub_comments.csv\", col.names = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in write.csv(repub_comments, \"repub_comments.csv\", col.names = T):\nattempt to set 'col.names' ignored\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'repub_comments' not found\n```\n:::\n\n```{.r .cell-code}\nred_comments <- read_csv(\"repub_comments.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'repub_comments.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/Text_as_Data_Fall_2022/posts').\n```\n:::\n:::\n\n\nSo it looks like I only was able to get solely the comments, but I'd like to get a little more information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl_content_info <- get_thread_content(top_repub$url[1:500])$comments\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(urls, parse_thread_url): object 'top_repub' not found\n```\n:::\n:::\n\n\nAbove I am attempting to get addition information on reddit comments (user, date, post responding to, upvotes, downvotes). Below I am just reading them in as rds files like Saaradhaa has done because my other way of getting comments with RedditExtractoR has not worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(url_content_info, \"url_content_info.rds\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in saveRDS(url_content_info, \"url_content_info.rds\"): object 'url_content_info' not found\n```\n:::\n\n```{.r .cell-code}\nred_comments_info <- read_rds(\"url_content_info.rds\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readRDS(con, refhook = refhook): cannot open file\n'url_content_info.rds': No such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readRDS(con, refhook = refhook): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nwrite.csv(red_comments_info, \"repub_comments_info.csv\", col.names = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in write.csv(red_comments_info, \"repub_comments_info.csv\", col.names =\nT): attempt to set 'col.names' ignored\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'red_comments_info' not found\n```\n:::\n\n```{.r .cell-code}\nred_comments_info <- read_csv(\"repub_comments_info.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'repub_comments_info.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/Text_as_Data_Fall_2022/posts').\n```\n:::\n:::\n\n\nI want to next remove any comments that are \\[deleted\\] or \\[removed\\] as a user's comment could have been deleted by OP or removed by a moderator. I still need to remove the auto moderator messages from both subreddits since every post will most likely have an automod comment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_comments <- dem_comments_info %>%\n  filter(!(comment %in% c(\"[removed]\", \"[deleted]\"))) %>% \n  filter(!(author %in% \"AutoModerator\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., !(comment %in% c(\"[removed]\", \"[deleted]\"))): object 'dem_comments_info' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nred_comments <- red_comments_info %>% \n  filter(!(comment %in% c(\"[removed]\", \"[deleted]\")))%>% \n  filter(!(author %in% \"AutoModerator\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., !(comment %in% c(\"[removed]\", \"[deleted]\"))): object 'red_comments_info' not found\n```\n:::\n:::\n\n\nYay, finally I have my data! I now have all the comments I wanted to get so far. I'll still have to perform preprocessing techniques on the data. Now is time for preprocessing techniques. Below I turn the blue comments into a corpus, then tokenize it by removing all the excess junk.\n\n## Preprocessing /r/democrats\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_corpus <- corpus(blue_comments$comment)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(blue_comments$comment): object 'blue_comments' not found\n```\n:::\n\n```{.r .cell-code}\nblue_tokens <- tokens(blue_corpus,\n                      remove_punct = T,\n                      remove_symbols = T,\n                      remove_url = T,\n                      remove_numbers = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(blue_corpus, remove_punct = T, remove_symbols = T, remove_url = T, : object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nblue_tokens <- tokens_select(blue_tokens, selection = \"remove\", pattern = stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(blue_tokens, selection = \"remove\", pattern = stopwords(\"en\")): object 'blue_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n#I remove words that dont have any meaning to me that were in the network cloud.\n\nblue_tokens <- tokens_remove(blue_tokens, c(\"back\", \"really\", \"less\", \"saying\", \"look\", \"like\", \"get\", \"every\", \"said\", \"anything\", \"s\", \"right\", \"now\", \"see\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"remove\"): object 'blue_tokens' not found\n```\n:::\n:::\n\n\n# Democrats DFM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_dfm <- blue_tokens%>% \n  tokens_tolower() %>% \n  dfm()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(.): object 'blue_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n  dfm_trim(blue_dfm, min_termfreq = 3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(blue_dfm, min_termfreq = 3): object 'blue_dfm' not found\n```\n:::\n:::\n\n\nLets look at the most used words in the dfm\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(blue_dfm, 20)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(blue_dfm, 20): object 'blue_dfm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_fcm <- fcm(blue_dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm(blue_dfm): object 'blue_dfm' not found\n```\n:::\n:::\n\n\nI need to create a smaller fcm for the network plot because the current fcm is just too large.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmall_fcm_blue <- fcm_select(blue_fcm, pattern = names(topfeatures(blue_fcm, 50)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm_select(blue_fcm, pattern = names(topfeatures(blue_fcm, 50))): object 'blue_fcm' not found\n```\n:::\n\n```{.r .cell-code}\ntextplot_network(small_fcm_blue, min_freq = 0.5, omit_isolated = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_network(small_fcm_blue, min_freq = 0.5, omit_isolated = T): object 'small_fcm_blue' not found\n```\n:::\n:::\n\n\nThere are still some words I want to get rid of based off the network plot. The words I see on the outside of the network I would expect to be closer to the inside, but this could be because there are some words I just don't think are relevant.\n\n## Preprocessing on /r/republicans corpus\n\nBelow I do the same thing I did with the blue comments on the red comments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_corpus <- corpus(red_comments$comment)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(red_comments$comment): object 'red_comments' not found\n```\n:::\n\n```{.r .cell-code}\nred_tokens <- tokens(red_corpus,\n                      remove_punct = T,\n                      remove_symbols = T,\n                      remove_url = T,\n                      remove_numbers = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(red_corpus, remove_punct = T, remove_symbols = T, remove_url = T, : object 'red_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nred_tokens <- tokens_select(red_tokens, selection = \"remove\", pattern = stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(red_tokens, selection = \"remove\", pattern = stopwords(\"en\")): object 'red_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n#I remove words that dont have any meaning to me that were in the network cloud.\n\nred_tokens <- tokens_remove(red_tokens, c(\"back\", \"really\", \"less\", \"saying\", \"look\", \"like\", \"get\", \"every\", \"said\", \"anything\", \"s\", \"right\", \"now\", \"see\", \"anyone\", \"one\", \"say\", \"take\", \"much\", \"last\", \"never\", \"changed\", \"just\", \"questions\", \"r\", \"please\", \"note\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"remove\"): object 'red_tokens' not found\n```\n:::\n:::\n\n\n# Rebublicans DFM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_dfm <- red_tokens%>% \n  tokens_tolower() %>% \n  dfm()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(.): object 'red_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n  dfm_trim(red_dfm, min_termfreq = 3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_trim(red_dfm, min_termfreq = 3): object 'red_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nred_fcm <- fcm(red_dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm(red_dfm): object 'red_dfm' not found\n```\n:::\n:::\n\n\nThis is just a simple wordcloud to visually get a gist of some of the most popular words in the subreddit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_wordcloud(red_dfm, min_count = 10, max_words = 100, color = \"red\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud(red_dfm, min_count = 10, max_words = 100, : object 'red_dfm' not found\n```\n:::\n:::\n\n\nAgain, lets see the top terms in the republican subreddit dfm. I'm unsure of what \"t\" is, but some stemming may take care of that, or it could have some significant meaning within the subreddit (an inside joke perhaps).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(red_dfm, 20)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(red_dfm, 20): object 'red_dfm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsmall_fcm_red <- fcm_select(red_fcm, pattern = names(topfeatures(red_fcm, 50)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fcm_select(red_fcm, pattern = names(topfeatures(red_fcm, 50))): object 'red_fcm' not found\n```\n:::\n\n```{.r .cell-code}\ntextplot_network(small_fcm_red, min_freq = 0.5, omit_isolated = T, edge_color = \"orange\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_network(small_fcm_red, min_freq = 0.5, omit_isolated = T, : object 'small_fcm_red' not found\n```\n:::\n:::\n\n\nThis network plot seems closer to what I am looking for with the /r/democrats network plot. In both network plots, \"people\" is at the center of the network. The only problem is I don't know how they are using the word and in reference to what. I can solve this with a kwic function using \"people\" as a keyword.\n\n# Dictionary Methods\n\n### I want to use wordgraphs in the next blog post or final project.\n\n## NRC Dictionary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_nrc_sentiment <- liwcalike(red_corpus, data_dictionary_NRC)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(red_corpus, data_dictionary_NRC): object 'red_corpus' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(red_nrc_sentiment)+\n  geom_histogram(aes(positive), fill = \"orange\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_nrc_sentiment): object 'red_nrc_sentiment' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_nrc_sentiment <- liwcalike(blue_corpus, data_dictionary_NRC)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(blue_corpus, data_dictionary_NRC): object 'blue_corpus' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(blue_nrc_sentiment)+\n  geom_histogram(aes(positive), fill = \"blue\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_nrc_sentiment): object 'blue_nrc_sentiment' not found\n```\n:::\n:::\n\n\nThe graphs are very similar in structure, but the democrats subreddit has far more positive posts than the republicans subreddit, by an extreme margin.\n\n## Geninq Dictionary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_geninq_sentiment <- liwcalike(blue_corpus, data_dictionary_geninqposneg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(blue_corpus, data_dictionary_geninqposneg): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nnames(blue_geninq_sentiment)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'blue_geninq_sentiment' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(blue_geninq_sentiment)+\n  geom_histogram(aes(positive))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_geninq_sentiment): object 'blue_geninq_sentiment' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nred_geninq_sentiment <- liwcalike(red_corpus, data_dictionary_geninqposneg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(red_corpus, data_dictionary_geninqposneg): object 'red_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nnames(data_dictionary_geninqposneg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"positive\" \"negative\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(red_geninq_sentiment)+\n  geom_histogram(aes(positive))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_geninq_sentiment): object 'red_geninq_sentiment' not found\n```\n:::\n:::\n\n\n## Polarity measures for Geninq and NRC Dictionary\n\n/r/democrat\n\nNRC Polarity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_nrc_sentiment$polarity <- blue_nrc_sentiment$positive - blue_nrc_sentiment$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'blue_nrc_sentiment' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(blue_nrc_sentiment) +\n  geom_histogram(aes(polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_nrc_sentiment): object 'blue_nrc_sentiment' not found\n```\n:::\n:::\n\n\nGeninq Polarity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_geninq_sentiment$polarity <- blue_geninq_sentiment$positive - blue_geninq_sentiment$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'blue_geninq_sentiment' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(blue_geninq_sentiment)+\n  geom_histogram(aes(polarity))+\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_geninq_sentiment): object 'blue_geninq_sentiment' not found\n```\n:::\n:::\n\n\n/r/republican Polarity\n\nNRC polarity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_nrc_sentiment$polarity <- red_nrc_sentiment$positive - red_nrc_sentiment$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'red_nrc_sentiment' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(red_nrc_sentiment) +\n  geom_histogram(aes(polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_nrc_sentiment): object 'red_nrc_sentiment' not found\n```\n:::\n:::\n\n\nGeninq polarity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_geninq_sentiment$polarity <- red_geninq_sentiment$positive - red_geninq_sentiment$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'red_geninq_sentiment' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(red_geninq_sentiment)+\n  geom_histogram(aes(polarity))+\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_geninq_sentiment): object 'red_geninq_sentiment' not found\n```\n:::\n:::\n\n\nIt appears these two dictionaries in particular are quite similar, I'll have to check this with maybe a third dictionary. The only difference is NRC polarity has a higher count.\n\n## Dictionary Loughran and McDonald\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_loughran_mcdonald <- liwcalike(blue_corpus, data_dictionary_LoughranMcDonald)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(blue_corpus, data_dictionary_LoughranMcDonald): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nred_loughran_mcdonald <- liwcalike(red_corpus, data_dictionary_LoughranMcDonald)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(red_corpus, data_dictionary_LoughranMcDonald): object 'red_corpus' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(blue_loughran_mcdonald)+\n  geom_histogram(aes(positive))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_loughran_mcdonald): object 'blue_loughran_mcdonald' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(red_loughran_mcdonald)+\n  geom_histogram(aes(positive), fill = \"orange\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_loughran_mcdonald): object 'red_loughran_mcdonald' not found\n```\n:::\n:::\n\n\nBelow I implement the polarity measure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nred_loughran_mcdonald$polarity <- red_loughran_mcdonald$positive - red_loughran_mcdonald$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'red_loughran_mcdonald' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(red_loughran_mcdonald)+\n  geom_histogram(aes(polarity))+\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(red_loughran_mcdonald): object 'red_loughran_mcdonald' not found\n```\n:::\n:::\n\n\n## Dictionary Moral Foundations Dictionary\n\nI'll have to find a way to measure or graph these to compare the subreddits from a holistic view. Otherwise, I could find a way to join the data frames together, but I do not think that would benefit me.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nliwcalike(blue_corpus, data_dictionary_MFD)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(blue_corpus, data_dictionary_MFD): object 'blue_corpus' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(liwcalike(red_corpus, data_dictionary_MFD))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(red_corpus, data_dictionary_MFD): object 'red_corpus' not found\n```\n:::\n:::\n\n\nIf I want to create my own dictionary, which may be worth looking into, use this \"https://quanteda.io/reference/dictionary.html\"\n\n## Lexicoder Sentiment Dictionary\n\n/r/democrats\n\nI'm having some issues grouping the dfm by date so I can have a timeline at the bottom of the graph. I'll have to trouble shoot this later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmidt <- c(\"walker\", \"hershel\", \"warnock\", \"biden\", \"desantis\", \"trump\", \"vote\", \"fake\", \"fraud\")\n\ntoks_midt_blue <- tokens_keep(blue_tokens, pattern = phrase(midt), window = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"keep\"): object 'blue_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ndata_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2] #selects only negative and positive categories\n\ntoks_midt_blue_lsd <- tokens_lookup(toks_midt_blue, dictionary = data_dictionary_LSD2015_pos_neg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_lookup(toks_midt_blue, dictionary = data_dictionary_LSD2015_pos_neg): object 'toks_midt_blue' not found\n```\n:::\n\n```{.r .cell-code}\ndfmat_midt_lsd <- dfm(toks_midt_blue_lsd) %>% \n  dfm_group(groups = date)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(toks_midt_blue_lsd): object 'toks_midt_blue_lsd' not found\n```\n:::\n\n```{.r .cell-code}\nmatplot(dfmat_midt_lsd, type = \"l\", lty = 1, col = 1:2,\n        ylab = \"Frequency\", xlab = \"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in matplot(dfmat_midt_lsd, type = \"l\", lty = 1, col = 1:2, ylab = \"Frequency\", : object 'dfmat_midt_lsd' not found\n```\n:::\n\n```{.r .cell-code}\ngrid()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet\n```\n:::\n\n```{.r .cell-code}\nlegend(\"topleft\", col = 1:2, legend = colnames(dfmat_midt_lsd), lty = 1, bg = \"white\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'dfmat_midt_lsd' not found\n```\n:::\n:::\n\n\n# Dictionary with DFM\n\nHere I create a dfm with the NRC sentiment from the blue comments, then I create a polarity measure for the blue comments using the blue dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblue_dfm_nrc <- blue_dfm %>% \n  dfm_lookup(data_dictionary_NRC)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_lookup(., data_dictionary_NRC): object 'blue_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nblue_df_nrc <- convert(blue_dfm_nrc, to = \"data.frame\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in convert(blue_dfm_nrc, to = \"data.frame\"): object 'blue_dfm_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nblue_df_nrc$polarity <- (blue_df_nrc$positive - blue_df_nrc$negative)/(blue_df_nrc$positive + blue_df_nrc$negative)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'blue_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nblue_df_nrc$polarity[(blue_df_nrc$positive + blue_df_nrc$negative) == 0] <- 0\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in blue_df_nrc$polarity[(blue_df_nrc$positive + blue_df_nrc$negative) == : object 'blue_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(blue_df_nrc) +\n  geom_histogram(aes(x=polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(blue_df_nrc): object 'blue_df_nrc' not found\n```\n:::\n:::\n\n\nSo I feel like I did something wrong because the graph is completely symmetrical.\n\n## Creating my own dictionary\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndictionary()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in file.exists(file): invalid 'file' argument\n```\n:::\n:::\n\n\n# Keywords in Context\n\nHere I will fill in one of the top words once the code loads because I want to see how exactly some of these top words are used with the kwic function. I like using this function because I can pick specific words I want to look at in context of a larger sentence. Just by a glance, in the blue_corpus, people use \"they\" in reference to talking about the President. For example, \"they think that biden...\" or \"they knew biden...\". In both subreddits, you will get negative sentiment towards the President because people want to express their grievances, but do republicans tend to talk more negatively about him? I'll check a few other keywords as well to look at discourse at a glance for terms like \"ukraine\", \"midterm\", and \"Walker\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkwic_blue_biden <- kwic(blue_corpus, \"biden\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(blue_corpus, \"biden\"): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_red_biden <- kwic(red_corpus, \"biden\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(red_corpus, \"biden\"): object 'red_corpus' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkwic_blue_ukraine <- kwic(blue_corpus, \"Ukraine\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(blue_corpus, \"Ukraine\"): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_red_ukraine <- kwic(red_corpus, \"Ukraine\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(red_corpus, \"Ukraine\"): object 'red_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_blue_midterm <- kwic(blue_corpus, \"midterm\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(blue_corpus, \"midterm\"): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_red_midterm <- kwic(red_corpus, \"midterm\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(red_corpus, \"midterm\"): object 'red_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_blue_walker <- kwic(blue_corpus, \"Walker\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(blue_corpus, \"Walker\"): object 'blue_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nkwic_red_walker <- kwic(red_corpus, \"Walker\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kwic(red_corpus, \"Walker\"): object 'red_corpus' not found\n```\n:::\n:::\n\n\n# LDA Models for /r/democrats and /r/republicans\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(seededlda)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(seededlda): there is no package called 'seededlda'\n```\n:::\n\n```{.r .cell-code}\ndem_comments_lda <- textmodel_lda(blue_dfm, k = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textmodel_lda(blue_dfm, k = 10): could not find function \"textmodel_lda\"\n```\n:::\n\n```{.r .cell-code}\ndem_terms <- terms(dem_comments_lda, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in terms(dem_comments_lda, 10): object 'dem_comments_lda' not found\n```\n:::\n\n```{.r .cell-code}\ndem_terms\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'dem_terms' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngop_comments_lda <- textmodel_lda(red_dfm, k = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textmodel_lda(red_dfm, k = 10): could not find function \"textmodel_lda\"\n```\n:::\n\n```{.r .cell-code}\ngop_terms <- terms(gop_comments_lda, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in terms(gop_comments_lda, 10): object 'gop_comments_lda' not found\n```\n:::\n\n```{.r .cell-code}\ngop_terms\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'gop_terms' not found\n```\n:::\n:::\n\n\nI think I'll want to do LDA modelling based on what we learned in Tutorial 10 in my final project or future blog posts because the tutorial seemed more comprehensive and I noticed words were a bit more similar when grouped when the lambda was changed to various numbers between 0.2 and 0.4.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_keyness(textstat_keyness(blue_dfm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textstat_keyness(blue_dfm): object 'blue_dfm' not found\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}