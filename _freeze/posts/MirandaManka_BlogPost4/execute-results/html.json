{
  "hash": "aab040f3144754eb0e7bb9c1451a76dd",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 4 (Arlnow covid articles)\"\nauthor: \"Miranda Manka\"\ndesription: \"Working with data - Dictionaries\"\ndate: \"10/28/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Miranda Manka\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(devtools)\ndevtools::install_github(\"kbenoit/quanteda.dictionaries\")\nlibrary(quanteda.dictionaries)\nremotes::install_github(\"quanteda/quanteda.sentiment\")\nlibrary(quanteda.sentiment)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n\n## Data\n\nThe dataset contains 550 different articles from Arlnow (local news site in Northern Virginia) from March 2020 to September 2022. I decided to go back to March because that is when covid was officially declared a pandemic in the U.S. I may scrape more to get the months before March. I may also try to find a similar site for another county/city in another state to compare the two and see similarities and differences. \n\nHere I am just reading in the data from the csv I created and dropping the extra column that was created in the write.csv and renaming a column. I also removed articles where the title had \"Morning Notes\" as they weren't really articles but more recaps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narlnow_covid = read_csv(\"_data/arlnow_covid_posts.csv\", col_names = TRUE, show_col_types = FALSE)\narlnow_covid = subset(arlnow_covid, select = -c(1))\narlnow_covid = dplyr::rename(arlnow_covid, text_field = raw_text)\narlnow_covid = arlnow_covid[!grepl(\"Morning Notes\", arlnow_covid$header_text), ]\n```\n:::\n\n\n## Analysis\n\nMost of this analysis follows the week 8 tutorial we were given, I found it very helpful and I wanted to note where a lot of the code/information came from as I use a lot of it for this post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narlnow_covid_corpus = corpus(arlnow_covid, docid_field = \"doc_id\", text_field = \"text_field\")\narlnow_covid_summary = summary(arlnow_covid_corpus)\narlnow_covid_corpus_tokens = tokens(arlnow_covid_corpus, remove_punct = T)\n```\n:::\n\n\n### Dictionary Analysis\n\nThe basic idea with a dictionary analysis is to identify a set of words that connect to a certain concept, and to count the frequency of that set of words within a document. The liwcalike() function takes a corpus or character vector and carries out an analysis–based on a provide dictionary–that mimics the software LIWC (Linguistic Inquiry and Word Count). The LIWC software calculates the percentage of the document that reflects a host of different characteristics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use liwcalike() to estimate sentiment using NRC dictionary\nreviewSentiment_nrc = liwcalike(arlnow_covid_corpus, data_dictionary_NRC)\n\nnames(reviewSentiment_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"           \"Sixltr\"      \n [6] \"Dic\"          \"anger\"        \"anticipation\" \"disgust\"      \"fear\"        \n[11] \"joy\"          \"negative\"     \"positive\"     \"sadness\"      \"surprise\"    \n[16] \"trust\"        \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"         \"Quote\"       \n[26] \"Apostro\"      \"Parenth\"      \"OtherP\"      \n```\n:::\n:::\n\n\nI think this could be interesting but I don't know how helpful it is for my own terms because I am not sure how well they apply to the ideas.\n\nLooking at the most positive.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reviewSentiment_nrc) +\n  geom_histogram(aes(x = positive)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Based on that, let's look at those that are out in the right tail (i.e., which are greater than 8, most positive)\narlnow_covid_corpus[which(reviewSentiment_nrc$positive > 8)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 23 documents and 3 docvars.\ntext59 :\n\", Elementary-school-aged children will soon be able to get t...\"\n\ntext65 :\n\", This sponsored column is by James Montana, Esq., Doran She...\"\n\ntext89 :\n\"Members of Grace Community Church in Arlington honored thous...\"\n\ntext123 :\n\", This column is written and sponsored by Arlington Arts/Arl...\"\n\ntext135 :\n\", Peter’s Take is a weekly opinion column. The views and opi...\"\n\ntext140 :\n\", Progressive Voice is a bi-weekly opinion column. The views...\"\n\n[ reached max_ndoc ... 17 more documents ]\n```\n:::\n:::\n\n\nMost terms seem to be around 3-7, and the highest is above 10.\n\nLooking at the most negative.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reviewSentiment_nrc) +\n  geom_histogram(aes(x = negative)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\narlnow_covid_corpus[which(reviewSentiment_nrc$negative > 4)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 25 documents and 3 docvars.\ntext7 :\n\"Don’t look now but Covid cases are declining in Arlington., ...\"\n\ntext53 :\n\"A post-Thanksgiving rise in Covid cases in Arlington appears...\"\n\ntext124 :\n\", Progressive Voice is a bi-weekly opinion column. The views...\"\n\ntext125 :\n\"Health Matters is a biweekly opinion column. The views expre...\"\n\ntext127 :\n\", (Updated at 8:20 p.m.) The chairman of the Arlington GOP h...\"\n\ntext141 :\n\", This is a sponsored column by attorneys John Berry and Kim...\"\n\n[ reached max_ndoc ... 19 more documents ]\n```\n:::\n:::\n\n\nMost terms seem to be around 1-3, and the highest is above 6, and the lowest is 0.\n\nThese alone may not be the best indicators though, a combined measure may be better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviewSentiment_nrc$polarity = reviewSentiment_nrc$positive - reviewSentiment_nrc$negative\n\nggplot(reviewSentiment_nrc) +\n  geom_histogram(aes(polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\narlnow_covid_corpus[which(reviewSentiment_nrc$polarity < 0)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 29 documents and 3 docvars.\ntext2 :\n\"(Updated at 9:50 a.m.) Covid cases have held relatively stea...\"\n\ntext7 :\n\"Don’t look now but Covid cases are declining in Arlington., ...\"\n\ntext12 :\n\"The stock market drop aside, some other falling figures in A...\"\n\ntext16 :\n\"Arlington County Board member Libby Garvey is quarantining i...\"\n\ntext25 :\n\"For the last two months, Arlington County has been getting y...\"\n\ntext37 :\n\"The average rate of new Covid cases in Arlington has fallen ...\"\n\n[ reached max_ndoc ... 23 more documents ]\n```\n:::\n:::\n\n\nMost terms seem to be around 0-4, and the lowest close to -4 and the highest is almost 12.\n\n### Using Dictionaries with DFMs\n\nFor the dfm I am including most of the same preprocessing I did in the last post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a full dfm for comparison\narlnow_covid_dfm = tokens(arlnow_covid_corpus,\n                                    remove_punct = TRUE,\n                                    remove_symbols = TRUE,\n                                    remove_numbers = TRUE) %>%\n                           dfm(tolower = TRUE) %>%\n                           dfm_remove(stopwords('english')) %>%\n                           dfm_remove(c(\"arlington\", \"county\", \"virginia\", \"$\"))\n\nhead(arlnow_covid_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10 documents, 13,682 features (98.79% sparse) and 3 docvars.\n       features\ndocs    hundred parents say public schools prioritize recreating pre-covid\n  text1       2       5   2      2       5          1          1         2\n  text2       0       0   0      1       0          0          0         0\n  text3       0       0   0      4       1          0          0         0\n  text4       0       0   0      1       0          0          0         0\n  text5       0       3   0      1       3          0          0         0\n  text6       0       0   0      3       0          0          0         0\n       features\ndocs    normalcy classroom\n  text1        2         1\n  text2        0         0\n  text3        0         0\n  text4        0         0\n  text5        0         0\n  text6        0         0\n[ reached max_ndoc ... 4 more documents, reached max_nfeat ... 13,672 more features ]\n```\n:::\n\n```{.r .cell-code}\ndim(arlnow_covid_dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   457 13682\n```\n:::\n\n```{.r .cell-code}\n# convert corpus to dfm using the dictionary NRC\narlnow_covid_dfm_nrc = arlnow_covid_dfm %>%\n                          dfm_lookup(data_dictionary_NRC)\n\ndim(arlnow_covid_dfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 457  10\n```\n:::\n\n```{.r .cell-code}\nhead(arlnow_covid_dfm_nrc, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10 documents, 10 features (9.00% sparse) and 3 docvars.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     3           12       1    9   6        8       25       6        2\n  text2     3            6       2    9   2       19       10      13        4\n  text3     1            9       2    5   1        7       37       7        1\n  text4     0            4       1    9   1       11       19      11        6\n  text5     4           20       3   12   4       14       28       8        3\n  text6     2           11       1   17   2       22       34      14        7\n       features\ndocs    trust\n  text1    26\n  text2     6\n  text3    10\n  text4    15\n  text5    26\n  text6    10\n[ reached max_ndoc ... 4 more documents ]\n```\n:::\n\n```{.r .cell-code}\nclass(arlnow_covid_dfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"dfm\"\nattr(,\"package\")\n[1] \"quanteda\"\n```\n:::\n:::\n\n\nI think this is getting a little closer to what I can look at for my analysis. Looking at the emotions is interesting because things like anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, trust, etc. Looking at the emotions of the text and seeing what most texts have over time can be really interesting and what I want to do for my next blog post.\n\nNext I'll convert that to a data frame for further analysis, then create a polarity measure using the positive and negative measures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_nrc = convert(arlnow_covid_dfm_nrc, to = \"data.frame\")\nnames(df_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"doc_id\"       \"anger\"        \"anticipation\" \"disgust\"      \"fear\"        \n [6] \"joy\"          \"negative\"     \"positive\"     \"sadness\"      \"surprise\"    \n[11] \"trust\"       \n```\n:::\n\n```{.r .cell-code}\ndf_nrc$polarity = (df_nrc$positive - df_nrc$negative)/(df_nrc$positive + df_nrc$negative)\n\ndf_nrc$polarity[(df_nrc$positive + df_nrc$negative) == 0] = 0\n\nggplot(df_nrc) +\n  geom_histogram(aes(x = polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nA lot of the polarity is centered around 0.5, although some reach 1 and a few go to 0 and below.\n\n### Dictionary Comparison\n\nThere are multiple dictionaries that can be used, so it may be helpful to compare them and how they work on this data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert corpus to DFM using the General Inquirer dictionary\narlnow_covid_dfm_geninq = arlnow_covid_dfm %>%\n  dfm_lookup(data_dictionary_geninqposneg)\n\nhead(arlnow_covid_dfm_geninq, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 6 documents, 2 features (0.00% sparse) and 3 docvars.\n       features\ndocs    positive negative\n  text1       23        9\n  text2       17       14\n  text3       19        8\n  text4       12        5\n  text5       36       17\n  text6       20       18\n```\n:::\n:::\n\n\nI know that the dictionary used dependss on the analysis so I want to see this one too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create polarity measure for geninq\ndf_geninq = convert(arlnow_covid_dfm_geninq, to = \"data.frame\")\ndf_geninq$polarity = (df_geninq$positive - df_geninq$negative)/(df_geninq$positive + df_geninq$negative)\ndf_geninq$polarity[which((df_geninq$positive + df_geninq$negative) == 0)] = 0\n\n# look at first few rows\nhead(df_geninq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id positive negative   polarity\n1  text1       23        9 0.43750000\n2  text2       17       14 0.09677419\n3  text3       19        8 0.40740741\n4  text4       12        5 0.41176471\n5  text5       36       17 0.35849057\n6  text6       20       18 0.05263158\n```\n:::\n:::\n\n\nCombine all of these into a single dataframe to see how well they match up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create unique names for each data frame\ncolnames(df_nrc) = paste(\"nrc\", colnames(df_nrc), sep = \"_\")\ncolnames(df_geninq) = paste(\"geninq\", colnames(df_geninq), sep = \"_\")\n\n# now let's compare our estimates\nsent_df = merge(df_nrc, df_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\n\nhead(sent_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\n1      text1         3               12           1        9       6\n2     text10         0                4           0        1       4\n3    text100         0                7           0        3       3\n4    text101         9               22           4        9      15\n5    text102         0                5           1        2       3\n6    text103         1                5           2        5       3\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust nrc_polarity\n1            8           25           6            2        26    0.5151515\n2            3            8           2            0         6    0.4545455\n3            5           19           1            1        14    0.5833333\n4           20           60          11            6        41    0.5000000\n5            4            9           2            0         2    0.3846154\n6            7            9           4            0         3    0.1250000\n  geninq_positive geninq_negative geninq_polarity\n1              23               9       0.4375000\n2               7               3       0.4000000\n3              21              11       0.3125000\n4              58              32       0.2888889\n5              11               3       0.5714286\n6              11               6       0.2941176\n```\n:::\n:::\n\n\nI think there are some differences between the measures of polarity for the nrc vs geninq based on the measure.\n\nHow well different measures of polarity agree across the different approaches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(sent_df$nrc_polarity, sent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4553892\n```\n:::\n\n```{.r .cell-code}\nggplot(sent_df, mapping = aes(x = nrc_polarity,\n                              y = geninq_polarity)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThe correlation of 0.45 is moderate, which is ok but not the best.\n\n### Apply Dictionary within Contexts\n\nHow is \"vaccine\" treated across the corpus of articles? Limit the corpus to just vaccine related tokens (vax_words) and the window they appear within.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize corpus\ntokens_LMRD = tokens(arlnow_covid_corpus, remove_punct = TRUE)\n\n# what are the context (target) words or phrases\nvax_words = c(\"vaccine\", \"vaccinate\", \"vaccinated\", \"vax\", \"shot\", \"dose\", \"booster\")\n\n# retain only our tokens and their context\ntokens_vax = tokens_keep(tokens_LMRD, pattern = phrase(vax_words), window = 40)\n```\n:::\n\n\nPull out the positive and negative dictionaries and look for those within our token sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_dictionary_LSD2015_pos_neg = data_dictionary_LSD2015[1:2]\n\ntokens_vax_lsd = tokens_lookup(tokens_vax,\n                               dictionary = data_dictionary_LSD2015_pos_neg)\n```\n:::\n\n\nConvert this to a DFM.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm_vax = dfm(tokens_vax_lsd)\nhead(dfm_vax, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10 documents, 2 features (65.00% sparse) and 3 docvars.\n       features\ndocs    negative positive\n  text1        0        0\n  text2        2        5\n  text3        6       10\n  text4        0        0\n  text5        0        2\n  text6        6        5\n[ reached max_ndoc ... 4 more documents ]\n```\n:::\n:::\n\n\nDrop articles that did not feature any emotionally valence words from the analysis, then take a look at the distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert to data frame\nmat_vax = convert(dfm_vax, to = \"data.frame\")\n\n# drop if both features are 0\nmat_vax = mat_vax[-which((mat_vax$negative + mat_vax$positive)==0), ]\n\n# print a little summary info\npaste(\"We have \", nrow(mat_vax), \" reviews that mention positive or negative words in the context of vaccine terms.\", sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"We have 97 reviews that mention positive or negative words in the context of vaccine terms.\"\n```\n:::\n\n```{.r .cell-code}\n# create polarity scores\nmat_vax$polarity = (mat_vax$positive - mat_vax$negative)/(mat_vax$positive + mat_vax$negative)\n\n# summary\nsummary(mat_vax$polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.0000 -0.1667  0.1667  0.1503  0.4667  1.0000 \n```\n:::\n\n```{.r .cell-code}\n# plot\nggplot(mat_vax) + \n  geom_histogram(aes(x = polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost4_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nI kept this very general because I wasn't sure how/if I should make my own dictionary (scheduled office hours to discuss and will update). I think I still want to keep working on some past suggestions like including another news source and looking at how specific words are connected and used (I did focus on vaccine and similar words here), but I haven't included them yet as I don't know exactly what I want to do with it.\n",
    "supporting": [
      "MirandaManka_BlogPost4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}