{
  "hash": "c055c64cc256b929428292d1548d77ca",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 5: Topic Modeling\"\nauthor: \"Andrea Mah\"\ndesription: \"Topic modeling of speeches\"\ndate: \"11/30/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - BlogPost4\n  - Andrea Mah\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n#loading in nececssary libraries\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(text2vec)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'text2vec' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stopwords)\nlibrary(RCurl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'RCurl'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tidyr':\n\n    complete\n```\n:::\n:::\n\n\nFor the next stage in my project, I wanted to use topic modeling. Although I'm not sure I will use it in my final project, since I don't have clear hypotheses about topics and so whatever I find will be purely exploratory and/or descriptive. While I think it would be interesting to see the links between topics and my metadata (climate risk index, year of speech), I don't have strong ideas about a) what the topics will be, given that in my reading of speeches they seem highly similar and b) what topics might relate to in terms of climate risk or year of speech.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load in and check the data\nspeech.meta <- read.csv(file = \"FINAL_combined_meta-text_dataset.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in file(file, \"rt\"): cannot open file 'FINAL_combined_meta-\ntext_dataset.csv': No such file or directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in file(file, \"rt\"): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\ntail(speech.meta)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tail(speech.meta): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nnames(speech.meta)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\n#limit the dataset to just text and filenum\nspeech.meta <- speech.meta[,c(2,7)]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nhead(speech.meta)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(speech.meta): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nspeech.meta.ac <- as.character(speech.meta$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nspeech.meta.ac\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta.ac' not found\n```\n:::\n:::\n\n\nAfter getting the data ready, I followed the steps to set up the model as we learned in class. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create iterator\nit <-itoken(speech.meta.ac, tolower, word_tokenizer, ids = speech.meta$textnum, n_chunks = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in itoken(speech.meta.ac, tolower, word_tokenizer, ids = speech.meta$textnum, : object 'speech.meta.ac' not found\n```\n:::\n\n```{.r .cell-code}\n# prints iterator\nit\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'it' not found\n```\n:::\n\n```{.r .cell-code}\n# build the vocabulary, removing stopwords and some other tokens that are not meaningful\nsw <- stopwords(\"en\", source = \"snowball\")\ntypeof(sw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"character\"\n```\n:::\n\n```{.r .cell-code}\nsw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n```\n:::\n\n```{.r .cell-code}\nsw <- c(sw, \"must\", \"can\", \"c\", \"mr\", 'v', \"il\", \"tt\", \"ll\", \"aij\", \"j\", \"es\", \"ul\",\"wi\", \"q\", \"el\", \"tl\", \"cl\", \"la\", \"er\", \"tt\",\"ul\", \"fl\", \"fi\", \"r\", \"l\", \"lo\", \"tel\", \"cl\", \"la\", \"z\", \"le\", \"en\", \"ch\", \"ed\", \"fl\", \"er\", \"fi\", \"co\")\n\n#create vocabulary\nv <- create_vocabulary(it, stopwords = sw, doc_proportion_max = .95, doc_proportion_min = .05)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in create_vocabulary(it, stopwords = sw, doc_proportion_max = 0.95, : object 'it' not found\n```\n:::\n\n```{.r .cell-code}\n#I want to prune the vocabulary: \nv <- prune_vocabulary(v, term_count_min = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in prune_vocabulary(v, term_count_min = 10): object 'v' not found\n```\n:::\n\n```{.r .cell-code}\n# creates a closure that helps transform list of tokens into vector space\nvectorizer <- vocab_vectorizer(v)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in force(vocabulary): object 'v' not found\n```\n:::\n\n```{.r .cell-code}\n# creates document term matrix\ndtm <- create_dtm(it, vectorizer, type = \"dgTMatrix\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in create_dtm(it, vectorizer, type = \"dgTMatrix\"): object 'it' not found\n```\n:::\n:::\n\n\n\nNext I ran a series of models, testing different numbers of topics. I started with 10 since\nthat seemeed like a lot of topics to me and I thought it would be informative. As I thought, many topics were not very prevalent in the documents. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new LDA model with 10 topics\nlda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1,\n                     topic_word_prior = 0.01)\n\n\n# fitting the model\ndoc_topic_distr <- \n  lda_model$fit_transform(x = dtm, n_iter = 5000,\n                          convergence_tol = 0.001, n_check_convergence = 25,\n                          progressbar = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n# View the topics \nlda_model$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L),\n                        lambda = 0.3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lda_model$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, : n >= 1 && n <= length(private$vocabulary) is not TRUE\n```\n:::\n\n```{.r .cell-code}\n#What proportion of documents fit different topics? \nbarplot(doc_topic_distr[2, ], xlab = \"topic\",\n        ylab = \"proportion\", ylim = c(0,1),\n        names.arg = 1:ncol(doc_topic_distr))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in barplot(doc_topic_distr[2, ], xlab = \"topic\", ylab = \"proportion\", : object 'doc_topic_distr' not found\n```\n:::\n:::\n\n\nSince the percentages were so low, I decided to try a model with a lower number of topics, moving to extraction of 5 topics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#for some topics, the proportion is very low. \n\n#now I want to test with 5 topics. \nlda_model5 <- LDA$new(n_topics = 5, doc_topic_prior = 0.1,\n                     topic_word_prior = 0.01)\ndoc_topic_distr5 <- \n  lda_model5$fit_transform(x = dtm, n_iter = 5000,\n                          convergence_tol = 0.001, n_check_convergence = 25,\n                          progressbar = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n# View the topics \nlda_model5$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, 4L, 5L),\n                        lambda = 0.3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lda_model5$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, : n >= 1 && n <= length(private$vocabulary) is not TRUE\n```\n:::\n\n```{.r .cell-code}\n#what proportion of docs fit the topics? \nbarplot(doc_topic_distr5[2, ], xlab = \"topic\",\n        ylab = \"proportion\", ylim = c(0,1),\n        names.arg = 1:ncol(doc_topic_distr5))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in barplot(doc_topic_distr5[2, ], xlab = \"topic\", ylab = \"proportion\", : object 'doc_topic_distr5' not found\n```\n:::\n:::\n\n\nLess than 20% of documents are of 3 of these topics. Further, there were some topics that I had difficulty interpreting or thinking why they were separate. At this point I was starting to see some consistency in terms of which topics were extracted. For example, there was one topic that seemed to be very \"solutions\" focused with words like energy, development, technologies...However, I thought that maybe I should use even fewer topics? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_model4 <- LDA$new(n_topics = 4, doc_topic_prior = 0.1,\n                      topic_word_prior = 0.01)\n\ndoc_topic_distr4 <- \n  lda_model4$fit_transform(x = dtm, n_iter = 5000,\n                           convergence_tol = 0.001, n_check_convergence = 25,\n                           progressbar = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nlda_model4$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, 4L),\n                         lambda = 0.3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lda_model4$get_top_words(n = 20, topic_number = c(1L, 2L, 3L, : n >= 1 && n <= length(private$vocabulary) is not TRUE\n```\n:::\n\n```{.r .cell-code}\n#what proportion of docs fit the topics? \nbarplot(doc_topic_distr4[2, ], xlab = \"topic\",\n        ylab = \"proportion\", ylim = c(0,1),\n        names.arg = 1:ncol(doc_topic_distr4))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in barplot(doc_topic_distr4[2, ], xlab = \"topic\", ylab = \"proportion\", : object 'doc_topic_distr4' not found\n```\n:::\n\n```{.r .cell-code}\ndoc_topic_distr4\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'doc_topic_distr4' not found\n```\n:::\n:::\n\n\nI think 4 topics look meaningful, and at least each topic has 10% of documents classified as most likely being within that topic. But, to be safe, and to explore the data even more, I also looked at a 3 topic model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_model3 <- LDA$new(n_topics = 3, doc_topic_prior = 0.1,\n                      topic_word_prior = 0.01)\n\ndoc_topic_distr3 <- \n  lda_model3$fit_transform(x = dtm, n_iter = 5000,\n                           convergence_tol = 0.001, n_check_convergence = 25,\n                           progressbar = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nlda_model3$get_top_words(n = 20, topic_number = c(1L, 2L, 3L),\n                         lambda = 0.3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lda_model3$get_top_words(n = 20, topic_number = c(1L, 2L, 3L), : n >= 1 && n <= length(private$vocabulary) is not TRUE\n```\n:::\n\n```{.r .cell-code}\n#what proportion of docs fit the topics? \nbarplot(doc_topic_distr3[2, ], xlab = \"topic\",\n        ylab = \"proportion\", ylim = c(0,1),\n        names.arg = 1:ncol(doc_topic_distr3))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in barplot(doc_topic_distr3[2, ], xlab = \"topic\", ylab = \"proportion\", : object 'doc_topic_distr3' not found\n```\n:::\n:::\n\n\nIt seems like the 4-topic solution is what I should go with. Now I should try to describe each topic. Because this is completely exploratory, I thought it would be better to use a naming method that simply relies on top words in each topic. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_model4$plot()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lda_model4$plot(): To use visualisation, please install 'LDAvis' package first.\n```\n:::\n:::\n\n\nThis resulted in the following four topics. I can speculate about the content of each but want to explore the speeches which are most likely classified under each before doing so...\n\n#topic 1: convention_parties_president_protocol_annex - perhaps this is about the UNFCC itself/the proccess\n#topic 2: us_agreement_need_action_challenge - this also seems sort of process focused, \n'how will we go about solving the problems'\n#topic 3: energy_development_technologies_projects_sustainable - this seems solution-focused to me, \n'what should we do about climate change'\n#topic 4: island_people_human_small_sea - this final topic feels different from the others, based on \nmy initial exploration I feel like it is really focused on how people are being impacted\nor the 'human element of climate change'. \n\nNext steps: Although it would be completely exploratory (and again, maybe not relevant to my key research \nquestions) I was wondering what it would mean to see how the climate risk index relates to topic prevalence?\nAs well, I could look at topic probabilities changing over time (i.e., by year of speech)? \n\nTo examine this, I want to save the probabilities for the four topics for each document, and then use my\nmetadata to look at correlations/regressions between these and the topics.\nI haven't figured out a clean way to export these probabilities\nand join them with my original data. Because it isn't a priority for me, my plan is to return to topic \nmodeling after completing the other key analyses.\n\nFurther, based on my past experience doing similar types of analyses (to me, the concept seems\nhighly similar to latent class/profile analysis and factor analysis) I feel like I need to more deeply think\nabout what these topics are about, what they represent, and (becaues of my area of interest) what they mean \npsychologically. It would be fun to rush ahead and look at some of the analyses I'd like to do, but I am still\nuncertain about what I think underlies each of the topics I found, and I want to think about it more before\nI do anlayses, maybe so I can even think of hypotheses to test rather than just testing every possibility...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}