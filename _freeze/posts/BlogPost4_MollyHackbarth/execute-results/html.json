{
  "hash": "26ed932f1f700638bcf6eea36e03d3c4",
  "result": {
    "markdown": "---\ntitle: \"Blog Post Four\"\nauthor: \"Molly Hackbarth\"\ndescription: \"Working with the data\"\ndate: \"10/29/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog posts\n  - hw4\n  - molly hackbarth\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cld3)\nlibrary(dplyr)\nlibrary(here)\nlibrary(devtools)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\n#new packages\n#devtools::install_github(\"kbenoit/quanteda.dictionaries\") \nlibrary(quanteda.dictionaries)\n#devtools::install_github(\"quanteda/quanteda.sentiment\")\nlibrary(quanteda.sentiment)\n\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Research Question\n\n**My current research question:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?\n\n# Reading in the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write csv has been commented out due to it continously trying to save an \"updated version\" in Git. \n\nreddit_data <- read.csv(here::here(\"posts\", \"_data\", \"loveisblindjapan.csv\"))\n\ntwitter1 <- read.csv(here::here(\"posts\", \"_data\", \"tweets.csv\"))\n\ntwitter2 <- read.csv(here::here(\"posts\", \"_data\", \"tweets#.csv\"))\n\nreddit <- subset(reddit_data, select = c(\"body\", \"created_utc\")) \n\nreddit$created_utc <- as.Date.POSIXct(reddit$created_utc)\n\nreddit <- reddit %>% \n  select(text = body, \n            date = created_utc)\n# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])\nreddit <- reddit %>% \n  filter(!text == '[deleted]') %>% \n  filter(!text == '[removed]')\n\n#remove counting column\ntwitter1 <- twitter1 %>% select(!c(X, User))\ntwitter2 <- twitter2 %>% select(!c(X, User))\n\ntwitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)\n#write.csv(twitter, here::here(\"posts\", \"_data\", \"twitter.csv\") , all(T) )\n\nnames(twitter) <- tolower(names(twitter))\ntwitter <- twitter %>% \n  rename_at('tweet', ~ 'text', \n            'Date' ~ 'date')\ntwitter$date <- as.Date(strftime(twitter$date, format=\"%Y-%m-%d\"))\n\n# remove duplicate tweets\ntwitter <- twitter %>% distinct(text, date, .keep_all = TRUE)\n\n#check for duplicate tweets\ntwitter %in% unique(twitter[ duplicated(twitter)]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE\n```\n:::\n\n```{.r .cell-code}\nallsocialmedia <- merge(twitter, reddit, by=c('text','text', 'date', 'date'),all=T, ignore_case =T)\n#write.csv(twitter, here::here(\"posts\", \"_data\", \"loveisblind_socialmedia.csv\") , all(T) )\n```\n:::\n\n\n# Creating a Separate Word Cloud for Twitter\n\nIn order to remove the dates from Twitter I decided to run the same formula only on the Twitter text column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_text <- twitter$text\ntwitter_text_corpus <- subset(twitter_text, detect_language(twitter) == \"en\")\ntwitter_text_corpus <- twitter_text_corpus[!is.na(twitter_text_corpus)]\ntwitter_text_corpus <- corpus(twitter_text_corpus)\ntwittertextsummary <- summary(twitter_text_corpus)\n\ntwitter_text_corpus_tokens <- tokens(twitter_text_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_text_lemmitized <- tokens_replace(twitter_text_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\ntwitter_corpus_text_dfm <- twitter_text_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_text_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/twitter text word cloud-1.png){width=672}\n:::\n:::\n\n\n## Removing The Show's Name\n\nAlthough the data is a lot cleaner now I do wonder if removing the key words \"love\", \"blind\", \"japan\" and \"#loveisblindjapan\" will give a better picture of how posters are feeling. I will go ahead and remove the phrases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_text <- twitter$text\n\ntwitter_text_corpus <- subset(twitter_text, detect_language(twitter) == \"en\")\ntwitter_text_corpus <- twitter_text_corpus[!is.na(twitter_text_corpus)]\ntwitter_text_corpus <- corpus(twitter_text_corpus)\ntwittertextsummary <- summary(twitter_text_corpus)\n\nmystopwords <- c(\"love is blind japan\", \"#loveisbindjapan\", \"#LoveIsBlindJapan\",\"Love Is Blind Japan\",\"Love is Blind Japan\", \"Love Is Blind: Japan\", \"#loveisblind\", \"ラブイズブラインドjapan\", \"#ラブイズブラインドjapan\", \"loveisblind\", \"#loveisblind2\", \"blind:japan\")\n\ntwitter_text_corpus_tokens <- tokens(twitter_text_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_text_lemmitized <- tokens_replace(twitter_text_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\ntwitter_corpus_text_dfm <- twitter_text_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_text_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/removing the shows title-1.png){width=672}\n:::\n:::\n\n\nHere is we can see a more accurate idea of how people are tweeting about the show. Even with the show's title being remove love is still a large part of tweets.\n\nI also noticed that \"blind\" was used often. Looking at the tweets it seems unlikely that the word blind was used in any other way than to mention the show. Thus I'd like to remove the word blind as well since it seems unlikely to be useful to sentiment analysis.\n\nI will also remove the word \"show\" as show seemingly is only talking about the series rather than any emotions.\n\nI have also found a different way to remove dates and have done so below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_text_corpus <- subset(twitter, detect_language(twitter) == \"en\")\ntwitter_text_corpus <- corpus(twitter_text_corpus)\ntwitter_text_corpus <- twitter_text_corpus[!is.na(twitter_text_corpus)]\ntwittertextsummary <- summary(twitter_text_corpus)\n\ntwitter_text_corpus <- trimws(gsub(\"[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}\", \"\", twitter_text_corpus))\n\nmystopwords <- c(\"love is blind japan\", \"#loveisbindjapan\", \"#LoveIsBlindJapan\",\"Love Is Blind Japan\",\"Love is Blind Japan\", \"Love Is Blind: Japan\", \"#loveisblind\", \"ラブイズブラインドjapan\", \"#ラブイズブラインドjapan\", \"loveisblind\", \"#loveisblind2\", \"blind:japan\", \"blind\", \"show\")\n\ntwitter_text_corpus_tokens <- tokens(twitter_text_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\n\ntwitter_text_lemmitized <- tokens_replace(twitter_text_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\ntwitter_corpus_text_dfm <- twitter_text_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_text_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/removing the words blind and show-1.png){width=672}\n:::\n:::\n\n\nI have found out why the dates column has become messy. This is because originally I had put the corpus() function after the \\[!is.na()\\]. However when put first it still shows the date column in the summary.\n\n# Reddit and Updated Social Media word cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- subset(reddit, detect_language(reddit) == \"en\")\nreddit_corpus <- corpus(reddit_corpus)\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\nredditsummary <- summary(reddit_corpus)\n\nreddit_corpus <- trimws(gsub(\"[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}\", \"\", reddit_corpus))\n\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T, \n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nreddit_corpus_dfm <- reddit_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(reddit_corpus_dfm, max_words=200, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit word cloud same-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsocial_corpus <- subset(allsocialmedia, detect_language(allsocialmedia) == \"en\")\nsocial_corpus <- corpus(social_corpus)\nsocialsummary <- summary(social_corpus)\n\nsocial_corpus <- social_corpus[!is.na(social_corpus)]\n\n\nmystopwords <- c(\"love is blind japan\", \"#loveisbindjapan\", \"#LoveIsBlindJapan\",\"Love Is Blind Japan\",\"Love is Blind Japan\", \"Love Is Blind: Japan\", \"#loveisblind\", \"ラブイズブラインドjapan\", \"#ラブイズブラインドjapan\", \"loveisblind\", \"#loveisblind2\", \"blind:japan\", \"blind\", \"show\")\n\nsocial_corpus_tokens <- tokens(social_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\nsocial_lemmitized <- tokens_replace(social_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nsocial_corpus_dfm <- social_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(social_corpus_dfm, max_words=200, color=\"orange\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/social updated word cloud-1.png){width=672}\n:::\n:::\n\n\n# Text Plot for Reddit\n\nI mentioned previously I would like to test a textplot for Reddit. Below you will see my test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nrsmaller_dfm <- dfm_trim(reddit_corpus_dfm, max_termfreq = 3400, min_termfreq = 10)\nrsmaller_dfm <- dfm_trim(rsmaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(rsmaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/text plot reddit-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nrsmaller_fcm <- fcm(rsmaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(rsmaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1185 1185\n```\n:::\n\n```{.r .cell-code}\nrmyFeatures <- names(topfeatures(rsmaller_fcm, 30))\n\n# retain only those top features as part of our matrix\nreven_smaller_fcm <- fcm_select(rsmaller_fcm, pattern = rmyFeatures, selection = \"keep\")\n\n# check dimensions\ndim(reven_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nrsize <- log(colSums(reven_smaller_fcm))\n\n# create plot\ntextplot_network(reven_smaller_fcm, vertex_size = rsize / max(rsize) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/text plot reddit-2.png){width=672}\n:::\n:::\n\n\nIt's interesting to see that the contestants names seem to be much more connected. Overall Reddit's text plot feels a lot more interconnected than Twitter's.\n\nI will also test this for the combined data set.\n\n# All Social Media Tex Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nssmaller_dfm <- dfm_trim(social_corpus_dfm, max_termfreq = 3400, min_termfreq = 10)\nssmaller_dfm <- dfm_trim(ssmaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(ssmaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/text plot socials-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nssmaller_fcm <- fcm(ssmaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(ssmaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1405 1405\n```\n:::\n\n```{.r .cell-code}\nsmyFeatures <- names(topfeatures(ssmaller_fcm, 30))\n\n# retain only those top features as part of our matrix\nseven_smaller_fcm <- fcm_select(ssmaller_fcm, pattern = smyFeatures, selection = \"keep\")\n\n# check dimensions\ndim(seven_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nssize <- log(colSums(seven_smaller_fcm))\n\n# create plot\ntextplot_network(seven_smaller_fcm, vertex_size = ssize / max(ssize) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/text plot socials-2.png){width=672}\n:::\n:::\n\n\nInterestingly the text plot has a similarity more to Reddit than to Twitter. Perhaps this is because Reddit has more tokens as there is not a character limit.\n\n# Dictionary Approach with Twitter\n\nI will now try the dictionary approach using reddit, twitter, and the data combined (allsocialmedia).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitterDfm_nrc <- dfm(tokens(twitter_text_lemmitized,\n                              remove_punct = TRUE),\n                       tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_NRC)\n\ndim(twitterDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7977   10\n```\n:::\n\n```{.r .cell-code}\ntwitterDfm_nrc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 7,977 documents, 10 features (59.55% sparse) and 1 docvar.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     1            1       0    1   1        1        2       1        0\n  text2     1            2       2    1   3        3        3       2        2\n  text3     0            0       0    0   1        0        2       0        0\n  text4     1            0       1    1   0        1        0       1        1\n  text5     0            0       0    0   0        0        0       0        0\n  text6     0            1       1    1   0        1        3       0        0\n       features\ndocs    trust\n  text1     1\n  text2     2\n  text3     1\n  text4     0\n  text5     0\n  text6     0\n[ reached max_ndoc ... 7,971 more documents ]\n```\n:::\n\n```{.r .cell-code}\ntdf_nrc <- convert(twitterDfm_nrc, to = \"data.frame\")\ntdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)\ntdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0\n\nggplot(tdf_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/review sentiment twitter-1.png){width=672}\n:::\n:::\n\n\nHere we can see that for twitter most people were more positive about the show! That does correlate from what I often saw from skimming the tweets.\n\nStill we should double check to make sure that these actually do skew positively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert corpus to DFM using the LSD2015 dictionary\ntwitterDfm_lsd2015 <- dfm(tokens(twitter_text_lemmitized, remove_punct = TRUE),\n                              tolower = TRUE) %>%\n                          dfm_lookup(data_dictionary_LSD2015)\n\n# convert corpus to DFM using the General Inquirer dictionary\ntwitterDfm_geninq <- dfm(tokens(twitter_text_lemmitized, remove_punct = TRUE),\n                             tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_geninqposneg)\n\n# create polarity measure for LSD2015\ntdf_lsd2015 <- convert(twitterDfm_lsd2015, to = \"data.frame\")\ntdf_lsd2015$polarity <- (tdf_lsd2015$positive - tdf_lsd2015$negative)/(tdf_lsd2015$positive + tdf_lsd2015$negative)\ntdf_lsd2015$polarity[which((tdf_lsd2015$positive + tdf_lsd2015$negative) == 0)] <- 0\n\n# look at first few rows\nhead(tdf_lsd2015)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id negative positive neg_positive neg_negative   polarity\n1  text1        2        3            0            0  0.2000000\n2  text2        1        4            0            0  0.6000000\n3  text3        1        2            0            0  0.3333333\n4  text4        2        0            0            0 -1.0000000\n5  text5        0        0            0            0  0.0000000\n6  text6        1        1            0            0  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# create polarity measure for GenInq\ntdf_geninq <- convert(twitterDfm_geninq, to = \"data.frame\")\ntdf_geninq$polarity <- (tdf_geninq$positive - tdf_geninq$negative)/(tdf_geninq$positive + tdf_geninq$negative)\ntdf_geninq$polarity[which((tdf_geninq$positive + tdf_geninq$negative) == 0)] <- 0\n\n# look at first few rows\nhead(tdf_geninq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id positive negative   polarity\n1  text1        5        1  0.6666667\n2  text2        3        1  0.5000000\n3  text3        4        0  1.0000000\n4  text4        0        2 -1.0000000\n5  text5        0        0  0.0000000\n6  text6        1        1  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# create unique names for each dataframe\ncolnames(tdf_nrc) <- paste(\"nrc\", colnames(tdf_nrc), sep = \"_\")\ncolnames(tdf_lsd2015) <- paste(\"lsd2015\", colnames(tdf_lsd2015), sep = \"_\")\ncolnames(tdf_geninq) <- paste(\"geninq\", colnames(tdf_geninq), sep = \"_\")\n\n# now let's compare our estimates\ntsent_df <- merge(tdf_nrc, tdf_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\ntsent_df <- merge(tsent_df, tdf_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\n\nhead(tsent_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\n1      text1         1                1           0        1       1\n2     text10         2                2           2        0       1\n3    text100         0                2           0        1       0\n4   text1000         0                0           0        0       0\n5   text1001         1                0           1        1       2\n6   text1002         0                4           0        1       3\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust nrc_polarity\n1            1            2           1            0         1    0.3333333\n2            2            2           1            0         1    0.0000000\n3            0            0           0            0         1    0.0000000\n4            2            0           1            0         0   -1.0000000\n5            0            2           2            0         2    1.0000000\n6            0            4           0            0         1    1.0000000\n  lsd2015_negative lsd2015_positive lsd2015_neg_positive lsd2015_neg_negative\n1                2                3                    0                    0\n2                1                2                    0                    0\n3                0                0                    0                    0\n4                0                2                    0                    0\n5                0                2                    0                    0\n6                0                5                    0                    0\n  lsd2015_polarity geninq_positive geninq_negative geninq_polarity\n1        0.2000000               5               1       0.6666667\n2        0.3333333               5               1       0.6666667\n3        0.0000000               1               1       0.0000000\n4        1.0000000               2               2       0.0000000\n5        1.0000000               1               0       1.0000000\n6        1.0000000               4               1       0.6000000\n```\n:::\n\n```{.r .cell-code}\ncor(tsent_df$nrc_polarity, tsent_df$lsd2015_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6708384\n```\n:::\n\n```{.r .cell-code}\ncor(tsent_df$nrc_polarity, tsent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5309834\n```\n:::\n\n```{.r .cell-code}\ncor(tsent_df$lsd2015_polarity, tsent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6357448\n```\n:::\n\n```{.r .cell-code}\n# Plot these out. You can update this to check the look of other combinations\nggplot(tsent_df, mapping = aes(x=nrc_polarity, y=lsd2015_polarity)) + \n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept=0,slope=1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/comparing dictionaries twitter-1.png){width=672}\n:::\n:::\n\n\nInterestingly it seems there is a good amount of correlation between the two. Although not as strong as the tutorial it does seem that the dictionary is fairly accurate. Let's test this for both reddit and allsocialmedia.\n\n# Dictionary Approach with Reddit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nredditDfm_nrc <- dfm(tokens(reddit_lemmitized,\n                              remove_punct = TRUE),\n                       tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_NRC)\n\ndim(redditDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10062    10\n```\n:::\n\n```{.r .cell-code}\nredditDfm_nrc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10,062 documents, 10 features (48.16% sparse) and 1 docvar.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     0            0       0    0   0        0        2       0        0\n  text2     0            0       0    0   1        1        1       0        0\n  text3     0            1       0    0   1        0        1       0        0\n  text4     1            2       0    2   1        1        2       0        1\n  text5     0            1       0    1   0        1        1       1        0\n  text6     0            4       1    0   2        0        2       0        1\n       features\ndocs    trust\n  text1     0\n  text2     1\n  text3     1\n  text4     1\n  text5     0\n  text6     2\n[ reached max_ndoc ... 10,056 more documents ]\n```\n:::\n\n```{.r .cell-code}\nrdf_nrc <- convert(redditDfm_nrc, to = \"data.frame\")\nrdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)\nrdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0\n\nggplot(rdf_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit dictionary-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrdf_nrc <- convert(redditDfm_nrc, to = \"data.frame\")\nrdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)\nrdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0\n\nggplot(rdf_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit sentiment-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# convert corpus to DFM using the LSD2015 dictionary\nredditDfm_lsd2015 <- dfm(tokens(reddit_lemmitized, remove_punct = TRUE),\n                              tolower = TRUE) %>%\n                          dfm_lookup(data_dictionary_LSD2015)\n\n# convert corpus to DFM using the General Inquirer dictionary\nredditDfm_geninq <- dfm(tokens(reddit_lemmitized, remove_punct = TRUE),\n                             tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_geninqposneg)\n\n# create polarity measure for LSD2015\nrdf_lsd2015 <- convert(redditDfm_lsd2015, to = \"data.frame\")\nrdf_lsd2015$polarity <- (rdf_lsd2015$positive - rdf_lsd2015$negative)/(rdf_lsd2015$positive + rdf_lsd2015$negative)\nrdf_lsd2015$polarity[which((rdf_lsd2015$positive + rdf_lsd2015$negative) == 0)] <- 0\n\n# look at first few rows\nhead(rdf_lsd2015)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id negative positive neg_positive neg_negative polarity\n1  text1        0        4            0            0        1\n2  text2        0        2            0            0        1\n3  text3        0        0            0            0        0\n4  text4        1        1            0            0        0\n5  text5        1        1            0            0        0\n6  text6        0        1            0            0        1\n```\n:::\n\n```{.r .cell-code}\n# create polarity measure for GenInq\nrdf_geninq <- convert(redditDfm_geninq, to = \"data.frame\")\nrdf_geninq$polarity <- (rdf_geninq$positive - rdf_geninq$negative)/(rdf_geninq$positive + rdf_geninq$negative)\nrdf_geninq$polarity[which((rdf_geninq$positive + rdf_geninq$negative) == 0)] <- 0\n\n# look at first few rows\nhead(rdf_geninq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id positive negative polarity\n1  text1        6        0      1.0\n2  text2        4        1      0.6\n3  text3        3        0      1.0\n4  text4        3        1      0.5\n5  text5        1        0      1.0\n6  text6        4        1      0.6\n```\n:::\n\n```{.r .cell-code}\n# create unique names for each dataframe\ncolnames(rdf_nrc) <- paste(\"nrc\", colnames(rdf_nrc), sep = \"_\")\ncolnames(rdf_lsd2015) <- paste(\"lsd2015\", colnames(rdf_lsd2015), sep = \"_\")\ncolnames(rdf_geninq) <- paste(\"geninq\", colnames(rdf_geninq), sep = \"_\")\n\n# now let's compare our estimates\nrsent_df <- merge(rdf_nrc, rdf_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\nrsent_df <- merge(rsent_df, rdf_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\n\nhead(rsent_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\n1      text1         0                0           0        0       0\n2     text10         0                3           1        2       4\n3    text100         0                0           0        0       0\n4   text1000         2                1           1        2       1\n5  text10000         0                2           0        2       2\n6  text10001         0                1           0        0       1\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust nrc_polarity\n1            0            2           0            0         0    1.0000000\n2            6            9           2            2         4    0.2000000\n3            0            0           0            0         1    0.0000000\n4            2            3           2            1         2    0.2000000\n5            1            2           0            0         2    0.3333333\n6            0            3           0            0         1    1.0000000\n  lsd2015_negative lsd2015_positive lsd2015_neg_positive lsd2015_neg_negative\n1                0                4                    0                    0\n2                7                9                    0                    0\n3                0                0                    0                    0\n4                2                3                    0                    0\n5                2                4                    0                    0\n6                0                4                    0                    0\n  lsd2015_polarity geninq_positive geninq_negative geninq_polarity\n1        1.0000000               6               0             1.0\n2        0.1250000              18               6             0.5\n3        0.0000000               0               0             0.0\n4        0.2000000               3               2             0.2\n5        0.3333333               3               2             0.2\n6        1.0000000               8               2             0.6\n```\n:::\n\n```{.r .cell-code}\ncor(rsent_df$nrc_polarity, rsent_df$lsd2015_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.564723\n```\n:::\n\n```{.r .cell-code}\ncor(rsent_df$nrc_polarity, rsent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4608733\n```\n:::\n\n```{.r .cell-code}\ncor(rsent_df$lsd2015_polarity, rsent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5894939\n```\n:::\n\n```{.r .cell-code}\n# Plot these out. You can update this to check the look of other combinations\nggplot(rsent_df, mapping = aes(x=nrc_polarity, y=lsd2015_polarity)) + \n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept=0,slope=1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit sentiment-2.png){width=672}\n:::\n:::\n\n\n# Dictionary Approach with All Social Media\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsocialDfm_nrc <- dfm(tokens(social_lemmitized,\n                              remove_punct = TRUE),\n                       tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_NRC)\n\ndim(socialDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18039    10\n```\n:::\n\n```{.r .cell-code}\nsocialDfm_nrc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 18,039 documents, 10 features (53.35% sparse) and 1 docvar.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     0            2       0    0   2        4        3       2        1\n  text2     0            0       0    0   0        0        2       0        0\n  text3     0            0       0    0   0        0        1       0        0\n  text4     2            2       1    3   2        4        6       3        2\n  text5     2            2       1    3   2        4        6       3        2\n  text6     0            0       0    0   1        0        2       0        0\n       features\ndocs    trust\n  text1     1\n  text2     2\n  text3     0\n  text4     1\n  text5     1\n  text6     2\n[ reached max_ndoc ... 18,033 more documents ]\n```\n:::\n\n```{.r .cell-code}\ndf_nrc <- convert(socialDfm_nrc, to = \"data.frame\")\ndf_nrc$polarity <- (df_nrc$positive - df_nrc$negative)/(df_nrc$positive + df_nrc$negative)\ndf_nrc$polarity[which((df_nrc$positive + df_nrc$negative) == 0)] <- 0\n\nggplot(df_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/social media dictionary-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert corpus to DFM using the LSD2015 dictionary\nsocialDfm_lsd2015 <- dfm(tokens(social_lemmitized, remove_punct = TRUE),\n                              tolower = TRUE) %>%\n                          dfm_lookup(data_dictionary_LSD2015)\n\n# convert corpus to DFM using the General Inquirer dictionary\nsocialDfm_geninq <- dfm(tokens(social_lemmitized, remove_punct = TRUE),\n                             tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_geninqposneg)\n\n# create polarity measure for LSD2015\ndf_lsd2015 <- convert(socialDfm_lsd2015, to = \"data.frame\")\ndf_lsd2015$polarity <- (df_lsd2015$positive - df_lsd2015$negative)/(df_lsd2015$positive + df_lsd2015$negative)\ndf_lsd2015$polarity[which((df_lsd2015$positive + df_lsd2015$negative) == 0)] <- 0\n\n# look at first few rows\nhead(df_lsd2015)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id negative positive neg_positive neg_negative polarity\n1  text1        0        3            0            0      1.0\n2  text2        0        4            0            0      1.0\n3  text3        0        0            0            0      0.0\n4  text4        2        3            0            0      0.2\n5  text5        2        3            0            0      0.2\n6  text6        0        4            0            0      1.0\n```\n:::\n\n```{.r .cell-code}\n# create polarity measure for GenInq\ndf_geninq <- convert(socialDfm_geninq, to = \"data.frame\")\ndf_geninq$polarity <- (df_geninq$positive - df_geninq$negative)/(df_geninq$positive + df_geninq$negative)\ndf_geninq$polarity[which((df_geninq$positive + df_geninq$negative) == 0)] <- 0\n\n# look at first few rows\nhead(df_geninq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  doc_id positive negative   polarity\n1  text1        5        0  1.0000000\n2  text2        3        1  0.5000000\n3  text3        1        3 -0.5000000\n4  text4        5        4  0.1111111\n5  text5        5        4  0.1111111\n6  text6        5        1  0.6666667\n```\n:::\n\n```{.r .cell-code}\n# create unique names for each dataframe\ncolnames(df_nrc) <- paste(\"nrc\", colnames(df_nrc), sep = \"_\")\ncolnames(df_lsd2015) <- paste(\"lsd2015\", colnames(df_lsd2015), sep = \"_\")\ncolnames(df_geninq) <- paste(\"geninq\", colnames(df_geninq), sep = \"_\")\n\n# now let's compare our estimates\nsent_df <- merge(df_nrc, df_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\nsent_df <- merge(sent_df, df_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\n\nhead(sent_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\n1      text1         0                2           0        0       2\n2     text10         1                5           0        2       3\n3    text100         1                1           0        1       0\n4   text1000         3                1           3        2       1\n5  text10000         0                2           0        0       2\n6  text10001         0                1           0        0       1\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust nrc_polarity\n1            4            3           2            1         1   -0.1428571\n2            1            6           0            1         4    0.7142857\n3            1            0           0            1         0   -1.0000000\n4            3            2           2            1         1   -0.2000000\n5            0            3           0            1         2    1.0000000\n6            0            1           0            1         1    1.0000000\n  lsd2015_negative lsd2015_positive lsd2015_neg_positive lsd2015_neg_negative\n1                0                3                    0                    0\n2                5                5                    0                    0\n3                1                0                    0                    0\n4                5                6                    0                    0\n5                0                3                    0                    0\n6                0                1                    0                    0\n  lsd2015_polarity geninq_positive geninq_negative geninq_polarity\n1       1.00000000               5               0      1.00000000\n2       0.00000000               8               5      0.23076923\n3      -1.00000000               1               2     -0.33333333\n4       0.09090909               7               6      0.07692308\n5       1.00000000               2               0      1.00000000\n6       1.00000000               1               0      1.00000000\n```\n:::\n\n```{.r .cell-code}\ncor(sent_df$nrc_polarity, sent_df$lsd2015_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6179578\n```\n:::\n\n```{.r .cell-code}\ncor(sent_df$nrc_polarity, sent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5042873\n```\n:::\n\n```{.r .cell-code}\ncor(sent_df$lsd2015_polarity, sent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6198002\n```\n:::\n\n```{.r .cell-code}\n# Plot these out. You can update this to check the look of other combinations\nggplot(sent_df, mapping = aes(x=nrc_polarity, y=lsd2015_polarity)) + \n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept=0,slope=1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/all social media sentiment-1.png){width=672}\n:::\n:::\n\n\n# Choosing a Dictionary\n\nInterestingly compared to Twitter, Reddit and \"allsocialmedia\" were a fair amount lower in correlation. Although it was still a fairly high correlation this may because of the different types of words used by Reddit.\n\nHowever when you compare the graphs all three are nearly identical in nature.\n\nI think I will use the NRC dictionary. I have chosen this one because I believe it's the strongest dictionary for my set of data because it explores through multiple categories of emotions. While both other dictionaries are good, NRC is a much larger dictionary. If you are interested I have found some more information on the dictionaries [here](https://journodev.tech/blog-12-main-dictionaries-for-sentiment-analysis/)!\n\n# Choosing a Research Question\n\nAfter looking through all of the data using the dictionary method I have decided to go back to this research question: How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan*?\n\nI have decided to do this because I believe there's more information that could be compared. Additionally while it is useful to know an overall sentiment for social media, I think that since each user base may be different we could see more interesting results when it is not mixed together.\n\n# Final Thoughts (TLDR) Currently Last Week\n\n## Final thoughts\n\n-   I have found a way to keep the date column in the summary but remove it from the actual tokens afterwards.\n\n-   I have made a list of stop words that are related to the title of the show from Twitter.\n\n    -   I have also included the removal of the word \"show\" as for tweets it only referenced show as a TV series. However for Reddit I have not removed the word show as Redditors seemed to use the word show outside of of meaning a TV show.\n\n    -   Reddit also did not have the same issues with the title of the show due to all of the posts in the sub reddit being related to Love is Blind Japan.\n\n-   I have decided to use the NRC dictionary.\n\n-   I have changed my question: How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan*?\n\n    -   This means I will no longer use the combined data set.\n\n-   I am unsure how I will use co-occurrence.\n\n## Future Work\n\nHere are a few things I'd like to do in blog post 5:\n\n-   I would like to look at co-occurrence more closely and decide if the code I have now is the one I would like to use for the final project. I currently have chosen it based off prior issues (such as having the show's name showing up).\n\n    -   This may include using a maxium document frequency and a minimum word frequency.\n\n-   I would like to see if I can create an emotional rating comparison using the dictionary method. This would include using the NRC for twitter and reddit to compare the average emotional response.\n\n# Final Code Moving Forward\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- subset(reddit, detect_language(reddit) == \"en\")\nreddit_corpus <- corpus(reddit_corpus)\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\nredditsummary <- summary(reddit_corpus)\n\nreddit_corpus <- trimws(gsub(\"[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}\", \"\", reddit_corpus))\n\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T, \n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nreddit_corpus_dfm <- reddit_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(reddit_corpus_dfm, max_words=200, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit word cloud-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_corpus <- subset(twitter, detect_language(twitter) == \"en\")\ntwitter_corpus <- corpus(twitter_corpus)\ntwitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]\ntwittersummary <- summary(twitter_corpus)\n\ntwitter_corpus <- trimws(gsub(\"[[:digit:]]{1,4}-[[:digit:]]{1,4}-[[:digit:]]{1,4}\", \"\", twitter_corpus))\n\nmystopwords <- c(\"love is blind japan\", \"#loveisbindjapan\", \"#LoveIsBlindJapan\",\"Love Is Blind Japan\",\"Love is Blind Japan\", \"Love Is Blind: Japan\", \"#loveisblind\", \"ラブイズブラインドjapan\", \"#ラブイズブラインドjapan\", \"loveisblind\", \"#loveisblind2\", \"blind:japan\", \"blind\", \"show\")\n\ntwitter_corpus_tokens <- tokens(twitter_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T) %>% \n  tokens_tolower() %>% \n  tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>% \n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized <- tokens_replace(twitter_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\ntwitter_corpus_dfm <- twitter_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/twitter word cloud-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(twitter_corpus_text_dfm, max_termfreq = 3400, min_termfreq = 10)\nsmaller_dfm <- dfm_trim(smaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(smaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/twitter cooccurrence-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 440 440\n```\n:::\n\n```{.r .cell-code}\nmyFeatures <- names(topfeatures(smaller_fcm, 30))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/twitter cooccurrence-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nrsmaller_dfm <- dfm_trim(reddit_corpus_dfm, max_termfreq = 3400, min_termfreq = 10)\nrsmaller_dfm <- dfm_trim(rsmaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(rsmaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit cooccurrence-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nrsmaller_fcm <- fcm(rsmaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(rsmaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1185 1185\n```\n:::\n\n```{.r .cell-code}\nrmyFeatures <- names(topfeatures(rsmaller_fcm, 30))\n\n# retain only those top features as part of our matrix\nreven_smaller_fcm <- fcm_select(rsmaller_fcm, pattern = rmyFeatures, selection = \"keep\")\n\n# check dimensions\ndim(reven_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nrsize <- log(colSums(reven_smaller_fcm))\n\n# create plot\ntextplot_network(reven_smaller_fcm, vertex_size = rsize / max(rsize) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit cooccurrence-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nredditDfm_nrc <- dfm(tokens(reddit_lemmitized,\n                              remove_punct = TRUE),\n                       tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_NRC)\n\ndim(redditDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10062    10\n```\n:::\n\n```{.r .cell-code}\nredditDfm_nrc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10,062 documents, 10 features (48.16% sparse) and 1 docvar.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     0            0       0    0   0        0        2       0        0\n  text2     0            0       0    0   1        1        1       0        0\n  text3     0            1       0    0   1        0        1       0        0\n  text4     1            2       0    2   1        1        2       0        1\n  text5     0            1       0    1   0        1        1       1        0\n  text6     0            4       1    0   2        0        2       0        1\n       features\ndocs    trust\n  text1     0\n  text2     1\n  text3     1\n  text4     1\n  text5     0\n  text6     2\n[ reached max_ndoc ... 10,056 more documents ]\n```\n:::\n\n```{.r .cell-code}\nrdf_nrc <- convert(redditDfm_nrc, to = \"data.frame\")\nrdf_nrc$polarity <- (rdf_nrc$positive - rdf_nrc$negative)/(rdf_nrc$positive + rdf_nrc$negative)\nrdf_nrc$polarity[which((rdf_nrc$positive + rdf_nrc$negative) == 0)] <- 0\n\nggplot(rdf_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/reddit dfm dictionary-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitterDfm_nrc <- dfm(tokens(twitter_lemmitized,\n                              remove_punct = TRUE),\n                       tolower = TRUE) %>%\n                    dfm_lookup(data_dictionary_NRC)\n\ndim(twitterDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7977   10\n```\n:::\n\n```{.r .cell-code}\ntwitterDfm_nrc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 7,977 documents, 10 features (59.55% sparse) and 1 docvar.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     1            1       0    1   1        1        2       1        0\n  text2     1            2       2    1   3        3        3       2        2\n  text3     0            0       0    0   1        0        2       0        0\n  text4     1            0       1    1   0        1        0       1        1\n  text5     0            0       0    0   0        0        0       0        0\n  text6     0            1       1    1   0        1        3       0        0\n       features\ndocs    trust\n  text1     1\n  text2     2\n  text3     1\n  text4     0\n  text5     0\n  text6     0\n[ reached max_ndoc ... 7,971 more documents ]\n```\n:::\n\n```{.r .cell-code}\ntdf_nrc <- convert(twitterDfm_nrc, to = \"data.frame\")\ntdf_nrc$polarity <- (tdf_nrc$positive - tdf_nrc$negative)/(tdf_nrc$positive + tdf_nrc$negative)\ntdf_nrc$polarity[which((tdf_nrc$positive + tdf_nrc$negative) == 0)] <- 0\n\nggplot(tdf_nrc) + \n  geom_histogram(aes(x=polarity)) + \n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](BlogPost4_MollyHackbarth_files/figure-html/twitter dfm dictionary-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "BlogPost4_MollyHackbarth_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}