{
  "hash": "7e3242f1748e5c7b5950e3d2a1265a27",
  "result": {
    "markdown": "---\ntitle: \"Sentiment Analysis on Covid-19 Vaccine\"\neditor: visual\ndesription: \"Analysis of Data\"\ndate: \"10/30/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog Post 4\n  - Kaushika \n---\n\n\nIn this project, I am going to predict the Sentiments of COVID-19 Vaccination tweets. The data I have used is collecting tweets on the topic \"Covid-19 Vaccination\" (web scraping) and preparing the data. The data was gathered from Twitter and I'm going to use the R environment to implement this project. During the pandemic, lots of studies carried out analyses using Twitter data.\n\nIn the previous blog I have mentioned that I have access to only the last 7 days of tweets. However, I have applied for academic access to Twitter API that allows me to collect more tweets for my analysis. I will be using the Premium search rather than the Standard search for tweets using Twitter API.\n\n##Loading important libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(twitteR) #R package which provides access to the Twitter API\nlibrary(tm) #Text mining in R\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate) #Lubridate is an R package that makes it easier to work with dates and times.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda) #Makes it easy to manage texts in the form of a corpus.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud) #Visualize differences and similarity between documents\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(ggplot2) #For creating Graphics \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2) # Transform data between wide and long formats.\nlibrary(dplyr) #Provides a grammar of data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:twitteR':\n\n    id, location\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) #Helps to transform and tidy data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nâ”€â”€ Attaching packages\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntidyverse 1.3.2 â”€â”€\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nâœ” tibble  3.1.8     âœ” purrr   0.3.5\nâœ” tidyr   1.2.1     âœ” stringr 1.4.1\nâœ” readr   2.1.3     âœ” forcats 0.5.2\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– ggplot2::annotate()      masks NLP::annotate()\nâœ– lubridate::as.difftime() masks base::as.difftime()\nâœ– lubridate::date()        masks base::date()\nâœ– dplyr::filter()          masks stats::filter()\nâœ– dplyr::id()              masks twitteR::id()\nâœ– lubridate::intersect()   masks base::intersect()\nâœ– dplyr::lag()             masks stats::lag()\nâœ– dplyr::location()        masks twitteR::location()\nâœ– lubridate::setdiff()     masks base::setdiff()\nâœ– lubridate::union()       masks base::union()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) #Applies the principles of the tidyverse to analyzing text.\nlibrary(tidyr) #Helps to get tidy data\nlibrary(gridExtra) #Arrange multiple grid-based plots on a page, and draw tables\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid) #Produce graphical output\nlibrary(rtweet) #Collecting Twitter Data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nThe following object is masked from 'package:twitteR':\n\n    lookup_statuses\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet) #Returns a data frame in which each row represents a sentence from the original file\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'syuzhet'\n\nThe following object is masked from 'package:rtweet':\n\n    get_tokens\n```\n:::\n:::\n\n\n## Scraping Data from Twitter\n\nAfter getting access to the Twitter API I can run the following (replacing \\###### by my specific credentials) and search for tweets. (\"\\######\" used for protection)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# twitter keys and tokens\napi_key <- \"######\"\napi_secret <- \"######\"\naccess_token <- \"######\"\naccess_token_secret <- \"######\"\n\n# create token for rtweet\ntoken <- create_token(\n  app = \"######\",\n  api_key,\n  api_secret,\n  access_token,\n  access_token_secret,\n  set_renv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `create_token()` was deprecated in rtweet 1.0.0.\nâ„¹ See vignette('auth') for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSaving auth to 'C:\\Users\\srika\\AppData\\Roaming/R/config/R/rtweet/\ncreate_token.rds'\n```\n:::\n\n```{.r .cell-code}\nsetup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in check_twitter_oauth(): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\n#what to search\n\n#Searching for tweets using terms covid + 19 + vaccine and filtering out the retweets to avoid repetitions. After that I converted the list of tweets into a data frame.\n\ntweets_covid = searchTwitter(\"covid+19+vaccine -filter:retweets\", n = 20000, lang = \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twInterfaceObj$doAPICall(cmd, params, \"GET\", ...): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\ntweets.df = twListToDF(tweets_covid)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twListToDF(tweets_covid): object 'tweets_covid' not found\n```\n:::\n\n```{.r .cell-code}\nfor (i in 1:nrow(tweets.df)) {\n    if (tweets.df$truncated[i] == TRUE) {\n        tweets.df$text[i] <- gsub(\"[[:space:]]*$\",\"...\",tweets.df$text[i])\n    }\n}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(tweets.df): object 'tweets.df' not found\n```\n:::\n\n```{.r .cell-code}\n#Saving the collected tweets into a csv file.\nwrite.csv(tweets.df, file = \"covidtweets.csv\", row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'tweets.df' not found\n```\n:::\n:::\n\n\n## Reading the csv file\n\nThe csv file has approximately 15,000 tweets on the topic \"Covid 19 Vaccination\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_19_vaccination <- read.csv(\"covidtweets.csv\", header = T)\nstr(covid_19_vaccination)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t15040 obs. of  16 variables:\n $ text         : chr  \"@1goodtern Who suffer the most, vaccine and mask ðŸ˜· off, not thinking long term effects with COVID-19 being a ma\"| __truncated__ \"@palminder1990 Google much?\\nhttps://t.co/SXOBS5INdJ\" \"Arrest #JoeBiden for the assault on the #american people forcing and conning them to take the #vaccine forâ€¦ htt\"| __truncated__ \"@9NewsSyd Remember that time \\\"conspiracy theorists\\\" said that the Covid-19 Vaccine was undertested, wouldn't \"| __truncated__ ...\n $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ favoriteCount: int  0 0 0 0 0 0 0 2 0 0 ...\n $ replyToSN    : chr  \"1goodtern\" \"palminder1990\" NA \"9NewsSyd\" ...\n $ created      : chr  \"2022-10-31 01:35:17\" \"2022-10-31 01:33:07\" \"2022-10-31 01:27:07\" \"2022-10-31 01:24:45\" ...\n $ truncated    : logi  TRUE FALSE TRUE TRUE TRUE TRUE ...\n $ replyToSID   : num  1.59e+18 1.59e+18 NA 1.59e+18 NA ...\n $ id           : num  1.59e+18 1.59e+18 1.59e+18 1.59e+18 1.59e+18 ...\n $ replyToUID   : num  9.61e+17 1.49e+18 NA 1.72e+08 NA ...\n $ statusSource : chr  \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" ...\n $ screenName   : chr  \"ecmoyer\" \"henri_gg\" \"Twitgovbot\" \"DjrellAZDelta\" ...\n $ retweetCount : int  0 0 0 0 0 0 0 0 0 0 ...\n $ isRetweet    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ retweeted    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ longitude    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ latitude     : num  NA NA NA NA NA NA NA NA NA NA ...\n```\n:::\n:::\n\n\n##Build Corpus A corpus, or collection of text documents(in this case tweets), is the primary document management structure in the R package \"tm\" (text mining).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- iconv(covid_19_vaccination$text, to = \"utf-8\")\ncorpus <- Corpus(VectorSource(corpus))\ninspect(corpus[1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 5\n\n[1] @1goodtern Who suffer the most, vaccine and mask ðŸ˜· off, not thinking long term effects with COVID-19 being a mass dâ€¦ https://t.co/hxabqyjaIn...  \n[2] @palminder1990 Google much?\\nhttps://t.co/SXOBS5INdJ                                                                                             \n[3] Arrest #JoeBiden for the assault on the #american people forcing and conning them to take the #vaccine forâ€¦ https://t.co/VKh5GBecFn...           \n[4] @9NewsSyd Remember that time \"conspiracy theorists\" said that the Covid-19 Vaccine was undertested, wouldn't work eâ€¦ https://t.co/qNAoety4Y2...  \n[5] One squat, deadlift, or benchpress session a day; keeps #COVID19 away!\\n\\nRun, stretch or dance: #Exercise could imprâ€¦ https://t.co/Gh60QDwcvZ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Suppress warnings in the global setting.\noptions(warn=-1)\n```\n:::\n\n\n#Cleaning the Data : Data Pre-Processing Cleaning the data include removing stopwords, numbers, punctuation, and other elements. Stopwords are words that have no sentimental meaning, such as conjunctions, pronouns, negations, etc. Common yet meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc. are also eliminated in this case.\n\nHere we follow a particular order of removing Usernames before Punctuations. Since the symbol '\\@' would be removed if we remove punctuations first and that would create an issue while removing usernames after that since the '\\@' symbol would not be detected anymore.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# clean text\nremoveUsername <- function(x) gsub('@[^[:space:]]*', '', x) #Removes usernames\nremoveURL <- function(x) gsub('http[[:alnum:]]*', '', x) #Removes URLs attached to tweets\nremoveNumPunct<- function(x) gsub(\"[^[:alpha:][:space:]]*\",\"\",x) #Remove Punctuations\n\n#Text Mining Functions\ncleandata <- tm_map(corpus, PlainTextDocument) #Function to create plain text documents.\ncleandata <- tm_map(cleandata, content_transformer(removeUsername)) #Function to remove Usernames attached to the text.\ncleandata <- tm_map(cleandata, content_transformer(removeURL)) #Function to remove URLs attached to the text.\ncleandata <- tm_map(cleandata, content_transformer(tolower)) #Function to convert text into lowercase.\ncleandata <- tm_map(cleandata, content_transformer(removeNumPunct)) #Function to remove Punctuations attached to text.\ncleandata <- tm_map(cleandata, content_transformer(removeNumbers)) # #Function to remove Numbers attached to texts.\ncleandata <- tm_map(cleandata, removeWords, stopwords(\"english\"))\n\n#Removing meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc\ncleandata <- tm_map(cleandata, removeWords, c('covid','vaccination', \n                                            'vaccinations','vaccine','vaccines',\n                                            'vaccinated', \"corona\", \n                                            \"coronavirus\"))\ncleandata <- tm_map(cleanset, gsub,\n                   pattern = 'available',\n                   replacement = 'availability')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleanset, gsub, pattern = \"available\", replacement = \"availability\"): object 'cleanset' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, stripWhitespace) #Function to strip extra whitespace from a text document.\ninspect(cleandata[1:5]) #Inspecting the first 5 rows.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 5\n\n[1]  suffer mask thinking long term effects mass d tcohxabqyjain                                    \n[2]  google much tcosxobsindj                                                                       \n[3] arrest joebiden assault american people forcing conning take tcovkhgbecfn                       \n[4]  remember time conspiracy theorists said undertested wouldnt work e tcoqnaoetyy                 \n[5] one squat deadlift benchpress session day keeps away run stretch dance exercise impr tcoghqdwcvz\n```\n:::\n:::\n\n\n## Term Document Matrix\n\nThe second function constructs the term-document matrix, that describes the frequency of terms that occur in a collection of documents. This matrix has terms in the first column and documents across the top as individual column names. The rows are the terms (words) and the columns are the documents (tweets). So I made a tdm of our cleandata in the next step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntdm <- TermDocumentMatrix(cleandata)\ntdm <- as.matrix(tdm)\ntdm[1:5, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Docs\nTerms     1 2 3 4 5 6 7 8 9 10\n  effects 1 0 0 0 0 0 1 0 0  0\n  long    1 0 0 0 0 0 0 0 0  0\n  mask    1 0 0 0 0 0 0 0 0  0\n  mass    1 0 0 0 0 0 0 0 0  0\n  suffer  1 0 0 0 0 0 0 0 0  0\n```\n:::\n:::\n\n\n## Analysis of the Most Frequent Words - Word Cloud\nA wordcloud is a collection of words displayed in different sizes. The bigger and bolder the word appears, the more often it is mentioned within the tweets and the more important it is.\nWords like \"pfizer\", \"booster\", \"flu\", \"biden\", \"people\" , \"get\" seem to be appearing more\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# row sums\nw <- rowSums(tdm) # how often appears each word?\nw <- subset(w, w>=3000)\nw <- sort(rowSums(tdm))\n\n# wordcloud\noptions(repr.plot.width=14, repr.plot.height=10)\nwordcloud(words = names(w),\n          freq = w,\n          colors=brewer.pal(8, \"Dark2\"),\n          random.color = TRUE,\n          max.words = 100,\n          scale = c(4, 0.04))\n```\n\n::: {.cell-output-display}\n![](Covid19SentimentAnalysis_BlogPost4_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNext I will Perform Sentiment Analysis and will keep updating the same:\n",
    "supporting": [
      "Covid19SentimentAnalysis_BlogPost4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}