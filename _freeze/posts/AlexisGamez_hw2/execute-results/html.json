{
  "hash": "ed725ed6512889e13ebf5fbed9a057c3",
  "result": {
    "markdown": "---\ntitle: \"Blog Post #2: Gathering Data\"\nauthor: \"Alexis Gamez\"\ndesription: \"Studying text-as-data as it relates to eating bugs\"\ndate: \"11/03/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blogpost2\n  - Alexis Gamez\n  - research\n  - academic articles\n---\n\n# Setup\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(readr)\nlibrary(devtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: usethis\n```\n:::\n\n```{.r .cell-code}\nlibrary(plyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    compact\n```\n:::\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rvest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nlibrary(rtweet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n:::\n\n```{.r .cell-code}\nlibrary(twitteR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'twitteR'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rtweet':\n\n    lookup_statuses\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:plyr':\n\n    id\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    id, location\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"mMatrix\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"replValueSp\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 6 of 6 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n</details>\n\n# **Data Sources**\nFor this assignment, I gathered the data for my corpus from the Twitter social media platform. The 'rtweet' R package was used heavily in the gathering and preliminary analysis of the data I used. In order to extract twitter data to R, I needed to first create a developer account to gain the appropriate permissions. Once the account was made, I was able to create a new project through the developer app and connect it to R. From there, I was able to begin gathering data and conducting a preliminary analysis of what I could find so far.\n\n# **Gathering Data**\n\nThe first step in data analysis, is gathering the data you intend to analyze! I started by gathering as many tweets as possible related to the subject of my project. At this point, the goal of my project is to conduct a sentiment analysis surrounding the perception of eating insects among twitter users. So, I pulled tweets containing the keywords `eating` and `bugs/insects`. Finding it unnecessary, I excluded re-tweets and chose to restrict source language to only those in English. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Pull together tweets containing keywords 'eating` and `bugs/insects`.\ntweet_bugs <- search_tweets(\"eating bugs OR insects\", n = 10000,\n                             type = \"mixed\",\n                             include_rts = FALSE,\n                             lang = \"en\")\n```\n:::\n\n\n# **Creating a Corpus**\n\nFrom the previous chunk, I was able to gather a total of 1,533 tweets along with their metadata. The following chunk is dedicated toward cleaning up the data a bit and pulling out the information I need in order to conduct the analysis. First I extracted the `full_text` column from the `tweet_bugs` object I created and stored it as `tweet_text`. From there, I converted the text object to a corpus, i.e. `tweet_corpus`. Lastly, I used the `summary` function to summarize corpus information like sentence and token count per tweet (for some reason, the summary function limited itself to only 100 out of the 1,533 entries. A goal for the future is to figure out how to extend it so that I can summarize the full corpus data).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Separate out the text from tweet_bugs and build the corpus.\ntweet_text <- as.vector.data.frame(tweet_bugs$full_text, mode = \"any\")\ntweet_corpus <- corpus(tweet_text)\ntweet_summary <- summary(tweet_corpus)\n```\n:::\n\n\nWhile not entirely useful, I also decided to add in a tweet count indicator to number the tweets. Once again, the count only extended up to the first 100 entries. Hopefully, there is a workaround to include the remainder of the entries, but ultimately it isn't crucial when conducting sentiment analysis.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n#Creating tweet count indicator (i.e. Count).\ntweet_summary$Count <- as.numeric(str_extract(tweet_summary$Text,\"[0-9]+\"))\ntweet_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 957 documents, showing 100 documents:\n\n    Text Types Tokens Sentences Count\n   text1    14     14         1     1\n   text2    43     51         3     2\n   text3    41     52         1     3\n   text4    47     55         2     4\n   text5    48     56         4     5\n   text6    29     32         1     6\n   text7    17     24         2     7\n   text8    48     60         4     8\n   text9    14     16         2     9\n  text10    44     53         7    10\n  text11    28     29         3    11\n  text12    20     22         1    12\n  text13    54     65         3    13\n  text14    13     13         1    14\n  text15    29     32         1    15\n  text16    42     45         1    16\n  text17    49     59         3    17\n  text18    13     13         1    18\n  text19    32     38         1    19\n  text20    13     14         1    20\n  text21    45     65         7    21\n  text22    24     27         2    22\n  text23    31     41         4    23\n  text24    42     48         3    24\n  text25    15     15         1    25\n  text26    31     36         4    26\n  text27    45     57         3    27\n  text28    57     61         4    28\n  text29     8      9         1    29\n  text30    38     48         3    30\n  text31    15     15         1    31\n  text32    14     14         1    32\n  text33    22     24         3    33\n  text34     9      9         1    34\n  text35     5      5         1    35\n  text36    47     52         6    36\n  text37    15     15         1    37\n  text38    26     30         3    38\n  text39    21     21         1    39\n  text40    21     22         1    40\n  text41     9     13         1    41\n  text42    23     29         3    42\n  text43    28     37         1    43\n  text44    15     16         1    44\n  text45    40     48         1    45\n  text46    28     35         3    46\n  text47    18     26         4    47\n  text48    28     29         1    48\n  text49    17     18         1    49\n  text50    43     58         1    50\n  text51    25     28         1    51\n  text52    39     49         1    52\n  text53    14     17         4    53\n  text54    39     51         3    54\n  text55    48     58         2    55\n  text56    22     28         1    56\n  text57    46     53         4    57\n  text58    17     21         1    58\n  text59    12     12         1    59\n  text60    41     45         3    60\n  text61    11     11         1    61\n  text62    25     26         2    62\n  text63    32     34         4    63\n  text64    13     14         1    64\n  text65     9     12         3    65\n  text66     7      7         1    66\n  text67    27     32         1    67\n  text68    38     42         3    68\n  text69    34     36         2    69\n  text70    10     10         1    70\n  text71    28     30         1    71\n  text72    13     13         1    72\n  text73    56     61         4    73\n  text74    34     41         4    74\n  text75    43     59         5    75\n  text76    22     24         2    76\n  text77    29     33         2    77\n  text78    32     34         1    78\n  text79     9      9         1    79\n  text80    33     39         1    80\n  text81    41     46         3    81\n  text82    11     12         2    82\n  text83    20     21         1    83\n  text84    42     46         3    84\n  text85    30     34         2    85\n  text86    21     31         3    86\n  text87    22     28         2    87\n  text88    10     10         1    88\n  text89    21     24         4    89\n  text90    37     52         3    90\n  text91     7      7         1    91\n  text92    26     29         1    92\n  text93     9      9         1    93\n  text94    41     50         4    94\n  text95    12     13         1    95\n  text96    44     48         2    96\n  text97    41     45         3    97\n  text98    10     10         1    98\n  text99     8      8         1    99\n text100    37     43         3   100\n```\n:::\n:::\n\n</details>\n\nAdditionally, because Twitter's base developer guidelines only allow me to extract tweets created within the last 6-9 days, I tried to pull as many within this time frame as possible and write them to a csv document for storage. My hope is that as my project progresses, I can accumulate and append additional tweets to my existing corpus so that by the end I have a larger data frame to work with.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Creating a new document to store existing corpus and hopefully add more over time.\nwrite.csv(tweet_corpus, file = \"eating_bugs_tweets_11_3_22.csv\", row.names = FALSE)\n```\n:::\n\n\n# **Preliminary Analysis**\n\nBeginning my analysis of the data, I decided to run the `docvars` function to check for metadata (which after extracting the text column from the the initial `tweet_bugs` object, I figured would no longer include any).\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n#Trying to pull metadata, but there no longer is any.\ndocvars(tweet_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndata frame with 0 columns and 957 rows\n```\n:::\n:::\n\n</details>\n\nAfterwards, I decided to split the corpus down into sentence level documents. I thought that by doing this, I would be able to call back to this object eventually and analyze sentiment within each individual sentence, but I realized and felt as though this spread my data a bit too thin and thought that it would be better to split it down to the tweet level (`documents`) instead. That way, I can consolidate the data a bit and effectively analyze the sentiment of the person behind each individual tweet. \n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n#These lines of code separate out each sentence within the tweet corpus. Only problem was, many tweets contained multiple sentences and this spreads the data very thin.\nndoc(tweet_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 957\n```\n:::\n\n```{.r .cell-code}\ntweet_corpus_sentences <- corpus_reshape(tweet_corpus, to = \"sentences\")\nndoc(tweet_corpus_sentences)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2051\n```\n:::\n\n```{.r .cell-code}\n#From there, I instead decided to separate them by tweet, i.e. document, to get a better idea of each individuals writing style and opinion. I felt as though 'sentences' was spreading it too thin.\ntweet_corpus_document <- corpus_reshape(tweet_corpus, to = \"documents\")\nndoc(tweet_corpus_document)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 957\n```\n:::\n\n```{.r .cell-code}\nsummary(tweet_corpus_document, n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 957 documents, showing 5 documents:\n\n  Text Types Tokens Sentences\n text1    14     14         1\n text2    43     51         3\n text3    41     52         1\n text4    47     55         2\n text5    48     56         4\n```\n:::\n:::\n\n</details>\n\n## *Tokens*\n\nNext I decided break the corpus down to the token level and conduct a surface level analysis on the use of certain keywords. The following code chunk is indicative of my thought process when creating the `tweet_token` object. First, I created the base object, then removed any punctuation, and finally removed all numbers.  \n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n#My next step was to tokenize the corpus. The initial 'token' code uses the base tokenizer which breaks on white space.  \ntweet_tokens <- tokens(tweet_corpus)\nprint(tweet_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 957 documents.\ntext1 :\n [1] \"Surging\"      \"populations\"  \"of\"           \"plant-eating\" \"insects\"     \n [6] \"are\"          \"disrupting\"   \"farms\"        \"and\"          \"the\"         \n[11] \"food\"         \"supply\"      \n[ ... and 2 more ]\n\ntext2 :\n [1] \"If\"       \"he\"       \"had\"      \"any\"      \"respect\"  \"for\"     \n [7] \"bereaved\" \"families\" \",\"        \"he\"       \"would\"    \"be\"      \n[ ... and 39 more ]\n\ntext3 :\n [1] \"Covid-19\"     \"Bereaved\"     \"Families\"     \"for\"          \"Justice\"     \n [6] \"says\"         \"Matt\"         \"Hancock\"      \"should\"       \"be\"          \n[11] \"co-operating\" \"with\"        \n[ ... and 40 more ]\n\ntext4 :\n [1] \"Entertainment\" \"show\"          \"with\"          \"so\"           \n [5] \"called\"        \"celebrities\"   \"A\"             \"gov\"          \n [9] \"minister\"      \"whos\"          \"gov\"           \"is\"           \n[ ... and 43 more ]\n\ntext5 :\n [1] \"@Joshua_Griffing\" \"@Bulletsnbrains\"  \"@runninvsthewind\" \"@iluminatibot\"   \n [5] \"Dude\"             \".\"                \".\"                \".\"               \n [9] \"Those\"            \"are\"              \"tweets\"           \"from\"            \n[ ... and 44 more ]\n\ntext6 :\n [1] \"Most\"     \"people\"   \"think\"    \"that\"     \"eating\"   \"insects\" \n [7] \"and\"      \"eating\"   \"crabs\"    \"/\"        \"lobsters\" \"is\"      \n[ ... and 20 more ]\n\n[ reached max_ndoc ... 951 more documents ]\n```\n:::\n\n```{.r .cell-code}\n#This line of code drops punctuation along with the default space breaks.\ntweet_tokens <- tokens(tweet_corpus,\n                        remove_punct = T)\nprint(tweet_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 957 documents.\ntext1 :\n [1] \"Surging\"      \"populations\"  \"of\"           \"plant-eating\" \"insects\"     \n [6] \"are\"          \"disrupting\"   \"farms\"        \"and\"          \"the\"         \n[11] \"food\"         \"supply\"      \n[ ... and 2 more ]\n\ntext2 :\n [1] \"If\"       \"he\"       \"had\"      \"any\"      \"respect\"  \"for\"     \n [7] \"bereaved\" \"families\" \"he\"       \"would\"    \"be\"       \"sharing\" \n[ ... and 33 more ]\n\ntext3 :\n [1] \"Covid-19\"     \"Bereaved\"     \"Families\"     \"for\"          \"Justice\"     \n [6] \"says\"         \"Matt\"         \"Hancock\"      \"should\"       \"be\"          \n[11] \"co-operating\" \"with\"        \n[ ... and 33 more ]\n\ntext4 :\n [1] \"Entertainment\" \"show\"          \"with\"          \"so\"           \n [5] \"called\"        \"celebrities\"   \"A\"             \"gov\"          \n [9] \"minister\"      \"whos\"          \"gov\"           \"is\"           \n[ ... and 40 more ]\n\ntext5 :\n [1] \"@Joshua_Griffing\" \"@Bulletsnbrains\"  \"@runninvsthewind\" \"@iluminatibot\"   \n [5] \"Dude\"             \"Those\"            \"are\"              \"tweets\"          \n [9] \"from\"             \"officials\"        \"there\"            \"There\"           \n[ ... and 35 more ]\n\ntext6 :\n [1] \"Most\"     \"people\"   \"think\"    \"that\"     \"eating\"   \"insects\" \n [7] \"and\"      \"eating\"   \"crabs\"    \"lobsters\" \"is\"       \"a\"       \n[ ... and 16 more ]\n\n[ reached max_ndoc ... 951 more documents ]\n```\n:::\n\n```{.r .cell-code}\n#This last line of code drops numbers within the token corpus as well.\ntweet_tokens <- tokens(tweet_corpus,\n                        remove_punct = T,\n                        remove_numbers = T)\nprint(tweet_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 957 documents.\ntext1 :\n [1] \"Surging\"      \"populations\"  \"of\"           \"plant-eating\" \"insects\"     \n [6] \"are\"          \"disrupting\"   \"farms\"        \"and\"          \"the\"         \n[11] \"food\"         \"supply\"      \n[ ... and 2 more ]\n\ntext2 :\n [1] \"If\"       \"he\"       \"had\"      \"any\"      \"respect\"  \"for\"     \n [7] \"bereaved\" \"families\" \"he\"       \"would\"    \"be\"       \"sharing\" \n[ ... and 31 more ]\n\ntext3 :\n [1] \"Covid-19\"     \"Bereaved\"     \"Families\"     \"for\"          \"Justice\"     \n [6] \"says\"         \"Matt\"         \"Hancock\"      \"should\"       \"be\"          \n[11] \"co-operating\" \"with\"        \n[ ... and 33 more ]\n\ntext4 :\n [1] \"Entertainment\" \"show\"          \"with\"          \"so\"           \n [5] \"called\"        \"celebrities\"   \"A\"             \"gov\"          \n [9] \"minister\"      \"whos\"          \"gov\"           \"is\"           \n[ ... and 40 more ]\n\ntext5 :\n [1] \"@Joshua_Griffing\" \"@Bulletsnbrains\"  \"@runninvsthewind\" \"@iluminatibot\"   \n [5] \"Dude\"             \"Those\"            \"are\"              \"tweets\"          \n [9] \"from\"             \"officials\"        \"there\"            \"There\"           \n[ ... and 35 more ]\n\ntext6 :\n [1] \"Most\"     \"people\"   \"think\"    \"that\"     \"eating\"   \"insects\" \n [7] \"and\"      \"eating\"   \"crabs\"    \"lobsters\" \"is\"       \"a\"       \n[ ... and 14 more ]\n\n[ reached max_ndoc ... 951 more documents ]\n```\n:::\n:::\n\n</details>\n\nAfter creating the `tweet_tokens` object, I searched through the corpus for the keywords using the `kwic` function. The first use of the function in the chunk below searched for the use of the keywords `bug` & `bugs`, while the second analyzes the words `insect` & `insects`. I decided to open up the window to 20 words surrounding the keywords (i.e. patterns). I felt this gave me a sufficient window to, at a glance, gauge sentiment within the use of the words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#These lines of code are dedicated toward the analysis of tokens on a more granular level. This first blurb is analyzing the use of the keywords 'bug' & 'bugs' within the corpus.\nkwic_bugs <- kwic(tweet_tokens,\n                   pattern = c(\"bug\", \"bugs\"),\n                   window = 20)\nview(kwic_bugs)\n\n#This second blurb analyzes the use of the keywords 'insect' & 'insects' instead.\nkwic_insects <- kwic(tweet_tokens, \n                   pattern = c(\"insect\", \"insects\"),\n                   window = 20)\nview(kwic_insects)\n```\n:::\n\n\nI noticed throughout the corpus that the specified keywords were often surrounded by argument and context provided by the individual tweeting them. This is what eventually led me to open the window to 20 words. I also noticed that there were a lot of advertisement campaigns based upon the concept of promoting ones channel/profile by eating bugs. Similarly, there were also some occurrences where the topic of eating bugs was brought up as it relates to nature (for example, bats eating insects and other animal diets) and not according to human dietary habits. I hope that I will be able to further clean up the corpus in future iterations of my project and blog post, consolidating the information further will definitely help me get better results.\n\nWith that said, there were plenty of entries relating to the perspective surrounding humans eating bugs. Many were in relation to political ideologies as well and from a bird's eye view, I noticed a relatively even dispersion of positive and negative opinions surrounding the topic (although, I feel like the average leaned a bit more toward the negative). My goal in the future would be to use dictionary functions to conduct a legitimate sentiment analysis and see how the use of the keywords appear in proportionality of positive vs. negative uses.\n\n## *Word Cloud*\n\nThat last thing I attempted was creating a word cloud using the basis of tokenization that I presented in the previous subsection. Please note, effectively, I could have just used the `tweet_token` object here instead `tweet_corpus`, but I wanted to show the thought process of creating a word cloud and practice the syntax behind the relevant functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Creating a dfm that we'll use to create a wordcloud to try and obtain a birds eye view of word usage throughout our corpus.\ntweet_dfm <- tokens(tweet_corpus, \n                     remove_punct = TRUE,\n                     remove_numbers = TRUE,\n                     remove_symbols = TRUE,\n                     remove_url = TRUE) %>%\n                             tokens_select(pattern = stopwords(\"en\"),\n                                           selection = \"remove\") %>%\n                             dfm()\ntextplot_wordcloud(tweet_dfm)\n```\n\n::: {.cell-output-display}\n![](AlexisGamez_hw2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nUnfortunately, I couldn't take away much from creating the wordcloud. Observably, 'eating', 'insects' and 'bugs' were the most common words, but nothing else in the cloud stood out much or held any real significance. I also noticed that twitter handles and other '@s' still appeared even though I thought I cut them out with the 'remove_symbols' function. A goal in the future will be to cut down further language that might be considered disruptive to the analysis.\n\n\n\n\n",
    "supporting": [
      "AlexisGamez_hw2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}