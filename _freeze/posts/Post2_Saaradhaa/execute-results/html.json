{
  "hash": "c1e0233aa06f2c5ab27189d780296a85",
  "result": {
    "markdown": "---\ntitle: \"Post 2\"\nauthor: \"Saaradhaa M\"\ndescription: \"Working with Dataset\"\ndate: \"09/30/2022\"\neditor: visual\nformat:\n    html:\n        df-print: paged\n        toc: true\n        code-copy: true\n        code-tools: true\n        css: \"styles.css\"\ncategories: \n  - post 2\n  - saaradhaa\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n\n## Refined Research Topic\n\nWhen comparing different data sources for this blog post, I found my original research questions to be too broad. I am refining them to just focus on one community for now.\n\n::: callout-tip\n## New Research Questions\n\n1.  How have the values and concerns of South Asian Americans changed over time?\n2.  Do older South Asian Americans align more with the honour culture prevalent in their home countries? Do younger South Asian Americans align more with the dignity culture prevalent in the USA?\n:::\n\nThis allows me to explore the cultural types described by Leung & Cohen (2011), but at a more manageable level and with greater depth.\n\n## Data Sources\n\nTo understand older South Asian Americans, I will be scraping data from the [South Asian Oral History Project](https://content.lib.washington.edu/saohcweb/index.html) by the University of Washington. This comprises oral histories of South Asians who migrated to the US from the 1950s to the 1980s. Although there are only \\~50 oral histories, each one is very detailed and seems to have at least 10 pages. Thus, a lot of valuable content can be extracted from these texts.\n\nTo understand younger South Asian Americans, I will be scraping posts and comments from the subreddit r/ABCDesis (\"American Born Confused Desis\"). Reddit is very popular in the US, with Americans comprising [half](https://thrivemyway.com/reddit-statistics/#Reddit-Usage-Statistics) of all Reddit Users. Reddit provides an optimal way of understanding communities of users with some common trait/interest - in this case, South Asian Americans. Reddit users also tend to fall in the 18-29 age range, representing South Asian Americans who migrated in the 1990s-2000s (and thus grew up in the Internet age).\n\nFor all code chunks below, I will document my thought process as comments in the code chunks, so that I can keep track of what I'm doing. I'll also be commenting out some of the code so that it doesn't re-run when I render (e.g., I don't want to re-download my files again when I render the document).\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(polite)\nlibrary(quanteda)\nlibrary(tidytext)\nlibrary(quanteda.textplots)\nlibrary(tm)\nlibrary(RedditExtractoR)\n```\n:::\n\n\n## Reading In PDF Data\n\nI'll examine the first 5 URLs to see if there's a pattern I can use to download the PDFs.\n\n-   https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/117/page/0/inline/saohc_117_0\n\n-   https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/14/page/0/inline/saohc_14_0\n\n-   https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/107/page/0/inline/saohc_107_0\n\n-   https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/12/page/0/inline/saohc_12_0\n\n-   https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/95/page/0/inline/saohc_95_0\n\nIt looks like each participant is assigned a unique ID, but it doesn't seem to follow a consistent format (e.g., 1 to 5).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check - is the webpage scrapable?\nbow(\"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/117/page/0/inline/saohc_117_0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<polite session> https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/117/page/0/inline/saohc_117_0\n    User-agent: polite R package\n    robots.txt: 176 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n:::\n\n```{.r .cell-code}\n# yes it is. let's try to download the file.\n# download.file(\"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/117/page/0/inline/saohc_117_0\", \"~/Downloads/test.pdf\")\n```\n:::\n\n\nIt works! I'll make a vector with all the URLs I need.\n\nUnfortunately, I'll have to create the vector manually, due to 2 reasons:\n\n1.  The URLs containing the PDFs cannot be found in the main page with the links to the interviews. I need to click on each interview page, then navigate to the pdf on that page (it's in a different location for each interview page).\n\n2.  The main pages are also not scrapable - only each specific PDF link is scrapable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create vector with transcript PDF URLs. there are 50 URLs - the last 9 are from students, who can be classified as young south asians. these should be tagged separately when i read them into R. \nurls <- c(\"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/117/page/0/inline/saohc_117_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/14/page/0/inline/saohc_14_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/107/page/0/inline/saohc_107_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/12/page/0/inline/saohc_12_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/95/page/0/inline/saohc_95_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/92/page/0/inline/saohc_92_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/110/page/0/inline/saohc_110_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/63/page/0/inline/saohc_63_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/57/page/0/inline/saohc_57_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/66/page/0/inline/saohc_66_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/181/page/0/inline/saohc_181_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/294/page/0/inline/saohc_294_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/23/page/0/inline/saohc_23_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/291/page/0/inline/saohc_291_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/30/page/0/inline/saohc_30_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/103/page/0/inline/saohc_103_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/72/page/0/inline/saohc_72_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/61/page/0/inline/saohc_61_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/10/page/0/inline/saohc_10_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/76/page/0/inline/saohc_76_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/300/page/0/inline/saohc_300_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/297/page/0/inline/saohc_297_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/185/page/0/inline/saohc_185_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/38/page/0/inline/saohc_38_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/175/page/0/inline/saohc_175_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/63/page/0/inline/saohc_63_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/81/page/0/inline/saohc_81_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/42/page/0/inline/saohc_42_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/121/page/0/inline/saohc_121_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/120/page/0/inline/saohc_120_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/179/page/0/inline/saohc_179_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/99/page/0/inline/saohc_99_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/26/page/0/inline/saohc_26_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/33/page/0/inline/saohc_33_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/45/page/0/inline/saohc_45_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/220/page/0/inline/saohc_220_0\",\n          \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/263/page/0/inline/saohc_263_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/277/page/0/inline/saohc_277_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/243/page/0/inline/saohc_243_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/217/page/0/inline/saohc_217_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/236/page/0/inline/saohc_236_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/251/page/0/inline/saohc_251_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/235/page/0/inline/saohc_235_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/259/page/0/inline/saohc_259_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/222/page/0/inline/saohc_222_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/279/page/0/inline/saohc_279_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/281/page/0/inline/saohc_281_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/283/page/0/inline/saohc_283_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/285/page/0/inline/saohc_285_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/239/page/0/inline/saohc_239_0\", \"https://digitalcollections.lib.washington.edu/digital/api/collection/saohc/id/241/page/0/inline/saohc_241_0\")\n\n# remove any duplicates from the above vector, in case i've double-pasted something.\nurls <- urls[!duplicated(urls)]\n\n# let's download the files.\n# download.file(urls, paste0(\"~/Desktop/2022_Fall/GitHub/Text_as_Data_Fall_2022/posts/Transcripts/\", seq(1:50), \".pdf\", sep=\"\"))\n```\n:::\n\n\nI've got all the PDFs, and now I'll try to convert them into a corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# list out all the pdfs - i shifted the folder to my working directory because it was not working otherwise!\npdfs <- list.files(\"~/Desktop/2022_Fall/GitHub/Text_as_Data_Fall_2022/posts/Transcripts\", full.names = TRUE, pattern = \"*.pdf\")\n\n# create a loop to extract texts.\ntext_a <- c()\ntext_s <- c()\nfor (i in 1:41)\n{ print(i)\n  text_a[[pdfs[i]]] <- pdf_text(pdfs[i])}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in normalizePath(path.expand(path), winslash, mustWork): path[1]=NA\n```\n:::\n\n```{.r .cell-code}\nfor (i in 42:50)\n{ print(i)\n  text_s[[pdfs[i]]] <- pdf_text(pdfs[i])}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 42\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in normalizePath(path.expand(path), winslash, mustWork): path[1]=NA\n```\n:::\n\n```{.r .cell-code}\n# convert to characters and save.\ntext_a <- unlist(text_a) %>% paste0(collapse=\" \") %>% stringr::str_squish()\ntext_s <- unlist(text_s) %>% paste0(collapse=\" \") %>% stringr::str_squish()\nsaveRDS(text_a, file=\"text_a.rds\")\nsaveRDS(text_s, file=\"text_s.rds\")\n\n# when reading them into R in the future, i can use this command: readRDS(file=\"text_a.rds\")\n\n# cool. let's convert them to corpora.\ncorpus_a <- corpus(text_a)\ncorpus_s <- corpus(text_s)\n```\n:::\n\n\n## Analyzing PDF Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# basic analysis: let's see how many sentences we have in each corpus.\nsentences_a <- corpus_reshape(corpus_a, to = \"sentences\")\nsentences_s <- corpus_reshape(corpus_s, to=\"sentences\")\n```\n:::\n\n\nThere are \\~43k sentences in the first corpus, and \\~6k sentences in the second one.\n\nLet's make a basic wordcloud using a document feature matrix to see how the transcripts of older vs. younger south asians differ.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create dfms.\ncorpus_a_edit <- corpus_a %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>%\n    dfm_trim(min_termfreq = 10, verbose = FALSE)\ncorpus_s_edit <- corpus_s %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>%\n    dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# create wordclouds.\ntextplot_wordcloud(corpus_a_edit, max_words=50, color=\"purple\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud.dfm(corpus_a_edit, max_words = 50, color = \"purple\"): dfm must have at least one non-zero value\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(corpus_s_edit, max_words=50, color=\"hotpink\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud.dfm(corpus_s_edit, max_words = 50, color = \"hotpink\"): dfm must have at least one non-zero value\n```\n:::\n:::\n\n\nThe purpose of generating the above wordclouds was to get a sense of the kinds of strings I need to clean up in the corpora. Still, it is interesting to see some of the meaningful words that show up: india(n), family, school, music, different.\n\nNow let's do some simple kwic manipulations - what are some differences in the uses of \"culture\" between adults vs. youth?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get tokens.\ntokens_a <- tokens(corpus_a, remove_punct = T, remove_numbers = T)\ntokens_s <- tokens(corpus_s, remove_punct = T, remove_numbers = T)\n\n# look at head.\nhead(kwic(tokens_a, pattern = c(\"culture\"), window=10))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"docname\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"from\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"to\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pre\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"keyword\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"post\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pattern\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nhead(kwic(tokens_s, pattern = c(\"culture\"), window=10))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"docname\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"from\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"to\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pre\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"keyword\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"post\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pattern\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nJust from looking at the head, it looks like adults interviewed were more concerned with how to adjust to the change in culture from South Asia to the US, and how to transmit their culture to the next generation. Meanwhile, the youths talk about things that help them respect and connect with their culture.\n\nMoving forward, I want to (1) add some metadata and (2) remove strings that don't add meaning to analysis (e.g., \"interview\", \"university of washington\").\n\n## Reading In Reddit Data\n\nI will be reading in the Reddit data using RedditExtractoR. It follows the guidelines and rate limits of the Reddit API, so only 1000 posts can be extracted per request.\n\nI ideally want to analyse more than 1000 posts, so I'll look into other methods of extracting Reddit data (potentially the PushShiftR package, or Python wrappers for the Reddit API). Most of the code below is commented out as it doesn't need to be re-run.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract top posts of all time in the subreddit.\n# content <- find_thread_urls(subreddit=\"ABCDesis\",period=\"all\",sort_by=\"top\")\n\n# look at its structure. there are 1000 posts extracted because that's the rate limit of Reddit API.\n# str(content)\n\n# the first row is \"NA\". let's remove that.\n# content <- content[-1,]\n\n# let's save the content object so that we don't have to keep re-running it. in the future, i can just re-load this.\n# saveRDS(content, \"content.rds\")\n\ncontent <- read_rds(\"content.rds\")\n\n# this took me pretty long to read in, then R hung and i lost it! \n# url_content <- get_thread_content(content$url[1:100])\n# url_content2 <- get_thread_content(content$url[101:200])\n# url_content3 <- get_thread_content(content$url[201:300])\n# url_content4 <- get_thread_content(content$url[301:400])\n# url_content5 <- get_thread_content(content$url[401:500])\n# url_content6 <- get_thread_content(content$url[501:600])\n# url_content7 <- get_thread_content(content$url[601:700])\n# url_content8 <- get_thread_content(content$url[701:800])\n# url_content9 <- get_thread_content(content$url[801:900])\n# url_content10 <- get_thread_content(content$url[901:1000])\n\n# i don't know how to make this work. i want to combine all the lists into 1 list --> it's not working the way i want it to!\n# url_contents <- c(url_content, url_content2, url_content3, url_content4, url_content5, url_content6, url_content7, url_content8, url_content9, url_content10)\n```\n:::\n\n\nI'm having some trouble loading the comments (takes \\~1h to load all of them for the 1000 posts, after which R hung). I'm going to do some basic analysis with the posts data first.\n\n## Analysing Reddit Data\n\nLet's first check the structure of our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(content)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1000 obs. of  7 variables:\n $ date_utc : chr  \"2019-03-22\" \"2019-03-09\" \"2019-01-28\" \"2022-05-21\" ...\n $ timestamp: num  1.55e+09 1.55e+09 1.55e+09 1.65e+09 1.61e+09 ...\n $ title    : chr  \"Indian kid won USA talent show, didn't see him here so here is Lydian Nadhaswaram's crowning moment\" \"My mom finally had the conversation with my dad: i want to marry a gora.\" \"Hi it's Jai Wolf - I'll be releasing my debut album \\\"The Cure To Loneliness\\\" + Go on a headline tour this spring\" \"Daytimers (Bhangra Do's) of the 80's-90's UK undergroud rave scene. Would love to hear any stories!\" ...\n $ text     : chr  \"Bit surprised we didn't see a post of him winning here, even more surprisingly it wasn't even mentioned in r/In\"| __truncated__ \"My father immigrated to the United States at 19 years old. He was raised in a small village in Punjab, had a ve\"| __truncated__ \"Hey there fellow desis - you might know me from my song \\\"Indian Summer\\\". \\n\\nI'm releasing my debut album \\\"T\"| __truncated__ \"\" ...\n $ subreddit: chr  \"ABCDesis\" \"ABCDesis\" \"ABCDesis\" \"ABCDesis\" ...\n $ comments : num  28 105 41 16 150 166 31 10 18 136 ...\n $ url      : chr  \"https://www.reddit.com/r/ABCDesis/comments/b43kot/indian_kid_won_usa_talent_show_didnt_see_him_here/\" \"https://www.reddit.com/r/ABCDesis/comments/az5372/my_mom_finally_had_the_conversation_with_my_dad_i/\" \"https://www.reddit.com/r/ABCDesis/comments/aktpvt/hi_its_jai_wolf_ill_be_releasing_my_debut_album/\" \"https://www.reddit.com/r/ABCDesis/comments/uucqhx/daytimers_bhangra_dos_of_the_80s90s_uk_undergroud/\" ...\n```\n:::\n:::\n\n\nIt's a dataframe with 7 columns: date_utc, title, text and comments will be interesting to analyse.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(content$date_utc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2015-03-05\" \"2022-10-27\"\n```\n:::\n:::\n\n\nThe top 1000 posts in the subreddit span 7 years, from 2015 to 20222.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(content$comments)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 87.06\n```\n:::\n:::\n\n\nThat's \\~86 comments per post, which means I was trying to load \\~86,000 comments in total in the previous code chunk - no wonder it took so long. Let's try to convert the title and text columns into a corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# put the 2 columns together and make it easier to read.\ncontent <- unite(content, \"title_text\", c(title, text), sep=\" \")\n\n# create corpus.\nreddit_corpus <- corpus(content$title_text)\nreddit_corpus <- corpus_reshape(reddit_corpus,to=\"sentences\")\n\n# save object.\nsaveRDS(reddit_corpus, \"reddit_corpus.rds\")\n```\n:::\n\n\nLet's make a wordcloud.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus_edit <- reddit_corpus %>% dfm(remove = stopwords('english'), remove_punct = TRUE) %>% dfm_trim(min_termfreq = 10, verbose = FALSE)\ntextplot_wordcloud(reddit_corpus_edit, max_words=50, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](Post2_Saaradhaa_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nFrom the wordcloud, the Reddit data seems much cleaner than the PDF data. India(n) and family comes up as frequently used, similar to the oral histories. There's also frequent usage of the words \"desi\" and \"white\".\n\nLet's also look at the use of the word \"culture\", similar to what I did for the oral histories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(kwic(reddit_corpus, pattern = c(\"culture\"), window=10))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"docname\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"from\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"to\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pre\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"keyword\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"post\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pattern\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"text6.4\",\"2\":\"46\",\"3\":\"46\",\"4\":\"the mother-land . . like they would hate on Indian\",\"5\":\"culture\",\"6\":\", calling Indian music stupid , talking trash about Indian\",\"7\":\"culture\",\"_rn_\":\"1\"},{\"1\":\"text6.4\",\"2\":\"87\",\"3\":\"87\",\"4\":\"country ) . most of them only knew about western\",\"5\":\"culture\",\"6\":\", they said Hindi or Indian language movies suck and\",\"7\":\"culture\",\"_rn_\":\"2\"},{\"1\":\"text6.8\",\"2\":\"17\",\"3\":\"17\",\"4\":\"am actually happy I know a lot more about the\",\"5\":\"culture\",\"6\":\"than a few people who lived in the motherland for\",\"7\":\"culture\",\"_rn_\":\"3\"},{\"1\":\"text25.4\",\"2\":\"8\",\"3\":\"8\",\"4\":\"I am super proud of our colorful\",\"5\":\"culture\",\"6\":\"like Holi , major Puja's and various spiritual aspects of\",\"7\":\"culture\",\"_rn_\":\"4\"},{\"1\":\"text25.4\",\"2\":\"20\",\"3\":\"20\",\"4\":\"Holi , major Puja's and various spiritual aspects of the\",\"5\":\"culture\",\"6\":\", but I do think the accent is funny and\",\"7\":\"culture\",\"_rn_\":\"5\"},{\"1\":\"text55.5\",\"2\":\"22\",\"3\":\"22\",\"4\":\"in my early teens where I was embarrassed of my\",\"5\":\"culture\",\"6\":\"and my parents accents but I genuinely never wanted us\",\"7\":\"culture\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThere seems to be a lot more detail in the Reddit data when looking at the windows around the word \"culture\".\n\n## Moving Forward\n\nAfter finalizing my datasets, I want to do topic modelling and extract personal values from the corpora using the [Personal Values Dictionary](https://osf.io/vt8nf/).\n",
    "supporting": [
      "Post2_Saaradhaa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}