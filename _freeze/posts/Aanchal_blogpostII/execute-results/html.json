{
  "hash": "76cb0b4afcfc875e325e70e81840b9f7",
  "result": {
    "markdown": "---\ntitle: \"Social Media Activists\"\nauthor: \"Aanchal Setia\"\ndesription: \"Initial Data Cleaning\"\ndate: \"10/10/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog post II\n  - Aanchal Setia\n  \n---\n\n\nSummary of My Results:\n\nI have scraped tweets from three different accounts: BLM NYC, BLM LA, and\nBLM Chicago so far. \n\nPre-processing\nI created Wordclouds to understand the words that were redundant. \nI found that there were a lot of mentions of different accounts \nwhich were not useful for my research  question, so I removed them.\nI also removed hashtags, numbers, emojis, punctuation, links, the phrase \"rt\",\nand I changed all letters to lower case.\n\n\nBasic Analysis Plan\nNow that I have formatted tweets from three different chapters, I will run \nsentiment analysis on three different chapters and compare them.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n\n#Calling Necessary Libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:httr':\n\n    content\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\nlibrary(rtweet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n:::\n\n```{.r .cell-code}\nlibrary(twitteR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'twitteR'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rtweet':\n\n    lookup_statuses\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    id, location\n```\n:::\n\n```{.r .cell-code}\nlibrary(purrr)\nlibrary(tidytext)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(scales)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'scales'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(ggplot2)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\n```\n:::\n\n#Getting Twitter Access\n\n::: {.cell}\n\n```{.r .cell-code}\nconsumerkey = \"\"\nconsumersecret = \"\"\naccesstoken = \"\"\naccesssecret = \"\"\n\noptions(httr_oauth_cache = T)\nsetup_twitter_oauth(consumer_key = consumerkey, consumer_secret = consumersecret,\n                    access_token = accesstoken, access_secret = accesssecret)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in check_twitter_oauth(): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n:::\n\n#Set up default authentication for rtweet package\n\n::: {.cell}\n\n```{.r .cell-code}\n#auth_setup_default()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##Creating a function to format tweets\n\nformatting_tweets <- function(tweets)\n{\n  #Removing mentions\n  tweets$full_text <-str_remove_all(string = tweets$full_text, \n                                    pattern = \"[@][\\\\w_-]+\" )\n  #Removing hashtags\n  tweets$full_text <-str_remove_all(string = tweets$full_text, \n                                    pattern = \"[#][\\\\w_-]+\" )\n  #Removing Links\n  tweets$full_text <-str_remove_all(string = tweets$full_text,\n                                    pattern = \"http\\\\S+\\\\s*\" )\n  #Removing Emojis\n  tweets$full_text <- iconv(x = tweets$full_text, from = \"latin1\",\n                            to = \"ASCII\", sub = \"\")\n  #Removing Punctuations\n  tweets$full_text <- str_remove_all(string = tweets$full_text, \n                                     pattern = \"[[:punct:]]\")\n  #Changing Case to Lower Case\n  tweets$full_text <- str_to_lower(string = tweets$full_text)\n  #Removing Numbers\n  tweets$full_text <- str_remove_all(string = tweets$full_text, \n                                     pattern = \"[:digit:]\")\n  #Removing stopwords\n  tweets$full_text <- removeWords(tweets$full_text,\n                                  c(stopwords(\"en\"), \"can\", \"will\"))\n  #Now, I will remove \"rt\" from the text\n  tweets$full_text <- gsub(\"^(rt)\",\"\",tweets$full_text)\n  tweets$full_text  <-  gsub(\"amp\", \"\", tweets$full_text) \n  #Removing Repeated Whitespace\n  tweets$full_text <- str_squish(string = tweets$full_text)\n  #Changing the format of time\n  tweets$created_at <- format(tweets$created_at, format = \"%Y\")\n  \n  return(tweets)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Scraping Tweets from BLack Lives Matter's Three Chapters\n\nBLMchapters <- c(\"BLMNYC\",  \"BLMChi\", \"BLMLA\")\n\n\nfor (i in BLMchapters) {\n  handle <- gsub(\" \", \"\", paste(\"@\", i))\n  result <-  get_timeline(use = handle, n = 1) \n  formatted_result <- formatting_tweets(result)\n  \n  df_name <- i\n  assign(df_name, data.frame(formatted_result))\n}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `default_cached_auth()`:\n! No default authentication found. Pick existing auth with:\nâ€¢ auth_as('create_token')\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}