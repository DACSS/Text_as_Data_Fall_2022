{
  "hash": "e8f9b685996bd651d3442c1459d2074e",
  "result": {
    "markdown": "---\ntitle: \"Blog 5-Twitter Engagement Analysis of MA Gubernatorial Candidates\"\nauthor: \"Rhowena Vespa\"\ndesription: \"Analysis of Replies on Gubernatorial Candidates' Tweets and Influence on Election Results\"\ndate: \"11/06/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Blog 5\n  - Twitter Replies\n  - Twarc2\n  - Rhowena Vespa\n  - Massachusetts Gubernatorial Elections\n  - Healey\n  - Diehl\n  - Supervised Machine Learning\n  - Sentiment Analysis\n  - TF-IDF\n  - Semantic Network\n  - Naive Bayes\n  - Support Vector Machines\n  - Random Forest\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, StringsAsFActors= FALSE)\n```\n:::\n\n\nThis project analyzes Twitter engagement of specific Massachusetts GOvernor Candidates namely\nMaura Healey and Geoff Diehl. \nCORPUS: Extracted twitter replies (Oct 29 to Nov 4) from all of Healey and Diehl's tweets.\nThe replies looks into how these candidates engages other twitter users by generating a response \nto their original tweet or retweet.\nThe replies are then cleaned and pre-processed.\nAnalysis: \n        Initial Data visualization (word cloud)\n        TF-IDF\n        Semantic Network Analysis\n        Sentiment Analysis + Polarity\n        SML -Naive Bayes, SVM, Random Forest\n        \n        \nI wanted to analyze twitter engagement and correlate it with poll results\nWith the election coming up, I would also like to correlate my analysis with actual election results.\n\n\n# LOAD PACKAGES\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta<-\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\nAttaching package: 'tm'\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(plyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n------------------------------------------------------------------------------\n\nAttaching package: 'plyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nThe following object is masked from 'package:purrr':\n\n    compact\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda.textmodels)\nlibrary(devtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: usethis\n```\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(e1071)\nlibrary(quanteda.dictionaries)\n#library(devtools)\n#devtools::install_github(\"kbenoit/quanteda.dictionaries\")\nlibrary(quanteda.dictionaries)\nlibrary(syuzhet) \n#remotes::install_github(\"quanteda/quanteda.sentiment\")\nlibrary(quanteda.sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda.sentiment'\n\nThe following object is masked from 'package:quanteda':\n\n    data_dictionary_LSD2015\n```\n:::\n:::\n\n\n# Healey DATA\n## Load Data\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy <- read_csv(\"Healy.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1900 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (33): edit_history_tweet_ids, text, lang, source, reply_settings, entit...\ndbl  (18): id, conversation_id, referenced_tweets.replied_to.id, referenced_...\nlgl  (24): referenced_tweets.retweeted.id, edit_controls.is_edit_eligible, r...\ndttm  (4): edit_controls.editable_until, created_at, author.created_at, __tw...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nHealy$text <- gsub(\"@[[:alpha:]]*\",\"\", Healy$text) #remove Twitter handles\nHealy$text <- gsub(\"&amp\", \"\", Healy$text)\nHealy$text <- gsub(\"healey\", \"\", Healy$text)\nHealy$text <- gsub(\"_\", \"\", Healy$text)\n```\n:::\n\n## Data Cleaning/ Preprocessing\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_corpus <- Corpus(VectorSource(Healy$text))\nHealy_corpus <- tm_map(Healy_corpus, tolower) #lowercase\nHealy_corpus <- tm_map(Healy_corpus, removeWords, \n                     c(\"s\",\"healey\", \"healy\",\"maura\",\"rt\", \"amp\"))\nHealy_corpus <- tm_map(Healy_corpus, removeWords, \n                     stopwords(\"english\"))\nHealy_corpus <- tm_map(Healy_corpus, removePunctuation)\nHealy_corpus <- tm_map(Healy_corpus, stripWhitespace)\nHealy_corpus <- tm_map(Healy_corpus, removeNumbers)\n```\n:::\n\n## Tokenize and stemming\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_corpus <- corpus(Healy_corpus,text_field = \"text\") \nHealy_text_df <- as.data.frame(Healy_corpus)\nHealy_tokens <- tokens(Healy_corpus)\nHealy_tokens <- tokens_wordstem(Healy_tokens) \nprint(Healy_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1,900 documents.\ntext1 :\n [1] \"four\"       \"day\"        \"nov\"        \"th\"         \"best\"      \n [6] \"pitchnnfor\" \"starter\"    \"abort\"      \"fulli\"      \"protect\"   \n[11] \"ma\"         \"chang\"     \n[ ... and 14 more ]\n\ntext2 :\n [1] \"reproduct\"  \"freedom\"    \"protect\"    \"ma\"         \"sent\"      \n [6] \"back\"       \"state\"      \"…\"          \"belong\"     \"ag\"        \n[11] \"understand\"\n\ntext3 :\n [1] \"serious\"    \"state\"      \"'\"          \"follow\"     \"scienc\"    \n [6] \"'\"          \"w\"          \"experiment\" \"drug\"       \"young\"     \n[11] \"femal\"      \"'\"         \n[ ... and 13 more ]\n\ntext4 :\n[1] \"preserv\"   \"democraci\" \"come\"      \"man\"      \n\ntext5 :\n[1] \"protect\" \"kid\"     \"mean\"    \"vote\"   \n\ntext6 :\n [1] \"like\"            \"serious\"         \"guy\"             \"republican\"     \n [5] \"fix\"             \"anyth\"           \"claim\"           \"abl\"            \n [9] \"just\"            \"gonna\"           \"pull\"            \"sociallyconserv\"\n[ ... and 11 more ]\n\n[ reached max_ndoc ... 1,894 more documents ]\n```\n:::\n\n```{.r .cell-code}\ndfm(Healy_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 1,900 documents, 3,683 features (99.79% sparse) and 0 docvars.\n       features\ndocs    four day nov th best pitchnnfor starter abort fulli protect\n  text1    1   1   1  1    1          1       1     1     1       1\n  text2    0   0   0  0    0          0       0     0     0       1\n  text3    0   0   0  0    0          0       0     0     0       2\n  text4    0   0   0  0    0          0       0     0     0       0\n  text5    0   0   0  0    0          0       0     0     0       1\n  text6    0   0   0  0    0          0       0     0     0       0\n[ reached max_ndoc ... 1,894 more documents, reached max_nfeat ... 3,673 more features ]\n```\n:::\n:::\n\n\n## Create dfm\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a full dfm for comparison---use this to append to polarity\nHealy_Dfm <- tokens(Healy_tokens,\n                  remove_punct = TRUE,\n                  remove_symbols = TRUE,\n                  remove_numbers = TRUE,\n                  remove_url = TRUE,\n                  split_hyphens = FALSE,\n                  split_tags = FALSE,\n                  include_docvars = TRUE) %>%\n  tokens_tolower() %>%\n  dfm(remove = stopwords('english')) %>%\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %>%\n  dfm()\n```\n:::\n\n\n\n## TF-IDF \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(Healy_Dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    vote     will       go    peopl    state     like democrat    right \n     403      124      123      117      114      100       96       93 \n    just    elect \n      89       89 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_tf_dfm <- dfm_tfidf(Healy_Dfm, force = TRUE) #create a new DFM by tf-idf scores\ntopfeatures(Healy_tf_dfm) ## this shows top words by tf-idf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    vote     will       go    state    peopl     like democrat    right \n308.5110 152.9454 151.2329 145.2853 145.2353 132.4511 127.6189 125.0157 \n   elect     just \n121.0111 119.1919 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert corpus to dfm using the dictionary---use to append ???\nHealyDfm_nrc <- tokens(Healy_tokens,\n                     remove_punct = TRUE,\n                     remove_symbols = TRUE,\n                     remove_numbers = TRUE,\n                     remove_url = TRUE,\n                     split_tags = FALSE,\n                     split_hyphens = FALSE,\n                     include_docvars = TRUE) %>%\n  tokens_tolower() %>%\n  dfm(remove = stopwords('english')) %>%\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %>%\n  dfm() %>%\n  dfm_lookup(data_dictionary_NRC)\n```\n:::\n\n\n## Word Cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\ntextplot_wordcloud(Healy_Dfm, scale=c(5,1), max.words=50, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Feature-Occurence Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DFM that contains hashtags \nHealytag_dfm <- dfm_select(Healy_Dfm, pattern = \"#*\")\nHealytoptag <- names(topfeatures(Healy_Dfm, 30)) \nhead(Healytoptag)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vote\"  \"will\"  \"go\"    \"peopl\" \"state\" \"like\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealytag_fcm <- fcm(Healy_Dfm, context = \"document\", tri = FALSE)\nhead(Healytag_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature co-occurrence matrix of: 6 by 334 features.\n         features\nfeatures  day th best abort protect ma chang two just anyth\n  day       3  1    2     1       1  3     1   1    9     1\n  th        1  0    1     2       1  2     1   1    1     1\n  best      2  1    0     1       2  2     1   1    1     1\n  abort     1  2    1     0       1  2     1   1    1     1\n  protect   1  1    2     1       4  3     1   1    2     1\n  ma        3  2    2     2       3  1     2   3    5     2\n[ reached max_nfeat ... 324 more features ]\n```\n:::\n:::\n\n## Semantic Network Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Visualization of semantic network based on hashtag co-occurrence\nHealytopgat_fcm <- fcm_select(Healytag_fcm, pattern = Healytoptag)\ntextplot_network(Healytopgat_fcm, min_freq = 0.8,\n                 omit_isolated = TRUE,\n                 edge_color = \"#1F78B4\",\n                 edge_alpha = 0.5,\n                 edge_size = 2,\n                 vertex_color = \"#4D4D4D\",\n                 vertex_size = 2,\n                 vertex_labelcolor = NULL,\n                 vertex_labelfont = NULL,\n                 vertex_labelsize = 8,\n                 offset = NULL)\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_network(Healytopgat_fcm, vertex_labelsize = 1.5 * rowSums(Healytopgat_fcm)/min(rowSums(Healytopgat_fcm)))\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Sentiment Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#convert cleaned Healy_tokens back tp corpus for sentiment analysis\nHealy_corpus <- corpus(as.character(Healy_tokens))\n```\n:::\n\n### NRC Dictionary\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use liwcalike() to estimate sentiment using NRC dictionary\nHealyTweetSentiment_nrc <- liwcalike(Healy_corpus, data_dictionary_NRC)\nnames(HealyTweetSentiment_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"           \"Sixltr\"      \n [6] \"Dic\"          \"anger\"        \"anticipation\" \"disgust\"      \"fear\"        \n[11] \"joy\"          \"negative\"     \"positive\"     \"sadness\"      \"surprise\"    \n[16] \"trust\"        \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"         \"Quote\"       \n[26] \"Apostro\"      \"Parenth\"      \"OtherP\"      \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealyTweetSentiment_nrc_viz <- HealyTweetSentiment_nrc %>%\n  select(c(\"anger\", \"anticipation\", \"disgust\", \"fear\",\"joy\",\"sadness\", \"surprise\",\"trust\",\"positive\",\"negative\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_tr<-data.frame(t(HealyTweetSentiment_nrc_viz)) #transpose\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_tr_new <- data.frame(rowSums(Healy_tr[2:1900]))\nHealy_tr_mean <- data.frame(rowMeans(Healy_tr[2:1900]))#get mean of sentiment values\nnames(Healy_tr_new)[1] <- \"Count\"\nHealy_tr_new <- cbind(\"sentiment\" = rownames(Healy_tr_new), Healy_tr_new)\nrownames(Healy_tr_new) <- NULL\nHealy_tr_new2<-Healy_tr_new[1:8,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(Healy_tr_new2,\"Healy-Sentiments\")\nwrite_csv(Healy_tr_new,\"Healy-8 Sentiments\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot One - Count of words associated with each sentiment\nquickplot(sentiment, data=Healy_tr_new2, weight=Count, geom=\"bar\", fill=sentiment, ylab=\"count\")+ggtitle(\"Emotions of REPLIES to Maura Healey Tweets\")\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(Healy_tr_mean)[1] <- \"Mean\"\nHealy_tr_mean <- cbind(\"sentiment\" = rownames(Healy_tr_mean), Healy_tr_mean)\nrownames(Healy_tr_mean) <- NULL\nHealy_tr_mean2<-Healy_tr_mean[9:10,]\nwrite_csv(Healy_tr_mean2,\"Healy-Mean Sentiments\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot One - Count of words associated with each sentiment\nquickplot(sentiment, data=Healy_tr_mean2, weight=Mean, geom=\"bar\", fill=sentiment, ylab=\"Mean Sentiment Score\")+ggtitle(\"Mean Sentiment Scores to Maura Healey Tweets\")\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n## Polarity scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# POLARITY\n\nHealydf_nrc <- convert(HealyDfm_nrc, to = \"data.frame\")\nnames(Healydf_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"doc_id\"       \"anger\"        \"anticipation\" \"disgust\"      \"fear\"        \n [6] \"joy\"          \"negative\"     \"positive\"     \"sadness\"      \"surprise\"    \n[11] \"trust\"       \n```\n:::\n\n```{.r .cell-code}\nwrite_csv(Healydf_nrc,\"Healy-Polarity Scores\")\n\nHealydf_nrc$polarity <- (Healydf_nrc$positive - Healydf_nrc$negative)/(Healydf_nrc$positive + Healydf_nrc$negative)\n\nHealydf_nrc$polarity[(Healydf_nrc$positive + Healydf_nrc$negative) == 0] <- 0\n\nggplot(Healydf_nrc) +\n  geom_histogram(aes(x=polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n## New DF with Polarity scores\n\n::: {.cell}\n\n```{.r .cell-code}\nHealy_text_df <-as.data.frame(Healy_text_df)\nHealyCorpus_Polarity <-as.data.frame((cbind(Healydf_nrc,Healy_text_df)))\n```\n:::\n\n\n## New CORPUS (Polarity+text)\n\n::: {.cell}\n\n```{.r .cell-code}\nHealyCorpus_Polarity <- HealyCorpus_Polarity %>%\n    select(c(\"polarity\",\"Healy_corpus\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHealyCorpus_Polarity$polarity <- recode(HealyCorpus_Polarity$polarity,\n                                      \"1\" = \"positive\",\n                                      \"-1\" = \"negative\",\n                                      \"0\" = \"neutral\",)\n```\n:::\n\n### Omit na\n\n::: {.cell}\n\n```{.r .cell-code}\nHealyCorpus_Polarity <- na.omit(HealyCorpus_Polarity)\nhead(HealyCorpus_Polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      polarity\ntext1  neutral\ntext2 positive\ntext4  neutral\ntext6  neutral\ntext7 negative\ntext8  neutral\n                                                                                                                                                                    Healy_corpus\ntext1        four days nov th best pitchnnfor starters abortion fully protected ma change two items list just hyperbole anything drive business ma cost energy even higher crazy\ntext2                                                                                                 reproductive freedom protected ma sent back states… belongs ag understand \ntext4                                                                                                                                              preserving democracy come man\ntext6  like seriously guys republicans fix anything claim able just gonna pull sociallyconservative bs take away right vote alongside social security medicare telegraphing hard\ntext7                                                                                                                  willing trade democracy lower gas prices will get neither\ntext8                                                                                                                                                          httpstcopqhknanuo\n```\n:::\n:::\n\n### Convert cleaned DF to corpus\n\n::: {.cell}\n\n```{.r .cell-code}\nHealyCorpus_P<- corpus(HealyCorpus_Polarity,text_field = \"Healy_corpus\")   \n```\n:::\n\n\n# HEALEY- MACHINE LEARNING METHODS\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(123)\n\n# create id variable in corpus metadata\ndocvars(HealyCorpus_P, \"id\") <- 1:ndoc(HealyCorpus_P)\n\n\n# create training set (60% of data) and initial test set\nN <- ndoc(HealyCorpus_P)\ntrainIndex <- sample(1:N,.6 * N) \ntestIndex <- c(1:N)[-trainIndex]\n\n\n# split test set in half (so 20% of data are test, 20% of data are held-out)\nN <- length(testIndex)\nheldOutIndex <- sample(1:N, .5 * N)\ntestIndex <- testIndex[-heldOutIndex]\n\n\n\n# now apply indices to create subsets and dfms\ndfmTrain <- corpus_subset(HealyCorpus_P, id %in% trainIndex) %>% tokens() %>% dfm()\ndfmTest <- corpus_subset(HealyCorpus_P, id %in% testIndex) %>% tokens() %>% dfm()\ndfmHeldOut <- corpus_subset(HealyCorpus_P, id %in% heldOutIndex) %>% tokens() %>% dfm()\n\n\nhead(trainIndex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 415 463 179 526 195 938\n```\n:::\n\n```{.r .cell-code}\nhead(testIndex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  3  4 12 14 15 20\n```\n:::\n:::\n\n\n## NB model\n\n::: {.cell}\n\n```{.r .cell-code}\npolarity_NaiveBayes <- textmodel_nb(dfmTrain, docvars(dfmTrain, \"polarity\"), distribution = \"Bernoulli\") \nsummary(polarity_NaiveBayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\ntextmodel_nb.dfm(x = dfmTrain, y = docvars(dfmTrain, \"polarity\"), \n    distribution = \"Bernoulli\")\n\nClass Priors:\n(showing first 3 elements)\nnegative  neutral positive \n  0.3333   0.3333   0.3333 \n\nEstimated Feature Scores:\n         reproductive  freedom protected      ma     sent     back   states\nnegative     0.011494 0.011494  0.011494 0.04598 0.011494 0.022989 0.011494\nneutral      0.001309 0.003927  0.001309 0.02618 0.002618 0.006545 0.003927\npositive     0.010582 0.015873  0.015873 0.04233 0.015873 0.037037 0.015873\n               …  belongs       ag understand  willing    trade democracy\nnegative 0.02299 0.011494 0.011494   0.011494 0.022989 0.022989  0.022989\nneutral  0.02225 0.001309 0.006545   0.001309 0.002618 0.001309  0.002618\npositive 0.01587 0.010582 0.031746   0.010582 0.005291 0.005291  0.015873\n            lower     gas   prices    will     get  neither httpstcopqhknanuo\nnegative 0.034483 0.02299 0.057471 0.09195 0.04598 0.022989          0.011494\nneutral  0.002618 0.01702 0.003927 0.04188 0.03272 0.001309          0.002618\npositive 0.005291 0.06878 0.010582 0.06878 0.06349 0.010582          0.005291\n            build  economy    works everyone  raising   living     cost\nnegative 0.022989 0.022989 0.022989 0.022989 0.022989 0.022989 0.022989\nneutral  0.001309 0.002618 0.001309 0.006545 0.001309 0.002618 0.003927\npositive 0.010582 0.015873 0.005291 0.005291 0.015873 0.010582 0.015873\n            taxes  chasing\nnegative 0.045977 0.022989\nneutral  0.006545 0.001309\npositive 0.005291 0.005291\n```\n:::\n\n```{.r .cell-code}\ndfmTestMatched <- dfm_match(dfmTest, features = featnames(dfmTrain))\n```\n:::\n\n\n### CONFUSION MATRIX\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a confusion matrix \nactual <- docvars(dfmTestMatched, \"polarity\")\npredicted <- predict(polarity_NaiveBayes, newdata = dfmTestMatched)\nconfusion <- table(actual, predicted)\n\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(confusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          predicted\nactual     negative neutral positive\n  negative        2      33        1\n  neutral         0     240        3\n  positive        0      58        8\n\nOverall Statistics\n                                          \n               Accuracy : 0.7246          \n                 95% CI : (0.6743, 0.7711)\n    No Information Rate : 0.9594          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1313          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                 1.000000         0.7251         0.66667\nSpecificity                 0.900875         0.7857         0.82583\nPos Pred Value              0.055556         0.9877         0.12121\nNeg Pred Value              1.000000         0.1078         0.98566\nPrecision                   0.055556         0.9877         0.12121\nRecall                      1.000000         0.7251         0.66667\nF1                          0.105263         0.8362         0.20513\nPrevalence                  0.005797         0.9594         0.03478\nDetection Rate              0.005797         0.6957         0.02319\nDetection Prevalence        0.104348         0.7043         0.19130\nBalanced Accuracy           0.950437         0.7554         0.74625\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_prob <- predict(polarity_NaiveBayes, newdata = dfmTestMatched, type = \"probability\")\nhead(predicted_prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           negative   neutral     positive\ntext4  8.369084e-14 1.0000000 4.927139e-08\ntext6  1.084335e-08 0.9999950 4.980505e-06\ntext14 1.244174e-11 0.9999826 1.737715e-05\ntext17 6.671955e-13 0.9999999 9.920512e-08\ntext18 9.471524e-15 1.0000000 3.598392e-08\ntext24 2.010070e-12 1.0000000 3.591003e-08\n```\n:::\n\n```{.r .cell-code}\nsummary(predicted_prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    negative           neutral          positive       \n Min.   :0.000000   Min.   :0.0000   Min.   :0.00e+00  \n 1st Qu.:0.000000   1st Qu.:1.0000   1st Qu.:0.00e+00  \n Median :0.000000   Median :1.0000   Median :3.00e-07  \n Mean   :0.005803   Mean   :0.9541   Mean   :4.01e-02  \n 3rd Qu.:0.000000   3rd Qu.:1.0000   3rd Qu.:1.33e-05  \n Max.   :1.000000   Max.   :1.0000   Max.   :1.00e+00  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# The most positive review\nmostPos <- sort.list(predicted_prob[,1], dec=F)[1]\nas.character(corpus_subset(HealyCorpus_P, id %in% testIndex))[mostPos]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              text1320 \n\" casting vote modern\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmostNeg <- sort.list(predicted_prob[,1], dec=T)[1]\nas.character(corpus_subset(HealyCorpus_P, id %in% testIndex))[mostNeg]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                                                                                        text903 \n\" black poverty charts dem areann  africanamerican households boston live poverty line statistic conceals huge inequities across neighborhoods ranging  hyde park  charlestown\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# The most positive review\nmixed <- sort.list(abs(predicted_prob[,1] - .5), dec=F)[1]\npredicted_prob[mixed,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  negative    neutral   positive \n0.00135779 0.29476010 0.70388211 \n```\n:::\n\n```{.r .cell-code}\nas.character(corpus_subset(HealyCorpus_P, id %in% testIndex))[mixed]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                                                                                                     text1194 \n\" gemini  omg almost east coast california speak especially witch becomes gov thing holding together balance r gov entire legislature d unliked many reluctance ban covid 🗥 kids pissed moms\" \n```\n:::\n:::\n\n\n\n# Naive Bayes -held out\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactual <- docvars(dfmHeldOut)$polarity\ncount(actual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x freq\n1 negative   23\n2  neutral  256\n3 positive   66\n```\n:::\n\n```{.r .cell-code}\ndfmHeldOutMatched <- dfm_match(dfmHeldOut, features = featnames(dfmTrain))\npredicted.nb <- predict(polarity_NaiveBayes, dfmHeldOutMatched)\ncount(predicted.nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x freq\n1 negative    1\n2  neutral  320\n3 positive   24\n```\n:::\n\n```{.r .cell-code}\nconfusion <- table(actual, predicted.nb)\nconfusionMatrix(confusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          predicted.nb\nactual     negative neutral positive\n  negative        1      22        0\n  neutral         0     254        2\n  positive        0      44       22\n\nOverall Statistics\n                                          \n               Accuracy : 0.8029          \n                 95% CI : (0.7569, 0.8436)\n    No Information Rate : 0.9275          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.3391          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                 1.000000         0.7937         0.91667\nSpecificity                 0.936047         0.9200         0.86293\nPos Pred Value              0.043478         0.9922         0.33333\nNeg Pred Value              1.000000         0.2584         0.99283\nPrecision                   0.043478         0.9922         0.33333\nRecall                      1.000000         0.7937         0.91667\nF1                          0.083333         0.8819         0.48889\nPrevalence                  0.002899         0.9275         0.06957\nDetection Rate              0.002899         0.7362         0.06377\nDetection Prevalence        0.066667         0.7420         0.19130\nBalanced Accuracy           0.968023         0.8569         0.88980\n```\n:::\n:::\n\n\n## SVM\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(123)\n\n# set of training data\nnewTrainIndex <- trainIndex[sample(1:length(trainIndex))]\n\n# create small DFM\ndfmTrainSmall <- corpus_subset(HealyCorpus_P, id %in% newTrainIndex) %>% dfm(remove = stopwords(\"English\"), remove_punct=T)\n\n# trim the DFM down to frequent terms\ndfmTrainSmall <- dfm_trim(dfmTrainSmall, min_docfreq = 20, min_termfreq = 20)\n\ndim(dfmTrainSmall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1034   34\n```\n:::\n\n```{.r .cell-code}\n# run model\npolarity_SVM <- textmodel_svm(dfmTrainSmall, docvars(dfmTrainSmall, \"polarity\")) \n\n# update test set\ndfmTestMatchedSmall <- dfm_match(dfmTest, features = featnames(dfmTrainSmall))\n\n# create a confusion matrix \nactual <- docvars(dfmTestMatchedSmall, \"polarity\")\npredicted <- predict(polarity_SVM, newdata = dfmTestMatchedSmall)\nconfusion <- table(actual, predicted)\n```\n:::\n\n\n### Confusion Matrix\n\n::: {.cell}\n\n```{.r .cell-code}\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(confusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          predicted\nactual     negative neutral positive\n  negative        2      32        2\n  neutral         0     240        3\n  positive        0      53       13\n\nOverall Statistics\n                                          \n               Accuracy : 0.7391          \n                 95% CI : (0.6894, 0.7847)\n    No Information Rate : 0.942           \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1995          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                 1.000000         0.7385         0.72222\nSpecificity                 0.900875         0.8500         0.83792\nPos Pred Value              0.055556         0.9877         0.19697\nNeg Pred Value              1.000000         0.1667         0.98208\nPrecision                   0.055556         0.9877         0.19697\nRecall                      1.000000         0.7385         0.72222\nF1                          0.105263         0.8451         0.30952\nPrevalence                  0.005797         0.9420         0.05217\nDetection Rate              0.005797         0.6957         0.03768\nDetection Prevalence        0.104348         0.7043         0.19130\nBalanced Accuracy           0.950437         0.7942         0.78007\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# check code---Error in order(V1) : object 'V1' not found\nsvmCoefs <- as.data.frame(t(coefficients(polarity_SVM)))\nhead(svmCoefs,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            positive    negative     neutral\nma        0.10642383  0.04095159 -0.15027369\ngas       0.63259122 -0.23232753 -0.45305789\nwill     -0.04905536  0.17855470 -0.11240942\nget       0.13327219 -0.08977719 -0.04335252\nstate     0.32094424 -0.03608965 -0.28125136\ngovernor -0.23191741  0.03187021  0.19601274\nre        0.19742408  0.07659889 -0.25219334\njust     -0.11477470  0.03795018  0.06346217\none       0.36764128 -0.05616672 -0.31978665\nbiden     0.20245429 -0.46449151  0.02273121\n```\n:::\n\n```{.r .cell-code}\ntail(svmCoefs,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            positive    negative     neutral\ngo       -0.30889746 -0.06202332  0.34283740\nneed      0.08641445 -0.01586162 -0.07350022\nnow       0.12340404  0.19171685 -0.26537615\nwant     -0.08106290  0.21969589 -0.12917587\nknow      0.44976743  0.01858271 -0.47835868\nus        0.06998385 -0.06974139 -0.01704071\nelection  1.45926123 -0.33623228 -1.30718337\nalready  -0.19044299 -0.02657631  0.21397132\nborder    0.30428331 -0.08398931 -0.24725324\nBias     -0.73363968 -0.84683933  0.58081363\n```\n:::\n:::\n\n\n## RANDOM FOREST\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(randomForest): there is no package called 'randomForest'\n```\n:::\n\n```{.r .cell-code}\ndfmTrainSmallRf <- convert(dfmTrainSmall, to = \"matrix\")\ndfmTestMatchedSmallRf <- convert(dfmTestMatchedSmall, to = \"matrix\")\n\nset.seed(123)\nHealey_polarity_RF <- randomForest(dfmTrainSmallRf, \n                            y = as.factor(docvars(dfmTrainSmall)$polarity),\n                            xtest = dfmTestMatchedSmallRf, \n                            ytest = as.factor(docvars(dfmTestMatchedSmall)$polarity),\n                            importance = TRUE,\n                            mtry = 20,\n                            ntree = 100\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in randomForest(dfmTrainSmallRf, y = as.factor(docvars(dfmTrainSmall)$polarity), : could not find function \"randomForest\"\n```\n:::\n:::\n\n### Confusion Matrix\n\n::: {.cell}\n\n```{.r .cell-code}\nactual <- as.factor(docvars(dfmTestMatchedSmall)$polarity)\npredicted <- Healey_polarity_RF$test[['predicted']]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Healey_polarity_RF' not found\n```\n:::\n\n```{.r .cell-code}\nconfusion <- table(actual,predicted)\nconfusionMatrix(confusion, mode=\"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          predicted\nactual     negative neutral positive\n  negative        2      32        2\n  neutral         0     240        3\n  positive        0      53       13\n\nOverall Statistics\n                                          \n               Accuracy : 0.7391          \n                 95% CI : (0.6894, 0.7847)\n    No Information Rate : 0.942           \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1995          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                 1.000000         0.7385         0.72222\nSpecificity                 0.900875         0.8500         0.83792\nPos Pred Value              0.055556         0.9877         0.19697\nNeg Pred Value              1.000000         0.1667         0.98208\nPrecision                   0.055556         0.9877         0.19697\nRecall                      1.000000         0.7385         0.72222\nF1                          0.105263         0.8451         0.30952\nPrevalence                  0.005797         0.9420         0.05217\nDetection Rate              0.005797         0.6957         0.03768\nDetection Prevalence        0.104348         0.7043         0.19130\nBalanced Accuracy           0.950437         0.7942         0.78007\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(Healey_polarity_RF)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in varImpPlot(Healey_polarity_RF): could not find function \"varImpPlot\"\n```\n:::\n:::\n\n\n\n--------------------------------------------------\n\n# DIEHL DATA\n\n## Load Data\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehl <- read_csv(\"Diehl.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 497 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (34): edit_history_tweet_ids, text, lang, source, reply_settings, entit...\ndbl  (18): id, conversation_id, referenced_tweets.replied_to.id, referenced_...\nlgl  (23): referenced_tweets.retweeted.id, edit_controls.is_edit_eligible, r...\ndttm  (4): edit_controls.editable_until, created_at, author.created_at, __tw...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nDiehl$text <- gsub(\"@[[:alpha:]]*\",\"\", Diehl$text) #remove Twitter handles\nDiehl$text <- gsub(\"&amp\", \"\", Diehl$text)\nDiehl$text <- gsub(\"_\", \"\", Diehl$text)\n```\n:::\n\n## Data Cleaning/ Preprocessing\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehl_corpus <- Corpus(VectorSource(Diehl$text))\nDiehl_corpus <- tm_map(Diehl_corpus, tolower) #lowercase\nDiehl_corpus <- tm_map(Diehl_corpus, removeWords, \n                       c(\"s\",\"geoff\", \"diehl\",\"rt\", \"amp\"))\nDiehl_corpus <- tm_map(Diehl_corpus, removeWords, \n                       stopwords(\"english\"))\nDiehl_corpus <- tm_map(Diehl_corpus, removePunctuation)\nDiehl_corpus <- tm_map(Diehl_corpus, stripWhitespace)\nDiehl_corpus <- tm_map(Diehl_corpus, removeNumbers)\n\nDiehl_corpus <- corpus(Diehl_corpus,text_field = \"text\") \n\nDiehl_text_df <- as.data.frame(Diehl_corpus)\n```\n:::\n\n\n## Tokenize and stemming\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehl_tokens <- tokens(Diehl_corpus)\nDiehl_tokens <- tokens_wordstem(Diehl_tokens)\nprint(Diehl_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 497 documents.\ntext1 :\n[1] \"still\"   \"beat\"    \"fascism\" \"day\"     \"week\"    \"📴\"     \n\ntext2 :\n[1] \"wear\" \"mask\" \"'\"    \"re\"   \"dumb\"\n\ntext3 :\n [1] \"'\"       \"mention\" \"mask\"    \"pay\"     \"compani\" \"can\"     \"charg\"  \n [8] \"whatev\"  \"want\"    \"follow\"  \"'\"       \"ll\"     \n[ ... and 10 more ]\n\ntext4 :\n [1] \"argument\" \"gas\"      \"pipelin\"  \"energi\"   \"independ\" \"right\"   \n [7] \"now\"      \"new\"      \"england\"  \"get\"      \"lng\"      \"deliveri\"\n[ ... and 18 more ]\n\ntext5 :\n [1] \"shit\"     \"u\"        \"dumb\"     \"masker\"   \"democrat\" \"still\"   \n [7] \"fuck\"     \"everyon\"  \"caus\"     \"price\"    \"hike\"     \"higher\"  \n[ ... and 7 more ]\n\ntext6 :\n [1] \"compani\"  \"take\"     \"profit\"   \"loss\"     \"kinder\"   \"morgan\"  \n [7] \"elect\"    \"need\"     \"proof\"    \"energi\"   \"independ\" \"way\"     \n[ ... and 7 more ]\n\n[ reached max_ndoc ... 491 more documents ]\n```\n:::\n\n```{.r .cell-code}\ndfm(Diehl_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 497 documents, 1,921 features (99.49% sparse) and 0 docvars.\n       features\ndocs    still beat fascism day week📴 wear mask ' re\n  text1     1    1       1   1    1 1    0    0 0  0\n  text2     0    0       0   0    0 0    1    1 1  1\n  text3     0    0       0   0    0 0    0    1 2  0\n  text4     0    0       0   0    0 0    0    0 0  0\n  text5     1    0       0   0    0 0    0    0 0  0\n  text6     0    0       0   0    0 0    0    0 0  0\n[ reached max_ndoc ... 491 more documents, reached max_nfeat ... 1,911 more features ]\n```\n:::\n:::\n\n## Create dfm\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a full dfm for comparison---use this to append to polarity\nDiehl_Dfm <- tokens(Diehl_tokens,\n                    remove_punct = TRUE,\n                    remove_symbols = TRUE,\n                    remove_numbers = TRUE,\n                    remove_url = TRUE,\n                    split_hyphens = FALSE,\n                    split_tags = FALSE,\n                    include_docvars = TRUE) %>%\n  tokens_tolower() %>%\n  dfm(remove = stopwords('english')) %>%\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %>%\n  dfm()\n```\n:::\n\n\n\n\n## TF-IDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(Diehl_Dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  vote school     go   like   will    get healey public   know   just \n    57     54     47     43     43     39     39     35     32     32 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehl_tf_dfm <- dfm_tfidf(Diehl_Dfm, force = TRUE) #create a new DFM by tf-idf scores\ntopfeatures(Diehl_tf_dfm) ## this shows top words by tf-idf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  school     vote       go     like     will      get   healey   public \n61.56291 55.88012 50.43603 46.14361 46.14361 44.46210 44.46210 43.18854 \n    know     just \n39.97435 39.48667 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert corpus to dfm using the dictionary---use to append \nDiehlDfm_nrc <- tokens(Diehl_tokens,\n                       remove_punct = TRUE,\n                       remove_symbols = TRUE,\n                       remove_numbers = TRUE,\n                       remove_url = TRUE,\n                       split_tags = FALSE,\n                       split_hyphens = FALSE,\n                       include_docvars = TRUE) %>%\n  tokens_tolower() %>%\n  dfm(remove = stopwords('english')) %>%\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %>%\n  dfm() %>%\n  dfm_lookup(data_dictionary_NRC)\n\n\ndim(DiehlDfm_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 497  10\n```\n:::\n\n```{.r .cell-code}\nhead(DiehlDfm_nrc, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 10 documents, 10 features (70.00% sparse) and 0 docvars.\n       features\ndocs    anger anticipation disgust fear joy negative positive sadness surprise\n  text1     0            0       0    0   0        0        0       0        0\n  text2     0            0       0    0   0        0        0       0        0\n  text3     1            1       0    0   1        1        1       1        1\n  text4     1            3       0    0   1        0        3       0        1\n  text5     0            0       0    0   0        0        0       0        0\n  text6     0            0       0    0   0        0        1       0        0\n       features\ndocs    trust\n  text1     0\n  text2     0\n  text3     1\n  text4     1\n  text5     0\n  text6     1\n[ reached max_ndoc ... 4 more documents ]\n```\n:::\n:::\n\n## Word CLoud\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\ntextplot_wordcloud(Diehl_Dfm, scale=c(5,1), max.words=50, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n## Feature-occurrence matrix \n\n::: {.cell}\n\n```{.r .cell-code}\nDiehltag_dfm <- dfm_select(Diehl_Dfm, pattern = \"#*\")\nDiehltoptag <- names(topfeatures(Diehl_Dfm, 30)) \nhead(Diehltoptag)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vote\"   \"school\" \"go\"     \"like\"   \"will\"   \"get\"   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehltag_fcm <- fcm(Diehl_Dfm)\nhead(Diehltag_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature co-occurrence matrix of: 6 by 74 features.\n        features\nfeatures re can want see vote like gas energi right now\n    re    1   0    2   0    0    1   0      1     4   0\n    can   0   1    3   1    5    3   2      2     2   2\n    want  0   0    4   1    2    3   0      0     4   3\n    see   0   0    0   1    5    3   0      0     0   0\n    vote  0   0    0   0    5    4   1      1     1   2\n    like  0   0    0   0    0    1   1      1     2   3\n[ reached max_nfeat ... 64 more features ]\n```\n:::\n:::\n\n\n## Semantic Network Analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n#Visualization of semantic network based on hashtag co-occurrence\n\nDiehltopgat_fcm <- fcm_select(Diehltag_fcm, pattern = Diehltoptag)\ntextplot_network(Diehltopgat_fcm, min_freq = 0.8,\n                 omit_isolated = TRUE,\n                 edge_color = \"#1F78B4\",\n                 edge_alpha = 0.5,\n                 edge_size = 2,\n                 vertex_color = \"#4D4D4D\",\n                 vertex_size = 2,\n                 vertex_labelcolor = NULL,\n                 vertex_labelfont = NULL,\n                 vertex_labelsize = 8,\n                 offset = NULL)\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n\n## Sentiment Analysis\n\n### NRC Dictionary\n\n::: {.cell}\n\n```{.r .cell-code}\n#convert cleaned Diehl_tokens back tp corpus for sentiment analysis\nDiehl_corpus <- corpus(as.character(Diehl_tokens))\n\n# use liwcalike() to estimate sentiment using NRC dictionary\nDiehlTweetSentiment_nrc <- liwcalike(Diehl_corpus, data_dictionary_NRC)\n\nnames(DiehlTweetSentiment_nrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"           \"Sixltr\"      \n [6] \"Dic\"          \"anger\"        \"anticipation\" \"disgust\"      \"fear\"        \n[11] \"joy\"          \"negative\"     \"positive\"     \"sadness\"      \"surprise\"    \n[16] \"trust\"        \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"         \"Quote\"       \n[26] \"Apostro\"      \"Parenth\"      \"OtherP\"      \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehlTweetSentiment_nrc_viz <- DiehlTweetSentiment_nrc %>%\n  select(c(\"anger\", \"anticipation\", \"disgust\", \"fear\",\"joy\",\"sadness\", \"surprise\",\"trust\",\"positive\",\"negative\"))\n\n\nDiehl_tr<-data.frame(t(DiehlTweetSentiment_nrc_viz)) #transpose\n\nDiehl_tr_new <- data.frame(rowSums(Diehl_tr[2:497]))\nDiehl_tr_mean <- data.frame(rowMeans(Diehl_tr[2:497]))#get mean of sentiment values\nnames(Diehl_tr_new)[1] <- \"Count\"\nDiehl_tr_new <- cbind(\"sentiment\" = rownames(Diehl_tr_new), Diehl_tr_new)\nrownames(Diehl_tr_new) <- NULL\nDiehl_tr_new2<-Diehl_tr_new[1:8,]\nwrite_csv(Diehl_tr_new2,\"Diehl- 8 Sentiments\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot One - Count of words associated with each sentiment\nquickplot(sentiment, data=Diehl_tr_new2, weight=Count, geom=\"bar\", fill=sentiment, ylab=\"count\")+ggtitle(\"Emotions to REPLIES Geoff Diehl Tweets\")\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(Diehl_tr_mean)[1] <- \"Mean\"\nDiehl_tr_mean <- cbind(\"sentiment\" = rownames(Diehl_tr_mean), Diehl_tr_mean)\nrownames(Diehl_tr_mean) <- NULL\nDiehl_tr_mean2<-Diehl_tr_mean[9:10,]\nwrite_csv(Diehl_tr_mean2,\"Diehl -Mean Sentiments\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot One - Count of words associated with each sentiment\nquickplot(sentiment, data=Diehl_tr_mean2, weight=Mean, geom=\"bar\", fill=sentiment, ylab=\"Mean Sentiment Score\")+ggtitle(\"Mean Sentiment Scores to Geoff Diehl Tweets\")\n```\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n\n\n## Polarity Scores\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehldf_nrc <- convert(DiehlDfm_nrc, to = \"data.frame\")\nwrite_csv(Diehldf_nrc, \"Diehl- Polarity Scores\")\n\nDiehldf_nrc$polarity <- (Diehldf_nrc$positive - Diehldf_nrc$negative)/(Diehldf_nrc$positive + Diehldf_nrc$negative)\n\nDiehldf_nrc$polarity[(Diehldf_nrc$positive + Diehldf_nrc$negative) == 0] <- 0\n\nggplot(Diehldf_nrc) +\n  geom_histogram(aes(x=polarity)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](MAGov_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\n## Bind to new DF\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehl_text_df <-as.data.frame(Diehl_text_df)\nDiehlCorpus_Polarity <-as.data.frame((cbind(Diehldf_nrc,Diehl_text_df)))\n```\n:::\n\n\n## subset to polarity and text\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehlCorpus_Polarity <- DiehlCorpus_Polarity %>%\n  select(c(\"polarity\",\"Diehl_corpus\"))\n\n\nDiehlCorpus_Polarity$polarity <- recode(DiehlCorpus_Polarity$polarity,\n                                        \"1\" = \"positive\",\n                                        \"-1\" = \"negative\",\n                                        \"0\" = \"neutral\",)\n```\n:::\n\n## Cleaned DF\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehlCorpus_Polarity <- na.omit(DiehlCorpus_Polarity)\nhead(DiehlCorpus_Polarity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      polarity\ntext1  neutral\ntext2  neutral\ntext3  neutral\ntext4 positive\ntext5  neutral\ntext6 positive\n                                                                                                                                                                                                                  Diehl_corpus\ntext1                                                                                                                                                                                           still beats fascism day week 📴\ntext2                                                                                                                                                                                                       wear mask ’re dumb\ntext3                                                                                     ’ mentioning masks paying companies can charge whatever want follow ’ll see shareholders companies keep voting party like dumb poor…\ntext4  argument gas pipelines energy independence right now new england gets lng deliveries international shipments nthese corporations making profit another story vg blckrck use public funds turn profit pull public money \ntext5                                                                                                         shit u dumb masker democrats still fucked everyone caused price hikes higher taxes fn maskers u don’t know squat\ntext6                                                                     companies take profit loss kinder morgan elected need proof energy independence way actually take control prices give billionaires httpstcodairdldph\n```\n:::\n:::\n\n## New Corpus with polarity scores\n\n::: {.cell}\n\n```{.r .cell-code}\nDiehlCorpus_P<- corpus(DiehlCorpus_Polarity,text_field = \"Diehl_corpus\")   \n```\n:::\n\n\n\n# DIEHL-MACHINE LEARNING\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(123)\n\n# create id variable in corpus metadata\ndocvars(DiehlCorpus_P, \"id\") <- 1:ndoc(DiehlCorpus_P)\n\n\n# create training set (60% of data) and initial test set\nDN <- ndoc(DiehlCorpus_P)\nDtrainIndex <- sample(1:DN,.6 * DN) \nDtestIndex <- c(1:N)[-DtrainIndex]\n\n\n# split test set in half (so 20% of data are test, 20% of data are held-out)\nDN <- length(DtestIndex)\nDheldOutIndex <- sample(1:DN, .5 * DN)\nDtestIndex <- DtestIndex[-DheldOutIndex]\n\n\n\n# now apply indices to create subsets and dfms\nDdfmTrain <- corpus_subset(DiehlCorpus_P, id %in% DtrainIndex) %>% tokens() %>% dfm()\nDdfmTest <- corpus_subset(DiehlCorpus_P, id %in% DtestIndex) %>% tokens() %>% dfm()\nDdfmHeldOut <- corpus_subset(DiehlCorpus_P, id %in% DheldOutIndex) %>% tokens() %>% dfm()\n\n\nhead(DtrainIndex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 415 463 179  14 195 426\n```\n:::\n\n```{.r .cell-code}\nhead(DtestIndex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  1  3  6  8  9 15\n```\n:::\n:::\n\n## NB model\n\n::: {.cell}\n\n```{.r .cell-code}\npolarity_NaiveBayes <- textmodel_nb(DdfmTrain, docvars(DdfmTrain, \"polarity\"), distribution = \"Bernoulli\") \nsummary(polarity_NaiveBayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\ntextmodel_nb.dfm(x = DdfmTrain, y = docvars(DdfmTrain, \"polarity\"), \n    distribution = \"Bernoulli\")\n\nClass Priors:\n(showing first 3 elements)\nnegative  neutral positive \n  0.3333   0.3333   0.3333 \n\nEstimated Feature Scores:\n             wear     mask      '      re    dumb argument     gas pipelines\nnegative 0.142857 0.142857 0.4286 0.14286 0.14286 0.142857 0.14286   0.14286\nneutral  0.008163 0.008163 0.1347 0.02857 0.02041 0.008163 0.02041   0.02449\npositive 0.023810 0.023810 0.1667 0.04762 0.02381 0.071429 0.09524   0.04762\n          energy independence   right     now     new  england     gets\nnegative 0.14286     0.142857 0.14286 0.28571 0.14286 0.142857 0.142857\nneutral  0.02449     0.008163 0.02449 0.03265 0.01633 0.004082 0.004082\npositive 0.07143     0.047619 0.11905 0.11905 0.11905 0.047619 0.095238\n              lng deliveries international shipments   nthese corporations\nnegative 0.142857   0.142857      0.142857  0.142857 0.142857     0.142857\nneutral  0.004082   0.004082      0.004082  0.004082 0.004082     0.008163\npositive 0.047619   0.047619      0.047619  0.047619 0.047619     0.047619\n          making   profit another    story       vg  blckrck      use   public\nnegative 0.14286 0.142857 0.14286 0.142857 0.142857 0.142857 0.142857 0.142857\nneutral  0.01633 0.008163 0.01633 0.008163 0.004082 0.004082 0.008163 0.008163\npositive 0.07143 0.047619 0.04762 0.047619 0.047619 0.047619 0.047619 0.357143\n            funds\nnegative 0.142857\nneutral  0.004082\npositive 0.047619\n```\n:::\n\n```{.r .cell-code}\nDdfmTestMatched <- dfm_match(DdfmTest, features = featnames(DdfmTrain))\n```\n:::\n\n\n### CONFUSION MATRIX\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a confusion matrix \nDactual <- docvars(DdfmTestMatched, \"polarity\")\nDpredicted <- predict(polarity_NaiveBayes, newdata = DdfmTestMatched)\nDconfusion <- table(Dactual, Dpredicted)\n\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(Dconfusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Dpredicted\nDactual    negative neutral positive\n  negative        0       1        0\n  neutral         0      84        0\n  positive        0      12        0\n\nOverall Statistics\n                                          \n               Accuracy : 0.866           \n                 95% CI : (0.7817, 0.9267)\n    No Information Rate : 1               \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                       NA         0.8660              NA\nSpecificity                  0.98969             NA          0.8763\nPos Pred Value                    NA             NA              NA\nNeg Pred Value                    NA             NA              NA\nPrecision                    0.00000         1.0000          0.0000\nRecall                            NA         0.8660              NA\nF1                                NA         0.9282              NA\nPrevalence                   0.00000         1.0000          0.0000\nDetection Rate               0.00000         0.8660          0.0000\nDetection Prevalence         0.01031         0.8660          0.1237\nBalanced Accuracy                 NA             NA              NA\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDpredicted_prob <- predict(polarity_NaiveBayes, newdata = DdfmTestMatched, type = \"probability\")\nhead(Dpredicted_prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            negative   neutral     positive\ntext1  1.925127e-102 1.0000000 1.708320e-16\ntext3   4.365338e-91 1.0000000 1.285406e-12\ntext6   2.104851e-89 1.0000000 6.880633e-09\ntext8   5.873103e-86 0.9999995 4.570643e-07\ntext9   7.688654e-95 1.0000000 1.265155e-12\ntext17 4.872562e-103 1.0000000 7.091057e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(Dpredicted_prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    negative            neutral     positive        \n Min.   :0.000e+00   Min.   :1   Min.   :0.000e+00  \n 1st Qu.:0.000e+00   1st Qu.:1   1st Qu.:0.000e+00  \n Median :0.000e+00   Median :1   Median :0.000e+00  \n Mean   :2.885e-87   Mean   :1   Mean   :9.401e-09  \n 3rd Qu.:0.000e+00   3rd Qu.:1   3rd Qu.:1.300e-12  \n Max.   :2.149e-85   Max.   :1   Max.   :4.571e-07  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# The most positive review\nmostPos <- sort.list(Dpredicted_prob[,1], dec=F)[1]\nas.character(corpus_subset(DiehlCorpus_P, id %in% DtestIndex))[mostPos]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              text39 \n\" httpstcogihndiqwj\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmostNeg <- sort.list(Dpredicted_prob[,1], dec=T)[1]\nas.character(corpus_subset(DiehlCorpus_P, id %in% DtestIndex))[mostNeg]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                                                                                                        text331 \n\" won’t also statement electric prices going  tomorrow customers categorically false lastly take personal responsibility curb energy use many trips dunks conserve haven’t turn heat house yet\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# The most positive review\nDmixed <- sort.list(abs(Dpredicted_prob[,1] - .5), dec=F)[1]\nDpredicted_prob[Dmixed,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     negative       neutral      positive \n1.925127e-102  1.000000e+00  1.708320e-16 \n```\n:::\n\n```{.r .cell-code}\nas.character(corpus_subset(DiehlCorpus_P, id %in% DtestIndex))[Dmixed]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             text1 \n\" still beats fascism day week 📴\" \n```\n:::\n:::\n\n\n\n## SVM\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(123)\n\n# sample smaller set of training data\nDnewTrainIndex <- DtrainIndex[sample(1:length(DtrainIndex))]\n\n# create small DFM\nDdfmTrainSmall <- corpus_subset(DiehlCorpus_P, id %in% DnewTrainIndex) %>% dfm(remove = stopwords(\"English\"), remove_punct=T)\n\n# trim the DFM down to frequent terms\nDdfmTrainSmall <- dfm_trim(DdfmTrainSmall, min_docfreq = 2, min_termfreq = 2)\n\ndim(DdfmTrainSmall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 288 447\n```\n:::\n:::\n\n### run model\n\n::: {.cell}\n\n```{.r .cell-code}\nDpolarity_SVM <- textmodel_svm(DdfmTrainSmall, docvars(DdfmTrainSmall, \"polarity\")) \n\n# update test set\nDdfmTestMatchedSmall <- dfm_match(DdfmTest, features = featnames(DdfmTrainSmall))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a confusion matrix \nDactual <- docvars(DdfmTestMatchedSmall, \"polarity\")\nDpredicted <- predict(Dpolarity_SVM, newdata = DdfmTestMatchedSmall)\nDconfusion <- table(Dactual, Dpredicted)\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(Dconfusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Dpredicted\nDactual    negative neutral positive\n  negative        0       1        0\n  neutral         1      82        1\n  positive        0       6        6\n\nOverall Statistics\n                                          \n               Accuracy : 0.9072          \n                 95% CI : (0.8312, 0.9567)\n    No Information Rate : 0.9175          \n    P-Value [Acc > NIR] : 0.7222          \n                                          \n                  Kappa : 0.5276          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                  0.00000         0.9213         0.85714\nSpecificity                  0.98958         0.7500         0.93333\nPos Pred Value               0.00000         0.9762         0.50000\nNeg Pred Value               0.98958         0.4615         0.98824\nPrecision                    0.00000         0.9762         0.50000\nRecall                       0.00000         0.9213         0.85714\nF1                               NaN         0.9480         0.63158\nPrevalence                   0.01031         0.9175         0.07216\nDetection Rate               0.00000         0.8454         0.06186\nDetection Prevalence         0.01031         0.8660         0.12371\nBalanced Accuracy            0.49479         0.8357         0.89524\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# SVM coeff\nDsvmCoefs <- as.data.frame(t(coefficients(Dpolarity_SVM)))\nhead(DsvmCoefs,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 neutral    positive      negative\nre           -0.03132930  0.05928472 -0.0329520937\ndumb          0.14182580 -0.04494786 -0.0874533193\nargument     -0.20821243  0.20288659 -0.0002532952\ngas          -0.17606195  0.13348240 -0.0058732396\npipelines     0.05733301 -0.09047331  0.0000000000\nenergy       -0.05797844  0.03723350 -0.0000666509\nindependence  0.00000000  0.00000000  0.0000000000\nright        -0.01302839  0.06484791 -0.0426623835\nnow          -0.16246334  0.15407481  0.0287002076\nnew          -0.18549462  0.19847717 -0.0056908459\n```\n:::\n\n```{.r .cell-code}\ntail(DsvmCoefs,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  neutral      positive    negative\nresults      8.104458e-02 -2.514501e-02 -0.03999678\ntom         -1.952894e-02 -8.281253e-02  0.02176794\npot          1.666422e-01 -1.438545e-01  0.00000000\nnorthampton -3.469447e-18  0.000000e+00  0.00000000\ndraining    -9.221612e-03  3.172656e-02  0.00000000\neffected     0.000000e+00  0.000000e+00  0.00000000\nhealth      -3.469447e-18  3.469447e-18  0.00000000\npregnancy    0.000000e+00  3.469447e-18  0.00000000\nabortion     5.451712e-02 -9.680120e-03 -0.04875344\nBias         1.013049e+00 -1.061780e+00 -0.98978009\n```\n:::\n:::\n\n\n## RANDOM FOREST\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(randomForest): there is no package called 'randomForest'\n```\n:::\n\n```{.r .cell-code}\nDdfmTrainSmallRf <- convert(DdfmTrainSmall, to = \"matrix\")\nDdfmTestMatchedSmallRf <- convert(DdfmTestMatchedSmall, to = \"matrix\")\n\nset.seed(123)\nDiehl_polarity_RF <- randomForest(DdfmTrainSmallRf, \n                            y = as.factor(docvars(DdfmTrainSmall)$polarity),\n                            xtest = DdfmTestMatchedSmallRf, \n                            ytest = as.factor(docvars(DdfmTestMatchedSmall)$polarity),\n                            importance = TRUE,\n                            mtry = 20,\n                            ntree = 100\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in randomForest(DdfmTrainSmallRf, y = as.factor(docvars(DdfmTrainSmall)$polarity), : could not find function \"randomForest\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDactual <- as.factor(docvars(DdfmTestMatchedSmall)$polarity)\nDpredicted <- Diehl_polarity_RF$test[['predicted']]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'Diehl_polarity_RF' not found\n```\n:::\n\n```{.r .cell-code}\nDconfusion <- table(Dactual,Dpredicted)\nconfusionMatrix(Dconfusion, mode=\"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Dpredicted\nDactual    negative neutral positive\n  negative        0       1        0\n  neutral         1      82        1\n  positive        0       6        6\n\nOverall Statistics\n                                          \n               Accuracy : 0.9072          \n                 95% CI : (0.8312, 0.9567)\n    No Information Rate : 0.9175          \n    P-Value [Acc > NIR] : 0.7222          \n                                          \n                  Kappa : 0.5276          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                  0.00000         0.9213         0.85714\nSpecificity                  0.98958         0.7500         0.93333\nPos Pred Value               0.00000         0.9762         0.50000\nNeg Pred Value               0.98958         0.4615         0.98824\nPrecision                    0.00000         0.9762         0.50000\nRecall                       0.00000         0.9213         0.85714\nF1                               NaN         0.9480         0.63158\nPrevalence                   0.01031         0.9175         0.07216\nDetection Rate               0.00000         0.8454         0.06186\nDetection Prevalence         0.01031         0.8660         0.12371\nBalanced Accuracy            0.49479         0.8357         0.89524\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(Diehl_polarity_RF)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in varImpPlot(Diehl_polarity_RF): could not find function \"varImpPlot\"\n```\n:::\n:::\n\n\n\n## Naive Bayes -Held out\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDactual <- docvars(DdfmHeldOut)$polarity\ncount(Dactual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x freq\n1 negative    1\n2  neutral  169\n3 positive   31\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDdfmHeldOutMatched <- dfm_match(DdfmHeldOut, features = featnames(DdfmTrain))\nDpredicted.nb <- predict(polarity_NaiveBayes, DdfmHeldOutMatched, force =  TRUE )\ncount(Dpredicted.nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x freq\n1  neutral  196\n2 positive    5\n```\n:::\n\n```{.r .cell-code}\nDconfusion <- table(Dactual, Dpredicted.nb)\nconfusionMatrix(Dconfusion, mode = \"everything\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Dpredicted.nb\nDactual    negative neutral positive\n  negative        0       1        0\n  neutral         0     169        0\n  positive        0      26        5\n\nOverall Statistics\n                                          \n               Accuracy : 0.8657          \n                 95% CI : (0.8106, 0.9096)\n    No Information Rate : 0.9751          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.238           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                       NA         0.8622         1.00000\nSpecificity                 0.995025         1.0000         0.86735\nPos Pred Value                    NA         1.0000         0.16129\nNeg Pred Value                    NA         0.1562         1.00000\nPrecision                   0.000000         1.0000         0.16129\nRecall                            NA         0.8622         1.00000\nF1                                NA         0.9260         0.27778\nPrevalence                  0.000000         0.9751         0.02488\nDetection Rate              0.000000         0.8408         0.02488\nDetection Prevalence        0.004975         0.8408         0.15423\nBalanced Accuracy                 NA         0.9311         0.93367\n```\n:::\n:::\n",
    "supporting": [
      "MAGov_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}