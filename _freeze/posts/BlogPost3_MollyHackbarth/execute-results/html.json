{
  "hash": "3bc1eb6ba64b10a3bf8c4c704354d41f",
  "result": {
    "markdown": "---\ntitle: \"Blog Post Three\"\nauthor: \"Molly Hackbarth\"\ndesription: \"Working with the data\"\ndate: \"10/15/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog posts\n  - hw3\n  - molly hackbarth\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cld3)\nlibrary(dplyr)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(here)\nlibrary(devtools)\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(quanteda)\nlibrary(preText)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(hunspell)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(tm)\nlibrary(emojifont)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Research Question\n\n**My current research question:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?\n\n# Working with the Data\n\nIn my previous blog post I had started to clean the data using two different methods. After discussing I decided I'll remove the \"textclean\" package in case of issues. I will also do some back tracking to have both twitter and reddit in separate databases.\n\n### Adding Data in Separately\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_data <- read.csv(here::here(\"posts\", \"_data\", \"loveisblindjapan.csv\"))\n\ntwitter1 <- read.csv(here::here(\"posts\", \"_data\", \"tweets.csv\"))\n\ntwitter2 <- read.csv(here::here(\"posts\", \"_data\", \"tweets#.csv\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit <- subset(reddit_data, select = c(\"body\", \"created_utc\")) \n\nreddit$created_utc <- as.Date.POSIXct(reddit$created_utc)\n\nreddit <- reddit %>% \n  select(text = body, \n            date = created_utc)\n# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])\nreddit <- reddit %>% \n  filter(!text == '[deleted]') %>% \n  filter(!text == '[removed]')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#remove counting column\ntwitter1 <- twitter1 %>% select(!c(X, User))\ntwitter2 <- twitter2 %>% select(!c(X, User))\n\ntwitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)\nwrite.csv(twitter, here::here(\"posts\", \"_data\", \"twitter.csv\") , all(T) )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in utils::write.table(twitter, here::here(\"posts\", \"_data\",\n\"twitter.csv\"), : appending column names to file\n```\n:::\n\n```{.r .cell-code}\nnames(twitter) <- tolower(names(twitter))\ntwitter <- twitter %>% \n  rename_at('tweet', ~ 'text', \n            'Date' ~ 'date')\ntwitter$date <- as.Date(strftime(twitter$date, format=\"%Y-%m-%d\"))\n\n# remove duplicate tweets\ntwitter <- twitter %>% distinct(text, date, .keep_all = TRUE)\n\n#check for duplicate tweets\ntwitter %in% unique(twitter[ duplicated(twitter)]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nallsocialmedia <- merge(twitter, reddit, by=c('text','text', 'date', 'date'),all=T, ignore_case =T)\nwrite.csv(twitter, here::here(\"posts\", \"_data\", \"loveisblind_socialmedia.csv\") , all(T) )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in utils::write.table(twitter, here::here(\"posts\", \"_data\",\n\"loveisblind_socialmedia.csv\"), : appending column names to file\n```\n:::\n:::\n\n\n## Using HunSpell\n\nAfter listening to class lectures, I decided to go ahead and download the \"hunspell\" package so I could look at my errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspell_check_reddit <- hunspell(reddit$text)\n\nspell_check_twitter <- hunspell(twitter$text)\n```\n:::\n\n\nHere I'm able to see that Twitter more often used shortened words (i.e. \"ppl\" instead of \"people\"). However I also noticed that Twitter more often used non english. This is likely due to Reddit being a mostly English speaking platform compared to Twitter.\n\nOverall though I think that the data wasn't too bad. I still would like to remove htmls (they aren't helpful to analyzing). However in class a good point was brought up that emojis do often display emotions. Thus I think for now I will leave them in.\n\nFor non English I'll still remove them as I can't accurately analyze them without knowing the language.\n\nBelow you will find ways that didn't seem to work for me,\n\n### Cleaning the Data and Creating Tokens\n\nTo see if cleaning the data works I'll go ahead and test first with Reddit data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- corpus(reddit)\nredditsummary <- summary(reddit_corpus)\n\n#remove non english languages \nreddit_corpus <- subset(reddit_corpus, detect_language(reddit_corpus) == \"en\") \n\n#remove htmls\nreddit_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", reddit_corpus)\n\n#remove emojis\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\nreddit_corpus <- reddit_corpus %>% \n str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove htmls\nreddit_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", reddit_corpus)\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T)\n\nreddit_corpus_tokens <- tokens_tolower(reddit_corpus_tokens)\n\nreddit_corpus_tokens <- tokens_select(reddit_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n```\n:::\n\n\n### Trying to create a DTM\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(reddit_lemmitized) \nmatrix <- as.matrix(dtm) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError: vector memory exhausted (limit reached?)\n```\n:::\n\n```{.r .cell-code}\nwords <- sort(rowSums(matrix),decreasing=TRUE) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in base::rowSums(x, na.rm = na.rm, dims = dims, ...): 'x' must be an array of at least two dimensions\n```\n:::\n\n```{.r .cell-code}\ndf <- data.frame(word = names(words),freq=words)\n```\n:::\n\n\nWhile this works for creating tokens, unfortunately it has removed the date column. While this is fine to gather an overall sentiment, it would be nice to keep the other column there.\n\nAdditionally trying to create a dtm from this doesn't seem to work.\n\n### Trying a Different Way\n\nBelow you will see me attempt to have a corpus vector date (where it says large simple corpus). This is because I would like the date columns. I will once again try with Reddit first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- corpus(reddit)\n\n#remove non english languages \ntest$text <- subset(test$text, detect_language(test) == \"en\") \n\n#remove htmls\ntest$text <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", test$text)\n\n#remove emojis\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\ntest$text <- test$text %>% \n str_replace_all(regex(only_ascii_regexp), \"\") \n\ntest <- Corpus(VectorSource(test))\n\ntest <- test %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace) %>% # remove stop words\n  tm_map(content_transformer(tolower)) %>% \n  tm_map(removeWords, stopwords(\"english\")) # remove stop words\n```\n:::\n\n\n#### Trying to Create Tokens\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_tokens <- tokens(test, \n    remove_punct = T,\n    remove_numbers = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: tokens() only works on character, corpus, list, tokens objects.\n```\n:::\n\n```{.r .cell-code}\ntest_tokens <- tokens_tolower(test_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(test_tokens): object 'test_tokens' not found\n```\n:::\n\n```{.r .cell-code}\nlemmitized <- tokens_replace(test_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_replace(test_tokens, pattern = lexicon::hash_lemmas$token, : object 'test_tokens' not found\n```\n:::\n:::\n\n\nWhile this has created a large simple corpus which has included the two columns I mentioned, I can't seem to create \"tokens\" this way which could make researching difficult.\n\n#### DTM & Word Cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(test) \nmatrix <- as.matrix(dtm) \nwords <- sort(rowSums(matrix),decreasing=TRUE) \ndf <- data.frame(word = names(words),freq=words)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nset.seed(1234) # for reproducibility \nwordcloud(words = df$word, freq = df$freq, min.freq = 50,           max.words=100, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/wordcloud-1.png){width=672}\n:::\n:::\n\n\nHere we can see the word cloud worked!\n\nI also noticed despite removing emojis and other languages this way it seems to still be in the matrix. While it only shows up once and awhile it is sort of strange.\n\n### Additional TM code\n\nTo try to help to remove more I will use addition tm code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\ntest <- test %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace) %>% \n  tm_map(toSpace, \"/\") %>% \n  tm_map(toSpace, \"@\") %>% \n  tm_map(toSpace, \"\\\\|\")\n```\n:::\n\n\n### Dealing with some issues\n\nI realized my current removing non English is not working completely. Thus I'm trying a different formula in addition to make it work.\n\nI also realized I was using tm_map to remove numbers which may have caused an issue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest2 <- corpus(reddit)\ntest2 <- subset(test2, detect_language(test2) == \"en\") \n\ntest2 <- stringi::stri_trans_general(test2, \"latin-ascii\")\n\n\n#remove htmls\ntest2 <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", test2)\n\ntest2 <- Corpus(VectorSource(reddit_tm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in SimpleSource(length = length(x), content = x, class = \"VectorSource\"): object 'reddit_tm' not found\n```\n:::\n\n```{.r .cell-code}\ntest2 <- subset(test2, detect_language(test2) == \"en\") \n\ntest2 <- stringi::stri_trans_general(test2, \"latin-ascii\")\n\n#remove htmls\ntest2 <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", test3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'test3' not found\n```\n:::\n\n```{.r .cell-code}\n#remove emojis\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\ntest2 <- test2 %>% \n str_replace_all(regex(only_ascii_regexp), \"\") \n\n#change non letters to spaces\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n\ntest2 <- test %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace) %>% # remove stop words\n  tm_map(toSpace, \"/\") %>% \n  tm_map(toSpace, \"@\") %>% \n  tm_map(toSpace, \"\\\\|\") %>% \n  tm_map(content_transformer(tolower)) %>% \n  tm_map(removeWords, stopwords(\"english\")) # remove stop words\n```\n:::\n\n\n#### Trying a DTM & Word Cloud Again\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(test2) \nmatrix <- as.matrix(dtm) \nwords <- sort(rowSums(matrix),decreasing=TRUE) \ndf <- data.frame(word = names(words),freq=words)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wordcloud)\nset.seed(1234) # for reproducibility \nwordcloud(words = df$word, freq = df$freq, min.freq = 500, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/updated wordcloud-1.png){width=672}\n:::\n:::\n\n\nThis seems to have kept the dates in the corpus! However here I noticed that the dates are also in the word cloud which I don't want.\n\nAdditionally I learned I need a high minimum frequency to really have this work well. Otherwise there was an error saying it couldn't show multiple words.\n\n# Way that Worked - Word Cloud with Lemmitized\n\n### Another way for Word Clouds\n\nI was able to find another way to do text plots that don't involve using the \"tm\" package to create a corpus thanks to the class blog! Below you will see the word cloud for Reddit Post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nreddit_corpus_edit <- reddit_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(reddit_corpus_edit, max_words=100, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/word cloud-1.png){width=672}\n:::\n:::\n\n\nI first tried to do it allowing stop words, however it seems that it was overtaken by words such as \"I\", \"the\", and \"and\". So I put the remove stop words back in.\n\n#### Twitter Word Cloud\n\nBelow you will see the word cloud for Twitter posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_corpus <- corpus(twitter)\ntwittersummary <- summary(twitter_corpus)\n\n#remove non english languages \ntwitter_corpus <- subset(twitter_corpus, detect_language(twitter_corpus) == \"en\") \n\ntwitter_corpus <- stringi::stri_trans_general(twitter_corpus, \"latin-ascii\")\n\n#remove htmls\ntwitter_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", twitter_corpus)\n\ntwitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]\n\n\ntwitter_corpus_tokens <- tokens(twitter_corpus, \n    remove_punct = T,\n    remove_numbers = T)\n\ntwitter_corpus_tokens <- tokens_tolower(twitter_corpus_tokens)\n\ntwitter_corpus_tokens <- tokens_select(twitter_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized <- tokens_replace(twitter_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntwitter_corpus_edit <- twitter_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(twitter_corpus_edit, max_words=100, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/cleaning tweets-1.png){width=672}\n:::\n:::\n\n\nSomething I noticed was that when I tried to subset with English, the amount of tweets in the Twitter corpus went down by almost half. This seemed to be an error issue as when I looked at my data it shouldn't have gone down more than 1,000. This has lead me to remove the detect language function. I have added a new function that strip non-ascii characters which will remove emojis and non English.\n\nSomething else I noticed for Twitter is that it seems that some non English or emojis are frequently used. My guess is that it's mostly \"love is blind japan\" in Japanese.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(emojifont) #reads japanese?\n\nonly_ascii_regexp <- '[^\\u0001-\\u03FF]+|<U\\\\+\\\\w+>'\n\ntwittertest <- twitter$text %>%\n  str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove non english languages \n\n#twittertest <- stringi::stri_trans_general(twitter_corpus, \"latin-ascii\")\n\n#remove htmls\ntwittertest <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", twittertest)\n\ntwittertest <- twittertest[!is.na(twittertest)]\n\ntwitter_corpus_tokens1 <- tokens(twittertest, \n    remove_punct = T,\n    remove_numbers = T)\n\ntwitter_corpus_tokens1 <- tokens_tolower(twitter_corpus_tokens1)\n\ntwitter_corpus_tokens1 <- tokens_select(twitter_corpus_tokens1, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized1 <- tokens_replace(twitter_corpus_tokens1, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntwittertest1 <- twittertest %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'dfm.character()' is deprecated. Use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(twittertest1, max_words=100, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/adding emojifont-1.png){width=672}\n:::\n:::\n\n\n[**A quick note: For some reason the TM package and word clouds were a bit finicky. It sometimes seemed to work and sometimes did not.**]{.underline}\n\n# Code For New Word Clouds\n\n### Lemmetize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- corpus(reddit)\nredditsummary <- summary(reddit_corpus)\n\nonly_ascii_regexp <- '[^\\u0001-\\u03FF]+|<U\\\\+\\\\w+>'\n\nreddit_corpus <- reddit$text %>%\n  str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove non english languages \nreddit_corpus <- stringi::stri_trans_general(reddit_corpus, \"latin-ascii\")\n\n#remove emojis\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\nreddit_corpus <- reddit_corpus %>% \n str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove htmls\nreddit_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", reddit_corpus)\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\n\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T)\n\nreddit_corpus_tokens <- tokens_tolower(reddit_corpus_tokens)\n\nreddit_corpus_tokens <- tokens_select(reddit_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nreddit_corpus_edit <- reddit_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(reddit_corpus_edit, max_words=200, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/reddit lem-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emojifont) #reads japanese?\n\nonly_ascii_regexp <- '[^\\u0001-\\u03FF]+|<U\\\\+\\\\w+>'\n\ntwitter_corpus<- twitter$text %>%\n  str_replace_all(regex(only_ascii_regexp), \"\") \n\ntwitter_corpus <- corpus(twitter_corpus)\ntwittersummary <- summary(twitter_corpus)\n\n#remove non english languages \ntwitter_corpus <- stringi::stri_trans_general(twitter_corpus, \"latin-ascii\")\n\n#remove htmls\ntwitter_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", twitter_corpus)\n\ntwitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]\n\ntwitter_corpus_tokens <- tokens(twitter_corpus, \n    remove_punct = T,\n    remove_numbers = T)\n\ntwitter_corpus_tokens <- tokens_tolower(twitter_corpus_tokens)\n\ntwitter_corpus_tokens <- tokens_select(twitter_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized <- tokens_replace(twitter_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntwitter_corpus_edit <- twitter_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(twitter_corpus_edit, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/twitter lem-1.png){width=672}\n:::\n:::\n\n\nSome interesting facts for Lemmetize plot\n\n-   #loveisblindjapan is one of the largest along with blind, love and japan for twitter. This is unsurprising as that's the name of the show.\n\n-   Reddit used the word like and think a lot. However interestingly Japanese is mentioned more than the word Japan.\n\n-   The names of the actual contestants are not mentioned as much as I would think. They're still mentioned often but are not major words in the word cloud.\n\n-   Japanese characters and possibly emojis both made there way into the word cloud. I'm unsure why.\n\n### TM Package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_tm <- corpus(reddit)\n\nonly_ascii_regexp <- '[^\\u0001-\\u03FF]+|<U\\\\+\\\\w+>'\n\nreddit_tm <- reddit$text %>%\n  str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove non english languages \nreddit_tm <- stringi::stri_trans_general(reddit_tm, \"latin-ascii\")\n\n#remove htmls\nreddit_tm <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", reddit_tm)\n\nreddit_tm <- Corpus(VectorSource(reddit_tm))\n\n#change non letters to spaces\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n\nreddit_tm <- reddit_tm %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace) %>% # remove multiple white space\n  tm_map(toSpace, \"/\") %>% \n  tm_map(toSpace, \"@\") %>% \n  tm_map(toSpace, \"\\\\|\") %>% \n  tm_map(content_transformer(tolower)) %>% \n  tm_map(removeWords, stopwords(\"english\")) # remove stop words\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removePunctuation): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., stripWhitespace): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"/\"): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"@\"): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"\\\\|\"): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., content_transformer(tolower)): transformation\ndrops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removeWords, stopwords(\"english\")):\ntransformation drops documents\n```\n:::\n\n```{.r .cell-code}\nreddit_dtm <- TermDocumentMatrix(reddit_tm) \nreddit_matrix <- as.matrix(reddit_dtm) \nreddit_words <- sort(rowSums(reddit_matrix),decreasing=TRUE) \nreddit_df <- data.frame(words = names(words),freq=words)\n\nset.seed(1234) # for reproducibility \nwordcloud(words = reddit_df$word, freq = reddit_df$freq, min.freq = 50,           max.words=100, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/reddit TM-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_tm <- corpus(twitter)\n\nonly_ascii_regexp <- '[^\\u0001-\\u03FF]+|<U\\\\+\\\\w+>'\n\ntwitter_tm <- twitter$text %>%\n  str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove non english languages \ntwitter_tm <- stringi::stri_trans_general(twitter_tm, \"latin-ascii\")\n\n#remove htmls\ntwitter_tm <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", twitter_tm)\n\ntwitter_tm <- Corpus(VectorSource(twitter_tm))\n\n#change non letters to spaces\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n\ntwitter_tm <- twitter_tm %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace) %>% # remove multiple white space\n  tm_map(toSpace, \"/\") %>% \n  tm_map(toSpace, \"@\") %>% \n  tm_map(toSpace, \"\\\\|\") %>%\n  tm_map(content_transformer(tolower)) %>% \n  tm_map(removeWords, stopwords(\"english\")) # remove stop words\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removePunctuation): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., stripWhitespace): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"/\"): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"@\"): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., toSpace, \"\\\\|\"): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., content_transformer(tolower)): transformation\ndrops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removeWords, stopwords(\"english\")):\ntransformation drops documents\n```\n:::\n\n```{.r .cell-code}\ntwitter_dtm <- TermDocumentMatrix(twitter_tm) \ntwitter_matrix <- as.matrix(twitter_dtm) \ntwitter_words <- sort(rowSums(twitter_matrix),decreasing=TRUE) \ntwitter_df <- data.frame(words = names(words),freq=words)\n\nset.seed(12345) # for reproducibility \nwordcloud(words = twitter_df$word, freq = twitter_df$freq, min.freq = 50,           max.words=100, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/twitter tm-1.png){width=672}\n:::\n:::\n\n\nSome interesting facts for TM:\n\n-   I thought that the 's was part of some sort of stop word. However it seems that it isn't for either. My guess is it may follow names of people in the show.\n-   Even with another version of removing emojis and non English there still seems to be some in the data, however not very much.\n-   Like and think were both very popular words for Twitter and Reddit.\n-   Not all the contestants were posted about the same amount.\n-   Despite the show being about marriage, the words married, marry, or marriage weren't the top words for either Twitter or Reddit.\n\nOverall between the two types of word clouds, I prefer the **lemmitized version** so I will continue using that one.\n\nHowever I am surprised by the fact that they're so different. The only difference in code is really the \"toSpace\" which removes just a couple of extra things.\n\n## Emoji Thoughts\n\nUsing the function that I found from the DACSS slack channel I found that it also has a nice side affect of removing non English as well.\n\nAdditionally I've noticed that there's has been few emojis that have made it into the word cloud. I also don't feel confident in decoding emojis and being able to do a sentiment analysis.\n\n**Thus for this project I will not keep emojis.**\n\n### Attempting to Remove Japanese\n\nSince I noticed there was a lot of Japanese still left in the word cloud I went to test if I could remove more with the previous function I had used. This along with the other three options seems to have the best results!\n\nBelow you will see a test version.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_rm_jap = function(x) {\n  #we replace japanese blocks with nothing, and clean any double whitespace from this\n  #reference at http://www.rikai.com/library/kanjitables/kanji_codes.unicode.shtml\n  x %>% \n    #japanese style punctuation\n    str_replace_all(\"[\\u3000-\\u303F]\", \"\") %>% \n    #katakana\n    str_replace_all(\"[\\u30A0-\\u30FF]\", \"\") %>% \n    #hiragana\n    str_replace_all(\"[\\u3040-\\u309F]\", \"\") %>% \n    #kanji\n    str_replace_all(\"[\\u4E00-\\u9FAF]\", \"\") %>% \n    #remove excess whitespace\n    str_replace_all(\"  +\", \" \") %>% \n    str_trim()\n}\n\ntest_corpus<- twitter$text%>% str_rm_jap\n\ntest_posts <- test_corpus %>%\n str_replace_all(regex(only_ascii_regexp), \"\") \n\n#remove non english languages \ntest_posts <- stringi::stri_trans_general(test_posts, \"latin-ascii\")\n\n#remove htmls\ntest_corpus <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", test_corpus)\n\ntest_corpus <- test_corpus[!is.na(test_corpus)]\n\ntest_corpus_tokens <- tokens(test_corpus, \n    remove_punct = T,\n    remove_numbers = T)\n\ntest_corpus_tokens <- tokens_tolower(test_corpus_tokens)\n\ntest_corpus_tokens <- tokens_select(test_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntest_lemmitized <- tokens_replace(test_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntest_corpus_edit <- test_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 50, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(test_corpus_edit, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/test removing japanese-1.png){width=672}\n:::\n:::\n\n\nWhile doing this I noticed there were a lot of Spanish words as well. In order to combat this I decided to go back to using the subset function.\n\n### Removing all non English\n\nWhile this has taken care of a lot of issues, unfortunately it has removed a lot of data. This has left me with only **7,990 tweets** out of the original 14,866.\n\nI also have added remove_symbols to the tokens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- corpus(twitter$text)\n\nlibrary(cld3)\ntest <- subset(test, detect_language(test) == \"en\")\n\ntest <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", test)\n\ntest <- test[!is.na(test)]\n\ntest_tokens <- tokens(test, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T)\n\ntest_tokens <- tokens_tolower(test_tokens)\n\ntest_tokens <- tokens_select(test_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntests_lemmitized <- tokens_replace(test_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntest_edit <- tests_lemmitized %>% \n  dfm(remove = stopwords('english'), remove_punct = TRUE) %>% \n  dfm_trim(min_termfreq = 50, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'remove' is deprecated; use dfm_remove() instead\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(test_edit, max_words=300, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/testing using subset-1.png){width=672}\n:::\n:::\n\n\n# Updated Word Cloud Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- subset(reddit, detect_language(reddit) == \"en\")\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\nreddit_corpus <- corpus(reddit)\nredditsummary <- summary(reddit_corpus)\n\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T, \n    remove_symbols = T,\n    remove_url = T)\n\nreddit_corpus_tokens <- tokens_tolower(reddit_corpus_tokens)\n\nreddit_corpus_tokens <- tokens_select(reddit_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nreddit_corpus_dfm <- reddit_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(reddit_corpus_dfm, max_words=200, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/reddit word cloud 2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_corpus <- subset(twitter, detect_language(twitter) == \"en\")\ntwitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]\ntwitter_corpus <- corpus(twitter_corpus)\ntwittersummary <- summary(twitter_corpus)\n\ntwitter_corpus_tokens <- tokens(twitter_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T)\n\ntwitter_corpus_tokens <- tokens_tolower(twitter_corpus_tokens)\n\ntwitter_corpus_tokens <- tokens_select(twitter_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized <- tokens_replace(twitter_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\ntwitter_corpus_dfm <- twitter_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/twitter word cloud-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsocial_corpus <- subset(allsocialmedia, detect_language(allsocialmedia) == \"en\")\nsocial_corpus <- corpus(social_corpus)\nsocialsummary <- summary(social_corpus)\n\nsocial_corpus <- social_corpus[!is.na(social_corpus)]\n\nsocial_corpus_tokens <- tokens(social_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T)\n\nsocial_corpus_tokens <- tokens_tolower(social_corpus_tokens)\n\nsocial_corpus_tokens <- tokens_select(social_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nsocial_lemmitized <- tokens_replace(social_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nsocial_corpus_dfm <- social_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(social_corpus_dfm, max_words=200, color=\"orange\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/both social medias word cloud-1.png){width=672}\n:::\n:::\n\n\n# Looking at the Data Closer\n\nBelow you will see some quick looks at data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(social_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             love             japan              like             blind \n             7352              5886              5647              5270 \n            think              just #loveisblindjapan              good \n             4582              3670              3445              3417 \n              say               get \n             3378              2982 \n```\n:::\n\n```{.r .cell-code}\ntopfeatures(twitter_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             love             blind             japan #loveisblindjapan \n             5965              5068              4766              3445 \n            watch              like              good              just \n             2135              1200              1001               936 \n              get              show \n              806               727 \n```\n:::\n\n```{.r .cell-code}\ntopfeatures(reddit_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  like  think    say   just   good    get   show really    see   feel \n  4504   4120   2856   2750   2455   2196   2120   2007   1953   1897 \n```\n:::\n:::\n\n\nHere we can see that overall the top ten words:\n\n| All Social Media  | Twitter           | Reddit |\n|-------------------|-------------------|--------|\n| love              | love              | like   |\n| japan             | blind             | think  |\n| like              | japan             | say    |\n| blind             | #loveisblindjapan | just   |\n| think             | watch             | good   |\n| just              | like              | get    |\n| #loveisblindjapan | good              | show   |\n| good              | just              | really |\n| say               | get               | see    |\n| get               | show              | feel   |\n\n: ~Love is Blind Japan Top Features~\n\nHere we can see a few things:\n\n-   While \"love\", \"blind\", and \"japan\" are the top features in Twitter, this is most likely due to the show's name. When comparing this to both the category social media that combine them and Reddit, like made it much higher.\n\n-   All three column's top ten words have multiple words that are focusing on how people are watching the show feel.\n\n-   From this brief look it seems there's a positive thought on show as \"good\" were both in the top ten for Twitter and Reddit.\n\n## Trying a Co-occurrence Matrix\n\nI will give a test try to having a max term frequency for Twitter, as Twitter's top four words have to do with the shows name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(twitter_corpus_dfm, max_termfreq = 3400, min_termfreq = 30)\nsmaller_dfm <- dfm_trim(smaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(smaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/cooccurrence-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 485 485\n```\n:::\n\n```{.r .cell-code}\nmyFeatures <- names(topfeatures(smaller_fcm, 30))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/cooccurrence-2.png){width=672}\n:::\n:::\n\n\n# Final Thoughts (TLDR)\n\n## Final thoughts\n\n-   I did end up looking at Youtube to see if there were clips of this show online that were from Netflix. There unfortunately only seemed to be a trailer for the show. Clips of this season were fairly rare there.\n\n-   After debating for awhile I decided to use the subset language option to remove all other languages for Twitter (Reddit did not have this problem most likely due to the main user base being English speaking Americans). Although this did cut my database down significantly I believe this was the best option as it removes any other language that I'm currently unable to analyze properly.\n\n    -   Conversely Reddit had very few other languages. However to keep the data similar I used subset for Reddit too.\n\n        -   This was also done so if I would like to use both the separate data and the data combined it is the same.\n\n-   I decided to use the quanteda.textplots for the word cloud instead of the TM package. This is due to the TM package having issues when I try to reload the word clouds.\n\n-   I have decided to lemmitize my tokens as it works to create a more accurate picture of words that were important to both Twitter and Reddit posters.\n\n-   I have removed emojis due to the issues of most of the emoji packages I have found not working. Additionally I'm not confident that I could accurately use them for a sentiment analysis, or that there are that many in the first place.\n\n-   For my tokens I have added remove_symbols and remove_url for both Reddit and Twitter to help out with people spamming urls to shows or gifs, and the \"\\@\" sign.\n\n-   When I checked the top features it was interesting to see what words were used the most. While none of it was surprising, it did help me to understand the words a bit more.\n\n-   I believe a co-occurance matrix may be useful for Twitter as it allows me to remove the name of the show and the hashtag better.\n\n## Questions and frustrations I still have\n\n-   I'm still unsure how to keep **both the date and the text** as separate columns using the corpus() function from quanteda. I can keep it in the summary, but I'm not sure if I can use the dates outside of that.\n\n    -   The summary also only goes to 100, I'm not sure how I can make it show me all of the data either.\n    -   When I tried having a maximum frequency for Twitter's word cloud it did bring out the dates in the word cloud but I'm not sure if that solves my problem either.\n\n-   I am unsure if looking at all the data together or separately is currently the best. I have a word cloud using all the social media data below. This would change my research question.\n\n    -   **Current Research Question:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?\n\n    -   **Possibly Research Question:** How do Reddit and Twitter users sentiment differentiate about the show *Love is Blind Japan*?\n\n        -   This is the question I had previously.\n\n## Future Work\n\nHere are a few things I'd like to do in blog post 4:\n\n-   **Try to group by date.** With the date column seemingly working now I'd like to see if grouping by date would work.\n\n-   **Try to use the textplot_network on reddit as well as the combined data set.** Although I'm not sure if I will I think it could produce some interesting results.\n\n-   **Try to find a way to remove the dates from the word clouds.** My current thought is to just make a separate r chunk that only has twitter\\$text to remove the dates.\n\n-   **Try to decide on how or if I want to use co-occurrence.**\n\n# Final Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write csv has been commented out due to it continously trying to save an \"updated version\" in Git. \n\nreddit_data <- read.csv(here::here(\"posts\", \"_data\", \"loveisblindjapan.csv\"))\n\ntwitter1 <- read.csv(here::here(\"posts\", \"_data\", \"tweets.csv\"))\n\ntwitter2 <- read.csv(here::here(\"posts\", \"_data\", \"tweets#.csv\"))\n\nreddit <- subset(reddit_data, select = c(\"body\", \"created_utc\")) \n\nreddit$created_utc <- as.Date.POSIXct(reddit$created_utc)\n\nreddit <- reddit %>% \n  select(text = body, \n            date = created_utc)\n# remove deleted or removed comments by moderators of the subreddit (ones that only contain [deleted] or [removed])\nreddit <- reddit %>% \n  filter(!text == '[deleted]') %>% \n  filter(!text == '[removed]')\n\n#remove counting column\ntwitter1 <- twitter1 %>% select(!c(X, User))\ntwitter2 <- twitter2 %>% select(!c(X, User))\n\ntwitter <- merge(twitter1, twitter2, by=c('Tweet','Tweet', 'Date', 'Date'),all=T, ignore_case =T)\n#write.csv(twitter, here::here(\"posts\", \"_data\", \"twitter.csv\") , all(T) )\n\nnames(twitter) <- tolower(names(twitter))\ntwitter <- twitter %>% \n  rename_at('tweet', ~ 'text', \n            'Date' ~ 'date')\ntwitter$date <- as.Date(strftime(twitter$date, format=\"%Y-%m-%d\"))\n\n# remove duplicate tweets\ntwitter <- twitter %>% distinct(text, date, .keep_all = TRUE)\n\n#check for duplicate tweets\ntwitter %in% unique(twitter[ duplicated(twitter)]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE\n```\n:::\n\n```{.r .cell-code}\nallsocialmedia <- merge(twitter, reddit, by=c('text','text', 'date', 'date'),all=T, ignore_case =T)\n#write.csv(twitter, here::here(\"posts\", \"_data\", \"loveisblind_socialmedia.csv\") , all(T) )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreddit_corpus <- subset(reddit, detect_language(reddit) == \"en\")\nreddit_corpus <- reddit_corpus[!is.na(reddit_corpus)]\nreddit_corpus <- corpus(reddit)\nredditsummary <- summary(reddit_corpus)\n\nreddit_corpus_tokens <- tokens(reddit_corpus, \n    remove_punct = T,\n    remove_numbers = T, \n    remove_symbols = T,\n    remove_url = T)\n\nreddit_corpus_tokens <- tokens_tolower(reddit_corpus_tokens)\n\nreddit_corpus_tokens <- tokens_select(reddit_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nreddit_lemmitized <- tokens_replace(reddit_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nreddit_corpus_dfm <- reddit_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(reddit_corpus_dfm, max_words=200, color=\"red\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/reddit word cloud-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_corpus <- subset(twitter, detect_language(twitter) == \"en\")\ntwitter_corpus <- twitter_corpus[!is.na(twitter_corpus)]\ntwitter_corpus <- corpus(twitter_corpus)\ntwittersummary <- summary(twitter_corpus)\n\ntwitter_corpus_tokens <- tokens(twitter_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T)\n\ntwitter_corpus_tokens <- tokens_tolower(twitter_corpus_tokens)\n\ntwitter_corpus_tokens <- tokens_select(twitter_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\ntwitter_lemmitized <- tokens_replace(twitter_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\ntwitter_corpus_dfm <- twitter_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(twitter_corpus_dfm, max_words=200, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/twitter word cloud 2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsocial_corpus <- subset(allsocialmedia, detect_language(allsocialmedia) == \"en\")\nsocial_corpus <- corpus(social_corpus)\nsocialsummary <- summary(social_corpus)\n\nsocial_corpus <- social_corpus[!is.na(social_corpus)]\n\nsocial_corpus_tokens <- tokens(social_corpus, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols = T,\n    remove_url = T)\n\nsocial_corpus_tokens <- tokens_tolower(social_corpus_tokens)\n\nsocial_corpus_tokens <- tokens_select(social_corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n\nsocial_lemmitized <- tokens_replace(social_corpus_tokens, \n                             pattern = lexicon::hash_lemmas$token, \n                             replacement = lexicon::hash_lemmas$lemma)\n\nlibrary(quanteda.textplots)\n\nsocial_corpus_dfm <- social_lemmitized %>% \n  dfm() %>% \n  dfm_remove(stopwords('english')) %>% \n  dfm_trim(min_termfreq = 30, verbose = FALSE)\n\ntextplot_wordcloud(social_corpus_dfm, max_words=200, color=\"orange\")\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/full social media word cloud-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures(social_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             love             japan              like             blind \n             7352              5886              5647              5270 \n            think              just #loveisblindjapan              good \n             4582              3670              3445              3417 \n              say               get \n             3378              2982 \n```\n:::\n\n```{.r .cell-code}\ntopfeatures(twitter_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             love             blind             japan #loveisblindjapan \n             5965              5068              4766              3445 \n            watch              like              good              just \n             2135              1200              1001               936 \n              get              show \n              806               727 \n```\n:::\n\n```{.r .cell-code}\ntopfeatures(reddit_corpus_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  like  think    say   just   good    get   show really    see   feel \n  4504   4120   2856   2750   2455   2196   2120   2007   1953   1897 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(twitter_corpus_dfm, max_termfreq = 3400, min_termfreq = 30)\nsmaller_dfm <- dfm_trim(smaller_dfm, max_docfreq = .3, docfreq_type = \"prop\")\n\ntextplot_wordcloud(smaller_dfm, min_count = 100,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/cooccurrence 2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 485 485\n```\n:::\n\n```{.r .cell-code}\nmyFeatures <- names(topfeatures(smaller_fcm, 30))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30 30\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_MollyHackbarth_files/figure-html/cooccurrence 2-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "BlogPost3_MollyHackbarth_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}