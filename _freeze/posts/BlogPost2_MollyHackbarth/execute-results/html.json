{
  "hash": "f641ecfe5e69d57246ff77ba08882605",
  "result": {
    "markdown": "---\ntitle: \"Blog Post two\"\nauthor: \"Molly Hackbarth\"\ndesription: \"Focusing on downloading data\"\ndate: \"10/01/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog posts\n  - hw2\n  - Molly Hackbarth\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cld3)\nlibrary(dplyr)\nlibrary(textclean)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(here)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## Understanding APIs and R packages\n\nIn order to understand how to download twitter and reddit I looked more into APIs. I've heard reddit is a bit frustrating, I decided to try twitter first. This included me downloading the R package \"rtweet\". Once I was able to use their API to create a project, I went ahead and tried to download multiple tweets.\n\n## Frustrations with APIs and R\n\nUnfortunately \"rtweet\" had a very similar issue to the package \"RedditextractorR\", both had a limit that made it difficult to work with. \"rtweet\" only allowed you to search from the last 6-9 days of tweets. This makes it hard to gather a lot of data over time. \"RedditextractorR\" only allowed you have comments from 7 posts at a time. It seems that using R for both types of packages proved to be very difficult.\n\nI also tried to use the package \"twitteR\" however it would not load properly for me. It kept giving me errors. Even with the properly set up Twitter API I was unable to have it connect to the account. This took over ***eight hours*** to try to get to work (including looking at multiple pages that suggested adding more packages to make both \"twitteR\" and \"rtweet\" to work) before I decide to give up on trying to using all of the packages.\n\n## Looking for a New Option\n\nI ended up deciding to look into other ways I could download tweets and Reddit posts. While most websites offered the same API options as mentioned before, a few of them recommended using Python instead.\n\nAfter awhile I ended up deciding to download Python and Visual Studio Code to run Python. I had little hope and had some frustrations with downloading the \"pip\" package but was able to download it.\n\n## Downloading Tweets and Reddit Posts through Python\n\nAfter finding a YouTube video I was able to use the python package \"[snscrape](https://github.com/mehranshakarami/AI_Spectrum/blob/main/2022/snscrape/tweets.py)\" that someone had created for python (you can watch the explanation of how it works [here](https://www.youtube.com/watch?v=jtIMnmbnOFo)!) in order to allow downloading tweets without having to us an API. This was extremely helpful as the whole time to download all of the tweets I was interested in (both #loveisblindjapan and \"love is blind japan\") were downloaded within a few minutes.\n\nFor the Reddit posts I used a [website](https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286) that explained to me how to download all the comments that were on the subreddit r/loveisblindjapan. This also only took a few minutes.\n\nBetween Reddit and Twitter I was able to download over 20k comments from users who watched the TV show.\n\n## Editing the Data in Google Sheets\n\nSince I did this through python I ended up saving the data into a csv file. This allowed me to check out the data in better detail in Google Sheets. I did a few things in Google Sheets since it was easier:\n\n-   Combined the two twitter csv files (One for #loveisblindjapan and another for the phrase \"love is blind japan\") and removed any duplicates between them with the \"remove duplicate\" function.\n\n-   I noticed the reddit csv file had time categorized as \"utc\" which stands for coordinate universal time. This gave me numbers such as \"1643382213\" which is fairly unreadable to me. Thus I used this formula to fix it: =X2/86400+DATE(1970,1,1)+time(5,30,0). This allowed me to have 1/28/2022 20:33:33 which is easier to understand. However to match the twitter csv file (done in year/month/day (YMD)) I used removed the time from the end and formatted it using Google Sheet's \"custom date and time\" format to end up with 2022-01-28.\n\n-   Since the twitter csv file had YMD and then time I split the column so it only had YMD.\n\n-   I ended up merging the files together (This included a count of comments from people, the username of the person, and the actually tweet or post). I made an extra column that would say if it was from Twitter or Reddit.\n\n## Data Quality\n\nWhile almost all Reddit posts were made in English, I noticed there were quite a few tweets that were partially or completely in a different language. This has lead to me debating on if I should just remove the non English tweets entirely or leave them in.\n\nI also noticed there were more tweets that had spelling errors than on Reddit posts. This is likely due to being unable to edit tweets, however this may cause a problem. Additionally tweets were more likely to use slang than Reddit.\n\nFrom a quick glance I also noticed that tweets were often writing about how the show made them feel rather than about the contestants on the show. This may lead me to change my research question or decide to use only Reddit posts. Reddit posts seemed to focus on the contestants more often.\n\nFor the Reddit posts I also noticed that unfortunately the data does not seem to tell me how to now if people are replying to another comment on the post. Some of the posts will start with \"I know what you mean!\" This could lead to less examples of contestants names being shown, which could make my research question difficult.\n\n## Updated Research Question\n\n**Previously my research question was:** Do Reddit and Twitter differentiate on their views of contestants and their relationships in *Love is Blind Japan*?\n\n**My current research question I'm leaning towards is:** How do Reddit and Twitter users feel about the show *Love is Blind Japan*?\n\n**Why I'm considering the change:** It seems that although the contestants are important, if I want to focus on purely how viewers felt about the contestants I would need to only use Reddit posts. Additionally I will be analyzing the positive and negative sentiments of Reddit and Twitter together.\n\n## Bringing in the Data\n\nIn order to check the data I've added my csv file to my repository. I will first check that it was added correctly.\n\nI use the \"here\" package because it allows you to bypass the issue of setwd(), allowing you to change your working directory file. **A relative path to the project root directory will always be created using here().**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#corpus <- read.csv(here(\"posts\",\"loveisblind_socialmedia.csv\"))\ncorpus <- read_csv(here(\"_data/loveisblind_socialmedia.csv\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'C:/Users/srika/OneDrive/Desktop/Text_as_Data_Fall_2022/_data/loveisblind_socialmedia.csv' does not exist.\n```\n:::\n\n```{.r .cell-code}\nhead(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(corpus): object 'corpus' not found\n```\n:::\n:::\n\n\nHere we can see the data loaded in correctly and all three of the columns I wanted!\n\n## Attempting to Clean the Data (a bit)\n\nWhile the data is in the correct columns, I would still like to try a bit of cleaning to see if we can remove some items. The first thing I will do is remove non english from all of my posts. This is due to me being unable to analyze other languages correctly.\n\n### Remove Languages\n\nThe first thing I thought of was removing Japanese as the show was in Japan, so I found this [answer](https://stackoverflow.com/questions/60181121/how-do-i-remove-japanese-characters).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_rm_jap = function(x) {\n  #we replace japanese blocks with nothing, and clean any double whitespace from this\n  #reference at http://www.rikai.com/library/kanjitables/kanji_codes.unicode.shtml\n  x %>% \n    #japanese style punctuation\n    str_replace_all(\"[\\u3000-\\u303F]\", \"\") %>% \n    #katakana\n    str_replace_all(\"[\\u30A0-\\u30FF]\", \"\") %>% \n    #hiragana\n    str_replace_all(\"[\\u3040-\\u309F]\", \"\") %>% \n    #kanji\n    str_replace_all(\"[\\u4E00-\\u9FAF]\", \"\") %>% \n    #remove excess whitespace\n    str_replace_all(\"  +\", \" \") %>% \n    str_trim()\n}\n\ncorpus_posts <- corpus$text %>% str_rm_jap\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'corpus' not found\n```\n:::\n:::\n\n\nHowever I realized there were many more languages. This made it a bit more difficult. So I decided to keep looking and found this [answer](https://stackoverflow.com/questions/49338549/remove-languages-other-than-english-from-corpus-or-data-frame-in-r).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"cld3\")\ncorpus2 <- subset(corpus, detect_language(corpus$text) == \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in subset(corpus, detect_language(corpus$text) == \"en\"): object 'corpus' not found\n```\n:::\n:::\n\n\nThis seemed to work well! It may not be the perfect solution but it seems to have removed any tweets or posts that were not in English.\n\n### Check Package TextClean\n\nThe next package I'll use for that is \"textclean\".\n\nI'll first check any posts or tweets (henceforth known as posts) using the check_text() function.\n\nThis takes quite awhile (I didn't actually time it but I had enough time to watch a ton of Youtube clips!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_text(corpus2$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'corpus2' not found\n```\n:::\n:::\n\n\nWe're able to see here there's multiple issues with the text that I pulled. What I like about this package is it also gives options to fix these items too. The first thing I'll try is to replace internet slang function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_posts <- replace_internet_slang(corpus2$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus2' not found\n```\n:::\n\n```{.r .cell-code}\nhead(corpus_posts, 20)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(corpus_posts, 20): object 'corpus_posts' not found\n```\n:::\n:::\n\n\nThis has worked well. It has changed slang words like \"ppl\" to \"people\"! This makes me quite happy.\n\nI'll go ahead and do \"replace_date\", \"replace_kern\" (to adjust spacing that was done manually such as writing \"A M A Z I N G\" as \"AMAZING\"), \"replace_curly_quotes\", \"replace_word_elongation\" (If someone writes \"woooah\" it'll change it to \"woah\") and \"replace_contraction\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_posts <- replace_date(corpus_posts) %>% \n  replace_contraction(corpus_posts) %>% \n  replace_kern(corpus_posts) %>% \n  replace_curly_quote(corpus_posts) %>% \n  replace_word_elongation(corpus_posts) %>% \n  replace_white(corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in isTRUE(impart.meaning): object 'corpus_posts' not found\n```\n:::\n:::\n\n\n### Removing Emojis\n\nI also want to remove emojis. To do this I found in the DACSS slack channel someone who was looking for similar information and was given an answer! Below you will see the emojis removed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\ncorpus_posts <- corpus_posts %>% \n  str_replace_all(regex(only_ascii_regexp), \"\") \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\nhead(corpus_posts, 20)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(corpus_posts, 20): object 'corpus_posts' not found\n```\n:::\n:::\n\n\n### Remove HTML links\n\nThere are a few html links that I believe mostly lead to Youtube clips of the shows or gifs. I would like to remove those as they don't add to my analysis. I followed [this](https://stackoverflow.com/questions/25352448/remove-urls-from-string) answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_posts <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n:::\n\n\n### Another Look at Check_Text\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_text(corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'corpus_posts' not found\n```\n:::\n:::\n\n\nHere we can see there's still a lot of problems. I'll attempt to fix some of them that I notice right away that seem easy.\n\n### A Bit More Cleaning\n\nHere I'm cleaning some issues that don't seem to work through the \"textclean\" replace packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_posts <- mgsub(corpus_posts, c(\"it's\", \"i'm\", \"i've\", \"she's\", \"he's\", \"don't\", \"isn't\", \"didn't\", \"they'll\", \"can't\", \"they're\", \"you're\", \"EP02Iv'e\"), c(\"it is\", \"i am\", \"i have\", \"she is\", \"he is\", \"does not\", \"is not\", \"did not\", \"they will\", \"cannot\", \"they are\", \"you are\", \"it is\", \"i have\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- mgsub(corpus_posts, c(\"1/11/22\", \"23\"), c(\"january eleventh two thousand twenty two\", \"twenty three\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- corpus_posts %>% textshape::split_sentence(corpus_posts, text.var = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textshape::split_sentence(., corpus_posts, text.var = TRUE): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\nhead(corpus_posts, 10) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(corpus_posts, 10): object 'corpus_posts' not found\n```\n:::\n:::\n\n\n### Testing Reverting to Data Frame\n\nThis a test area to see if this will allow me to put my data back from values to data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corpus)\nlibrary(textshape)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'textshape'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tibble':\n\n    column_to_rownames\n```\n:::\n\n```{.r .cell-code}\nlibrary(cleanNLP)\nlibrary(NLP)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\ncorpus_postse <- as_corpus_text(corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as_corpus_text(corpus_posts): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_postse <- tidy_list(corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x[[1]]): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_postse <- Token_Tokenizer(corpus_postse) # become tokens\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in Token_Tokenizer(corpus_postse): object 'corpus_postse' not found\n```\n:::\n:::\n\n\nIt worked somewhat! It isn't a true data table but it is back under the data category.\n\n## General Notes and Future Thoughts\n\n-   The columns I had originally in the table, date and if the post was a tweet or Reddit post are missing. This isn't a problem currently, but if I would like those back I'll have to join the edited post to the table.\n\n    -   Another option is to just upload two separate csv files with one containing tweets and another containing posts.\n\n-   The textclean package has been super useful! It's helped to make a lot of the cleaning fairly easily. However it has left /n around for a few items. I'm unsure why, (as I believe it happened removing other languages) so I'll have to remove it later.\n\n    -   Although it has been very helpful it seems to be unable to clean up everything. That's alright but a bit confusing as to why.\n\n-   I noticed that some posts are now \"NA\" I'd like to remove those from my database by the next blog post.\n\n-   I would also like to change all text to lower case.\n\n-   I may want to use the \"tm\" package to clean up my data even more. Currently I feel like a lot of the harder things have been taken out.\n\n-   I'm a bit confused on the creating the corpus as token with the \"textshape\" package. It seems to work but I'm unsure of how it's working.\n\n## Looking Ahead at Tutorial 5\n\nAfter taking a brief look at tutorial 5 there does seem to be some very useful tips in there. Here is currently what I have. I may be able to combine some of what I was looking at and what lesson 5 has together!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: usethis\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(plyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:here':\n\n    here\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    compact\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n\ncorpustest <- corpus(corpus$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus$text: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\ncorpussummary <- summary(corpustest) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(corpustest): object 'corpustest' not found\n```\n:::\n\n```{.r .cell-code}\ncorpussummary$show <- \"Love is Blind Japan\"\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpussummary$show <- \"Love is Blind Japan\": object 'corpussummary' not found\n```\n:::\n\n```{.r .cell-code}\ncorpussummary$count <- as.numeric(str_extract(corpussummary, \"[0-9]+\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_extract_first_regex(string, pattern, opts_regex = opts(pattern)): object 'corpussummary' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_tokens <- tokens(corpustest, \n    remove_punct = T,\n    remove_numbers = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(corpustest, remove_punct = T, remove_numbers = T): object 'corpustest' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_tokens <- tokens_tolower(corpus_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(corpus_tokens): object 'corpus_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_tokens <- tokens_select(corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(corpus_tokens, pattern = stopwords(\"en\"), selection = \"remove\"): object 'corpus_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_tokens_stem <- tokens_wordstem(corpus_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_wordstem(corpus_tokens): object 'corpus_tokens' not found\n```\n:::\n\n```{.r .cell-code}\nprint(corpus_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'print': object 'corpus_tokens' not found\n```\n:::\n:::\n\n\n## Full Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cld3)\nlibrary(dplyr)\nlibrary(textclean)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(textshape)\nlibrary(here)\n\ncorpus <- read.csv(here(\"posts\",\"loveisblind_socialmedia.csv\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in here(\"posts\", \"loveisblind_socialmedia.csv\"): unused argument (\"loveisblind_socialmedia.csv\")\n```\n:::\n\n```{.r .cell-code}\nhead(corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         \n1 function (x, ...)      \n2 {                      \n3     UseMethod(\"corpus\")\n4 }                      \n```\n:::\n\n```{.r .cell-code}\ncorpus2 <- subset(corpus, detect_language(corpus$text) == \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus$text: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- replace_internet_slang(corpus2$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus2' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- replace_date(corpus_posts) %>% \n  replace_contraction(corpus_posts) %>% \n  replace_kern(corpus_posts) %>% \n  replace_curly_quote(corpus_posts) %>% \n  replace_word_elongation(corpus_posts) %>% \n  replace_white(corpus_posts) %>% \n  replace_html(corpus_posts) %>% \n  textshape::split_sentence(corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in isTRUE(symbol): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\nonly_ascii_regexp <- '[^\\u0001-\\u007F]+|<U\\\\+\\\\w+>'\ncorpus_posts <- corpus_posts %>% \n  str_replace_all(regex(only_ascii_regexp), \"\") \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- gsub(\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", corpus_posts)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- mgsub(corpus_posts, c(\"it's\", \"i'm\", \"i've\", \"she's\", \"he's\", \"don't\", \"isn't\", \"didn't\", \"they'll\", \"can't\", \"they're\", \"you're\", \"EP02Iv'e\"), c(\"it is\", \"i am\", \"i have\", \"she is\", \"he is\", \"does not\", \"is not\", \"did not\", \"they will\", \"cannot\", \"they are\", \"you are\", \"it is\", \"i have\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_posts <- mgsub(corpus_posts, c(\"1/11/22\", \"23\"), c(\"january eleventh two thousand twenty two\", \"twenty three\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'corpus_posts' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_postse <- corpus_postse %>% textshape::split_sentence(corpus_postse) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textshape::split_sentence(., corpus_postse): object 'corpus_postse' not found\n```\n:::\n\n```{.r .cell-code}\nhead(corpus_posts, 10) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(corpus_posts, 10): object 'corpus_posts' not found\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}