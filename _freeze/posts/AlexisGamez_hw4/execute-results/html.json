{
  "hash": "370ab43f6fa7953943a0d4345d1c785a",
  "result": {
    "markdown": "---\ntitle: \"Blog Post #4: Word Embedding & Dictionaries\"\nauthor: \"Alexis Gamez\"\ndesription: \"Studying text-as-data as it relates to eating bugs\"\ndate: \"11/20/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Alexis Gamez \n  - blogpost4\n  - Word Embedding\n  - Dictionary Analysis\n---\n\n# Setup\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)\nlibrary(plyr)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(readr)\nlibrary(devtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: usethis\n```\n:::\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rvest' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nlibrary(rtweet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n:::\n\n```{.r .cell-code}\nlibrary(twitteR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'twitteR'\n\nThe following object is masked from 'package:rtweet':\n\n    lookup_statuses\n\nThe following objects are masked from 'package:dplyr':\n\n    id, location\n\nThe following object is masked from 'package:plyr':\n\n    id\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:data.table':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nAttaching package: 'quanteda'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'quanteda.textstats' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(text2vec)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'text2vec' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(devtools)\n# devtools::install_github(\"kbenoit/quanteda.dictionaries\")\nlibrary(quanteda.dictionaries)\n# remotes::install_github(\"quanteda/quanteda.sentiment\")\nlibrary(quanteda.sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda.sentiment'\n\nThe following object is masked from 'package:quanteda':\n\n    data_dictionary_LSD2015\n```\n:::\n:::\n\n</details>\n\n# **Data Source**\n\nAs with previous blog posts, I will be continuing to use the corpus I've built from pulling tweets from the social media platform Twitter. All tweets have been pulled with relevance to the key words and phrases of `eating bugs` and `eating insects`. The CSV file we will eventually read in is a compilation of such tweets extracted between the dates of November 3rd and November 13th. \n\n# **Goals**\n\nFor this post, I'm going to be exploring the use of word embedding, & dictionary methods within my corpus. I will exclusively be utilizing the techniques found within tutorials 7 & 8. As to whether each technique is effectively useful to our analysis is to be determined by our experimentation. \n\n# **Word Embeddings**\n\n## *Pre-Analysis*\n\nI start off by reading in my corpus as the `bug_tweets` object. Then, we'll work to tokenize and vectorize the corpus. We'll be using the `text2vec` package heavily in this section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we're going to read in our existing corpus, calling to the csv file we created in blog post #2\nbug_tweets <- read.csv(\"eating_bugs_tweets_11_13_22.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in file(file, \"rt\"): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nhead(bug_tweets, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_tweets, 10): object 'bug_tweets' not found\n```\n:::\n\n```{.r .cell-code}\ndim(bug_tweets)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_tweets' not found\n```\n:::\n:::\n\n\n### Tokenizing & Vectorizing\n\nNow that we've read in our `bug_tweets` object, we'll be using `word_tokenizer` to tokenize our documents into a new object, `bug_tokens`.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenizing the corpus\nbug_tokens <- word_tokenizer(bug_tweets)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stringi::stri_split_boundaries(strings, type = \"word\", skip_word_none = TRUE): object 'bug_tweets' not found\n```\n:::\n\n```{.r .cell-code}\nhead(bug_tokens, 5)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_tokens, 5): object 'bug_tokens' not found\n```\n:::\n:::\n\n</details>\n\nWith the token object created, we can move to creating an iterator object and begin building the vocabulary we're going to be using in this section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an iterator\nit <- itoken(bug_tokens, progressbar = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in itoken(bug_tokens, progressbar = FALSE): object 'bug_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n# Then we're going to build the vocabulary\nvocab <- create_vocabulary(it)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in create_vocabulary(it): object 'it' not found\n```\n:::\n\n```{.r .cell-code}\n# Calling the vocab object\nvocab\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'vocab' not found\n```\n:::\n\n```{.r .cell-code}\n# Calling for the dimensions of our vocabulary object\ndim(vocab)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'vocab' not found\n```\n:::\n:::\n\n\nWith the `vocab` object created, we can now prune and vectorize it. While vectorizing is pretty self explanatory (coercing our vocab object into a vector), pruning simply trims down our object and removes words that aren't mentioned above a certain number of times. In this case, that threshold is a minimum of 5 times. All tokens not mentioned at least 5 times will be dropped from our object.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we're going to prune the vocabulary\nvocab <- prune_vocabulary(vocab, term_count_min = 5)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in prune_vocabulary(vocab, term_count_min = 5): object 'vocab' not found\n```\n:::\n\n```{.r .cell-code}\n# Checking the dimensions of the vocab list again shows how much we were able to cut down the original list\ndim(vocab)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'vocab' not found\n```\n:::\n\n```{.r .cell-code}\n# Now we're going to vectorize our vocab\nvectorizer <- vocab_vectorizer(vocab)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in force(vocabulary): object 'vocab' not found\n```\n:::\n:::\n\n</details>\n\nFinally, we're going to create a term co-occurrence matrix. We're going to be sticking to a skip gram window of 5 considering we don't have a massive corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating a term co-occurrence matrix\ntcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in create_tcm(it, vectorizer, skip_grams_window = 5L): object 'it' not found\n```\n:::\n:::\n\n\n### Fitting out a GloVe Model\n\nWe have a Term Co-Occurrence Matrix! With our TCM created, let's move to creating a GloVe model and fitting it to objectively fit our analysis.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating GloVe model\nbug_glove <- GlobalVectors$new(rank = 50, x_max = 10)\nbug_glove\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<GloVe>\n  Public:\n    bias_i: NULL\n    bias_j: NULL\n    clone: function (deep = FALSE) \n    components: NULL\n    fit_transform: function (x, n_iter = 10L, convergence_tol = -1, n_threads = getOption(\"rsparse_omp_threads\", \n    get_history: function () \n    initialize: function (rank, x_max, learning_rate = 0.15, alpha = 0.75, lambda = 0, \n    shuffle: FALSE\n  Private:\n    alpha: 0.75\n    b_i: NULL\n    b_j: NULL\n    cost_history: \n    fitted: FALSE\n    glove_fitter: NULL\n    initial: NULL\n    lambda: 0\n    learning_rate: 0.15\n    rank: 50\n    w_i: NULL\n    w_j: NULL\n    x_max: 10\n```\n:::\n\n```{.r .cell-code}\n# Creating the fitting model here\nbug_main <- bug_glove$fit_transform(tcm, n_iter= 10, \n                                convergence_tol = 0.01,\n                                n_threads = 8)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in .class1(object): object 'tcm' not found\n```\n:::\n\n```{.r .cell-code}\ndim(bug_main)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_main' not found\n```\n:::\n:::\n\n</details>\n\nWe've now created our `target` word vector. As stated in tutorial 7, the `target` word vector can be thought of as the words of interest we seek within our corpus, while all other words would be considered to be a part of the `context` word vector. We'll be creating the `context` vector now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since we've created the `main` target vector, we're going to create a context vector now\nbug_context <- bug_glove$components\ndim(bug_context)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nWith the `target` and `context` vectors made, we can create a word vector matrix by taking the sum of both.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating vector matrix\nbug_vectors <- bug_main + t(bug_context)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_main' not found\n```\n:::\n\n```{.r .cell-code}\nbug_vectors\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n:::\n\n</details>\n\nNow that the word vector matrix has been created, we're free to begin our analysis using Cosine Similarity.\n\n## *Cosine Similarity*\n\nThe logic behind Cosine Similarity looks to find the correlation between two vectors (i.e. our `target` and `context` vectors).The functions below are fed our word vector matrix and a token of our selection in an attempt to find any correlation with said token and others within our corpus. I decided to start off by testing the word \"cricket\" as it seemed the most direct (aside from \"eating\", \"bugs\" and \"insects\") while maintaining an air of objectivity.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# We're going to test out Cosine Similarity now by running the following functions.\ncricket <- bug_vectors[\"Cricket\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\ncricket_cos_sim <- sim2(x = bug_vectors, y = cricket, \n                       method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\ncricket_cos_sim\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'cricket_cos_sim' not found\n```\n:::\n:::\n\n</details>\n\nWe can see some interesting results here, mostly that the most similar words (other than the word \"cricket\" itself), according to cosine similarity, are an assortment of seemingly negative ones. I say seemingly here because, without proper context, there's no way of definitively understanding the intended sentiment behind it. With that said, we can still take some educated guessed. In this case, we see objectively negative words such as \"nope\", \"crap\", \"mandatory\", \"nobody\", and \"bother\" near the top. Let's run a couple more tests and see what else pops up.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nright_wing <- bug_vectors[\"POTUS\", , drop = FALSE] -\n     bug_vectors[\"liberals\", , drop = FALSE] +\n     bug_vectors[\"Conservative\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nright_wing_cos_sim = sim2(x = bug_vectors, y = right_wing, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nright_wing_cos_sim\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'right_wing_cos_sim' not found\n```\n:::\n\n```{.r .cell-code}\nconservative <- bug_vectors[\"POTUS\", , drop = FALSE] -\n     bug_vectors[\"JoeBiden\", , drop = FALSE] +\n     bug_vectors[\"Trump\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nconservative_cos_sim = sim2(x = bug_vectors, y = conservative, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nconservative_cos_sim\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'conservative_cos_sim' not found\n```\n:::\n\n```{.r .cell-code}\nliberal <- bug_vectors[\"POTUS\", , drop = FALSE] -\n     bug_vectors[\"Trump\", , drop = FALSE] +\n     bug_vectors[\"JoeBiden\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nliberal_cos_sim = sim2(x = bug_vectors, y = liberal, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\nliberal_cos_sim\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'liberal_cos_sim' not found\n```\n:::\n\n```{.r .cell-code}\neating_bugs <- bug_vectors[\"eating\", , drop = FALSE] -\n     bug_vectors[\"meat\", , drop = FALSE] +\n     bug_vectors[\"insects\", , drop = FALSE] +\n     bug_vectors[\"bugs\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\neating_bugs_cos_sim = sim2(x = bug_vectors, y = eating_bugs, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\neating_bugs_cos_sim\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'eating_bugs_cos_sim' not found\n```\n:::\n:::\n\n</details>\n\nThere are some interesting results here, primarily those cosine similarities that are politically oriented. We see in the results for the last `eating_bugs` chunk of code that there's a bit of obstruction per many stop words that our pruning didn't seem to eliminate. Either that or the code we did write needs a little work. Either way, we can effectively see some sentiment behind the previous code and key words, let's see if we can rework the last bit of coding just a bit before we move on from cosine similarities.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\neating_insects <- bug_vectors[\"eating\", , drop = FALSE] -\n     bug_vectors[\"meat\", , drop = FALSE] +\n     bug_vectors[\"insects\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\neating_insects_cos_sim = sim2(x = bug_vectors, y = eating_bugs, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\neating_bugs <- bug_vectors[\"eating\", , drop = FALSE] -\n     bug_vectors[\"meat\", , drop = FALSE] +\n     bug_vectors[\"bugs\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_vectors' not found\n```\n:::\n\n```{.r .cell-code}\neating_bugs_cos_sim = sim2(x = bug_vectors, y = eating_bugs, method = \"cosine\", norm = \"l2\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopifnot(inherits(x, \"matrix\") || inherits(x, \"Matrix\")): object 'bug_vectors' not found\n```\n:::\n:::\n\n</details>\n\nWe still see the same skewing of results, even after simplifying our code. What we'll have to decide from here, is whether it's worth returning to the pre-processing stage and filtering out stop words from our corpus or whether we should include word embedding in our analysis at all. For now, we'll move on from cosine similarity tests.\n\n# **Dictionaries**\n\nNext we're going to make use of quanteda's dictionary package. We're going to start by reading in our data set again to reset it's properties and convert it into a `corpus`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbug_tweets <- read.csv(\"eating_bugs_tweets_11_13_22.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in file(file, \"rt\"): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nbug_corpus <- corpus(bug_tweets$x)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(bug_tweets$x): object 'bug_tweets' not found\n```\n:::\n:::\n\n\n## *Dictionary Analysis*\n\nWith our corpus loaded in, we can begin taking a stab at different dictionary analysis methods. In this case, we're going to start by using the NRC dictionary which will attempt to calculate a percentage of the the documents within our corpus that reflect certain emotional characteristics. The NRC dictionary refers to the NRC Emotion Lexicon which associates words with certain emotions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbug_sentiment_nrc <- liwcalike(bug_corpus, data_dictionary_NRC)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in liwcalike(bug_corpus, data_dictionary_NRC): object 'bug_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# The function below provides us with the column names within our new `bug_sentiment_nrc` object\nnames(bug_sentiment_nrc)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_sentiment_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(bug_sentiment_nrc) +\n     geom_histogram(aes(x = positive), binwidth = 2) +\n     theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(bug_sentiment_nrc): object 'bug_sentiment_nrc' not found\n```\n:::\n:::\n\n\nWe see some interesting results here after plotting our data, specifically, a lack of positive documents within our corpus. Additionally, according to our analysis, a majority of the documents are appearing as neutral sitting at a 0 \"positive\" score. First, let's take a look at some positive texts to see what we're dealing with.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nbug_corpus[which(bug_sentiment_nrc$positive > 15)]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_corpus' not found\n```\n:::\n:::\n\n\nSomething we can immediately see is that, while there are some positive sentiments referring to our eating bugs corpus, our code can't seem to pick up on all nuances of twitter based dialect. Let's look at this selection more thoroughly. \n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ntable(bug_corpus[which(bug_sentiment_nrc$positive > 15)])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in table(bug_corpus[which(bug_sentiment_nrc$positive > 15)]): object 'bug_corpus' not found\n```\n:::\n:::\n\n</details>\n\nEven at a glance, I think is alright to assume that a majority of there \"positive\" reviews contain heavy sarcasm and satirical language. I think this is something we can remedy through the use of different dictionaries. For now, we're going to continue with the NRC dictionary and take a look at the most negative documents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bug_sentiment_nrc) +\n     geom_histogram(aes(x = negative), binwidth = 2) +\n     theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(bug_sentiment_nrc): object 'bug_sentiment_nrc' not found\n```\n:::\n:::\n\n\nRight off the bat, we see that there is a higher spread of negative values from our plot. While the positive results were more concentrated toward the lower values of positivity, the negative values are more diverse in negative values. Also I noticed that, for some reason, there are more documents valued as neutral in the negative plot than the positive one. Let's take a deeper look into the negative documents as we did with the positive ones.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ntable(bug_corpus[which(bug_sentiment_nrc$negative > 15)])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in table(bug_corpus[which(bug_sentiment_nrc$negative > 15)]): object 'bug_corpus' not found\n```\n:::\n:::\n\n</details>\n\nNoticeably, I can see that a majority of the documents presented from this function are indeed negative! There are still some language nuances that NRC can't seem to pick up on, but it's interesting to see the quantity and intensity of the negative documents within our corpus.\n\nHaving isolated both positive and negative results, let's try analyzing our corpus by incorporating both sides of the proverbial coin. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbug_sentiment_nrc$polarity <- bug_sentiment_nrc$positive - bug_sentiment_nrc$negative\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_sentiment_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(bug_sentiment_nrc) +\n     geom_histogram(aes(polarity), binwidth = 2) +\n     theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(bug_sentiment_nrc): object 'bug_sentiment_nrc' not found\n```\n:::\n:::\n\n\nIt seems that our previous observations are still consistent here. There isn't much else to take away other than the fact that there seems to be a higher concentration of positive documents than negative, which is unexpected. I suspect that the NRC dictionary we're using is heavily skewing the results. The fact that there are so many neutral valued documents further fortifies my suspicions.\n\n## *Dictionaries with DFMs*\n\nNext, we're going to be utilizing dictionary analysis methods that utilize DFMs as opposed to the corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here we are coercing our corpus into a dfm without using the NRC dictionary. \nbug_dfm <- tokens(bug_corpus,\n                         remove_punct = TRUE,\n                         remove_symbols = TRUE,\n                         remove_numbers = TRUE,\n                         remove_url = TRUE,\n                         split_hyphens = FALSE,\n                         include_docvars = TRUE) %>%\n                         tokens_tolower() %>%\n  dfm()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(bug_corpus, remove_punct = TRUE, remove_symbols = TRUE, : object 'bug_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# Now we'll coerce our corpus to a dfm using the NRC dictionary\nbug_dfm_nrc <- tokens(bug_corpus,\n                         remove_punct = TRUE,\n                         remove_symbols = TRUE,\n                         remove_numbers = TRUE,\n                         remove_url = TRUE,\n                         split_hyphens = FALSE,\n                         include_docvars = TRUE) %>%\n  tokens_tolower() %>% \n  dfm() %>% \n  dfm_lookup(data_dictionary_NRC)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(bug_corpus, remove_punct = TRUE, remove_symbols = TRUE, : object 'bug_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nhead(bug_dfm_nrc, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_dfm_nrc, 10): object 'bug_dfm_nrc' not found\n```\n:::\n\n```{.r .cell-code}\n# These functions can be run to provide some more details behind our new bug_dfm_nrc object\n# dim(bug_dfm_nrc)\n# class(bug_dfm_nrc)\n```\n:::\n\n\nCompared to the standard dfm (w/o dictionaries) we can see there's a bit more diversity in terms of polarity measures. Instead of counts for each token we analyze, we can categorize all tokens into different sentiments which may serve us some good. Next, we're going to be converting the dictionary dfm into a dataframe. From there, we can create a  polarity measure and attempt to visualize the data similarly to how we did prior to converting to dfm. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbug_df_nrc <- convert(bug_dfm_nrc, to = \"data.frame\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in convert(bug_dfm_nrc, to = \"data.frame\"): object 'bug_dfm_nrc' not found\n```\n:::\n\n```{.r .cell-code}\n# Again, these provide the sentiment categories to which we'll be arranging our data\nnames(bug_df_nrc)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nbug_df_nrc$polarity <- (bug_df_nrc$positive - bug_df_nrc$negative)/(bug_df_nrc$positive + bug_df_nrc$negative)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nbug_df_nrc$polarity[(bug_df_nrc$positive + bug_df_nrc$negative) == 0] <- 0\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in bug_df_nrc$polarity[(bug_df_nrc$positive + bug_df_nrc$negative) == : object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(bug_df_nrc) +\n  geom_histogram(aes(x=polarity), binwidth = .25) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(bug_df_nrc): object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\n# This function provides us with the most positive reviews ranked at a value of 1\nwriteLines(head(bug_corpus[which(bug_df_nrc$polarity == 1)]))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_corpus[which(bug_df_nrc$polarity == 1)]): object 'bug_corpus' not found\n```\n:::\n:::\n\n\nInterestingly, we receive very similar results to those we received when using dictionaries without coercing to dfm. While not surprised, our operations are still struggling to define what is positive and negative accurately according to our `writeLines` function.\n\n## *Using Different Dictionaries*\n\nSeeing as the NRC dictionary didn't exactly give us what we wanted to see, we're going to test drive a few other options we have. We'll start with the General Inquirer dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here we're re-converting our corpus to a dfm using the general inquirer dictionary\nbug_dfm_geninq <- bug_dfm %>%\n  dfm_lookup(data_dictionary_geninqposneg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_lookup(., data_dictionary_geninqposneg): object 'bug_dfm' not found\n```\n:::\n\n```{.r .cell-code}\nhead(bug_dfm_geninq, 6)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_dfm_geninq, 6): object 'bug_dfm_geninq' not found\n```\n:::\n:::\n\n\nWe can see here that the general inquirer dictionary, as opposed NRC, splits sentiments into only 2 categories. Positive and Negative. From the surface, it already looks like we're going to be getting a majority positive. Let's continue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create polarity measure for `geninq`\nbug_df_geninq <- convert(bug_dfm_geninq, to = \"data.frame\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in convert(bug_dfm_geninq, to = \"data.frame\"): object 'bug_dfm_geninq' not found\n```\n:::\n\n```{.r .cell-code}\nbug_df_geninq$polarity <- (bug_df_geninq$positive - bug_df_geninq$negative)/(bug_df_geninq$positive + bug_df_geninq$negative)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bug_df_geninq' not found\n```\n:::\n\n```{.r .cell-code}\nbug_df_geninq$polarity[which((bug_df_geninq$positive + bug_df_geninq$negative) == 0)] <- 0\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in bug_df_geninq$polarity[which((bug_df_geninq$positive + bug_df_geninq$negative) == : object 'bug_df_geninq' not found\n```\n:::\n\n```{.r .cell-code}\nhead(bug_df_geninq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(bug_df_geninq): object 'bug_df_geninq' not found\n```\n:::\n:::\n\n\nHere we can see a bit of the logic behind the polarity scaling. We can also see that tokens of opposite polarities nullify each other, making it so that the documents are rated as a neutral 0. Interesting, but I'm not sure if it's all that useful toward our analysis.\n\n<details>\n  <summary> View Code</summary>\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's create unique names for each data frame\ncolnames(bug_df_nrc) <- paste(\"nrc\", colnames(bug_df_nrc), sep = \"_\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\ncolnames(bug_df_geninq) <- paste(\"geninq\", colnames(bug_df_geninq), sep = \"_\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'bug_df_geninq' not found\n```\n:::\n\n```{.r .cell-code}\n# Now let's compare our estimates\nsent_df <- merge(bug_df_nrc, bug_df_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in merge(bug_df_nrc, bug_df_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\"): object 'bug_df_nrc' not found\n```\n:::\n\n```{.r .cell-code}\nhead(sent_df)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(sent_df): object 'sent_df' not found\n```\n:::\n\n```{.r .cell-code}\ncor(sent_df$nrc_polarity, sent_df$geninq_polarity)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(y): object 'sent_df' not found\n```\n:::\n:::\n\n</details>\n\nWith the functions used above, we've now successfully built a correlation model according to the results we received while using the General Inquirer and NRC dictionaries! Now, lets try and plot it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Now we'll plot them out\nggplot(sent_df, mapping = aes(x = nrc_polarity,\n                              y = geninq_polarity)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(sent_df, mapping = aes(x = nrc_polarity, y = geninq_polarity)): object 'sent_df' not found\n```\n:::\n:::\n\n\nWhile there was some correlation between both models, we can see from our visual that there is a clear distinction between them as well. The NRC dictionary seems to rank polarity in an extremely linear fashion while the General Inquirer rankings are much more fluid and varying. The variance between both show us that it's going to come down to what better serves our analysis. What other options could we benefit from? Next, we're going to experiment a bit with applying dictionaries within contexts.\n\n## *Dictionaries within Contexts*\n\nUsing contexts within dictionary analysis essentially let us prompt our functions with \"context vectors\" that provide the data with key words to use in its associations. We start by isolating the tokens we wish to use as context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize corpus\nbug_tokens <- tokens(bug_corpus, remove_punct = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(bug_corpus, remove_punct = TRUE): object 'bug_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# what are the context (target) words or phrases\nbug_words <- c(\"eating bugs\", \"eating insects\", \"bug\", \"bugs\", \"insect\", \"insects\")\n\n# retain only our tokens and their context\ntokens_bugs <- tokens_keep(bug_tokens, pattern = phrase(bug_words), window = 40)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"keep\"): object 'bug_tokens' not found\n```\n:::\n:::\n\n\nNext, within those token sets, we can pull out the positive and negative dictionaries to get an inside look at what we're working with. In this case we'll be using the Lexicoder Sentiment Dictionary or `LSD` as it's denoted in the functions below. Once we've done that, we'll coerce our token object into a DFM.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]\n\ntokens_bugs_lsd <- tokens_lookup(tokens_bugs,\n                                dictionary = data_dictionary_LSD2015_pos_neg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_lookup(tokens_bugs, dictionary = data_dictionary_LSD2015_pos_neg): object 'tokens_bugs' not found\n```\n:::\n\n```{.r .cell-code}\ndfm_bugs <- dfm(tokens_bugs_lsd)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(tokens_bugs_lsd): object 'tokens_bugs_lsd' not found\n```\n:::\n\n```{.r .cell-code}\nhead(dfm_bugs, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(dfm_bugs, 10): object 'dfm_bugs' not found\n```\n:::\n:::\n\n\nFinally, we'll use the objects we've created thus far to; create a data frame, drop any features that contain only 0 values (have neither negative nor positive tokens within the document), print a summary sentence to tell us exactly how many tweets mention positive or negative tokens in the context of eating bugs, and finally create & plot the resulting polarity scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert to data frame\nmat_bugs <- convert(dfm_bugs, to = \"data.frame\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in convert(dfm_bugs, to = \"data.frame\"): object 'dfm_bugs' not found\n```\n:::\n\n```{.r .cell-code}\n# drop if both features are 0\nmat_bugs <- mat_bugs[-which((mat_bugs$negative + mat_bugs$positive)==0),]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'mat_bugs' not found\n```\n:::\n\n```{.r .cell-code}\n# print a little summary info\npaste(\"We have \",nrow(mat_bugs),\" tweets that mention positive or negative words in the context of eating bugs or insects.\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(mat_bugs): object 'mat_bugs' not found\n```\n:::\n\n```{.r .cell-code}\n# create polarity scores\nmat_bugs$polarity <- (mat_bugs$positive - mat_bugs$negative)/(mat_bugs$positive + mat_bugs$negative)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'mat_bugs' not found\n```\n:::\n\n```{.r .cell-code}\n# summary\nsummary(mat_bugs$polarity)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(mat_bugs$polarity): object 'mat_bugs' not found\n```\n:::\n\n```{.r .cell-code}\n# plot\nggplot(mat_bugs) + \n     geom_histogram(aes(x=polarity), binwidth = .25) + \n     theme_bw()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(mat_bugs): object 'mat_bugs' not found\n```\n:::\n:::\n\n\nLook at those results! Out of all the data we've pulled and created thus far, I believe the Lexicoder Sentiment Dictionary (`LSD`) has provided us with the most accurate results. If we were to use the `view(mat_bugs)` function, we could see the breakdown of the polarity scores and how each individual document received it's ranking. The one problem I still notice is the relevance of certain tweets in the context of humans eating bugs, but I believe our pre-processing work assists with filtering those down enough so that they don't skew our final results. With that, we'll close out our post for today. Thank you for reading! :)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}