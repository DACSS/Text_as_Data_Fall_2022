{
  "hash": "a81ca347b324dd09a5d20d4782a52a21",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3\"\nauthor: \"Nayan Jani\"\ndescription: \"Summary Stats\"\ndate: \"10/25/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Blog3\n  - Nayan Jani\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## My Data \n\nUsing Youtube API, I was able to extract comments from a video titles \"The Disgraces of World Cup Qatar 2022\". The Video is from Nov 13, 2021 and has 421,163 views and 1,862 comments. The reason that I chose this video is because it is the most interacted video on  youtube that talks about some of the controversies that surround this years World Cup I Qatar. I realize that this video is not current but my goal does not include comparison of language and thoughts over a period of time. The Qatar World cup has been a controversy for years and the discussion leading up to it has not changed because of the possible human rights violations. The comments I have scraped are the top 100 mostv relevant comments and the top 100 most recent commmentts. This only leaves me with 200 comments to deal with. For now I will only keep it at 200 so that I can figure out my plan fro preprocessing but for my next blog post I will scrape more videos that discuss the controversies of the World Cup.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_q<- read_csv(\"_data/comments_q.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\n\ndf_qRev<- read_csv(\"_data/comments_q.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_qRev<- df_qRev %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_qFull <- rbind(df_q,df_qRev)\n\ndf_qFull_new <- df_qFull %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_qFull_new <- df_qFull_new %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n  \n  \n  \n\ncorpus_qFull <- corpus(df_qFull_new)\n\ncorpus_sum <- summary(corpus_qFull)\n\n\n\n\nqatar_tokens <- tokens(corpus_qFull,\n    remove_numbers = T,\n    remove_punct = T,\n    remove_symbols = T)\n```\n:::\n\n\n## Key Questions\n\nMy Goal is to:\n\n  - Find the overall Sentiment of the comments (Positive and Negative, Other emotions)\n  \n  - What is the main focus in the comments (what topic is most important to the people in the comments)\n  \n  - Based on the most important topics and the sentiment of those comments, are those comments classified correctly as     \n    positive or negative? If yes, are those comments \"socially correct\"? (logical/acceptable POV vs Stereotyped/Stigmatized \n    POV)\n  \n\n## DFM and DFM_TFIDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqatar_tokens <-tokens_tolower(qatar_tokens)\nqatar_tokens <- tokens_select(qatar_tokens, \n                              pattern = c(stopwords(\"en\"),\"[a-z]\",\"t\",\"m\",\"=\",\"ve\",\"don\",\"s\"),\n                              selection = \"remove\")\n\n\n\n\nqatar_dfm <- dfm(qatar_tokens)\n\n\n\ntopfeatures(qatar_dfm,100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       qatar        world          cup         like      country         quot \n          60           58           44           38           32           28 \n   countries      workers         just         fifa          can           go \n          24           24           24           24           22           18 \n        know        lgbtq         laws     football    something         hate \n          18           16           16           16           16           16 \n        work       people         make          one      slavery        video \n          16           16           14           14           14           14 \n        west     stadiums        think        human       really      western \n          14           12           12           12           12           12 \n         get       middle         even          god   government         well \n          12           12           12           12           12           10 \n    religion       rights        money        thing         want         said \n          10           10           10           10           10           10 \n      hosted         made         arab         east          say         time \n          10           10           10           10           10           10 \n        also         life         href      support         used      someone \n          10           10           10           10            8            8 \n       going         love   corruption      corrupt       russia           us \n           8            8            8            8            8            8 \n        back        watch         died       worker         much         true \n           8            8            8            8            8            8 \n       still       humans      migrant        saudi       forgot      stadium \n           8            8            8            8            6            6 \n        give          gay        place      respect           wc       deaths \n           6            6            6            6            6            6 \n    anything        arabs     watching         huge construction     building \n           6            6            6            6            6            6 \n        care        cause          etc        slave       labour      believe \n           6            6            6            6            6            6 \n        good         home       making         lgbt        islam         stop \n           6            6            6            6            6            6 \n        hope         talk       though         lmao \n           6            6            6            6 \n```\n:::\n\n```{.r .cell-code}\nqatar_dfm_tfidf <- dfm_tfidf(qatar_dfm)\n\ntopfeatures(qatar_dfm_tfidf,55)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      quot      world      qatar       like        cup    country       work \n  36.30663   35.69459   34.83971   31.14267   30.56263   27.18423   24.29622 \ngovernment    workers       fifa  countries       just        can       laws \n  23.94762   23.89524   22.90182   21.99489   21.99489   21.90397   20.74664 \n      know         go      lgbtq   football  something       hate     people \n  20.70967   19.66581   19.47974   19.47974   18.40859   18.40859   18.40859 \n   slavery      video       west       make        one     really       even \n  18.15331   18.15331   18.15331   17.04478   17.04478   15.55998   15.55998 \n       god      think      human    western        get   religion       life \n  15.55998   15.55998   15.55998   15.55998   15.55998   15.18514   15.18514 \n      href   stadiums     middle     rights       said     hosted       made \n  15.18514   14.60981   14.60981   13.93575   13.93575   13.93575   13.93575 \n      time      still       well      money      thing       want       arab \n  13.93575   13.55684   12.96665   12.96665   12.96665   12.96665   12.96665 \n      east        say       also    support       true     humans \n  12.96665   12.96665   12.96665   12.96665   12.14811   12.14811 \n```\n:::\n:::\n\n\nThe words that stand out to me the most from the TFIDF are workers, rights, human, good, great, migrant, laws and hate an corruption. All of these words fall within the top 55 features of the TFIDF. Based off these findings within a small scope, I can infer that one major topic in these comments is the discussion of have migrant workers have been treated in the build up to the Qatar world Cup. \n\n## word Cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(qatar_dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 6 documents, 912 features (98.37% sparse) and 0 docvars.\n       features\ndocs    bullshit lgbtq rule perfect homophobic start everywhere disgrace forgot\n  text1        1     1    1       1          1     1          1        0      0\n  text2        0     0    0       0          0     0          0        1      1\n  text3        0     0    0       0          0     0          0        0      0\n  text4        0     0    0       0          0     0          0        0      0\n  text5        0     0    0       0          0     0          0        0      0\n  text6        0     0    0       0          0     0          0        0      0\n       features\ndocs    microsoft\n  text1         0\n  text2         1\n  text3         0\n  text4         0\n  text5         0\n  text6         0\n[ reached max_nfeat ... 902 more features ]\n```\n:::\n\n```{.r .cell-code}\nset.seed(1245)\n\ntextplot_wordcloud(qatar_dfm, min_count = 10, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThis show a visual of the frequency of the terms in the dfm.\n\n## Zipf's Law\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_counts <- as.data.frame(sort(colSums(qatar_dfm),dec=T))\ncolnames(word_counts) <- c(\"Frequency\")\nword_counts$Rank <- c(1:ncol(qatar_dfm))\nhead(word_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Frequency Rank\nqatar          60    1\nworld          58    2\ncup            44    3\nlike           38    4\ncountry        32    5\nquot           28    6\n```\n:::\n\n```{.r .cell-code}\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooking at my distribution of words it seems like 8 terms in mt DFM appear way more frequently than the rest of the terms. Byadding more data I belive this will smooth out.\n\n# Feature Co-occurrence matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq_fcm <- fcm(qatar_dfm)\n\n# keep only top features.\nsmall_fcm <- fcm_select(q_fcm, pattern = names(topfeatures(q_fcm, 50)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_fcm))\n\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe features that pop out the most from the Feature Co-occurrence matrix are conditions, workers and pay due to the size of their point and location within the Semantic Network. The grouping of words job,pay,companies,workers,system and conditions suggests that there is a thematic similarity between them. This could imply a topic that includes these words.\n\n\n## Next Post\n\nFor my next post I will add more data to my corpus and begin dictionary analysis.\n",
    "supporting": [
      "BlogPost3_NayanJani_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}