{
  "hash": "c4cb8d9f71e9b13ea33bbd4669fd8a16",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3\"\nauthor: \"Nayan Jani\"\ndescription: \"Summary Stats\"\ndate: \"10/25/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Blog3\n  - Nayan Jani\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## My Data \n\nUsing Youtube API, I was able to extract comments from a video titles \"The Disgraces of World Cup Qatar 2022\". The Video is from Nov 13, 2021 and has 421,163 views and 1,862 comments. The reason that I chose this video is because it is the most interacted video on  youtube that talks about some of the controversies that surround this years World Cup I Qatar. I realize that this video is not current but my goal does not include comparison of language and thoughts over a period of time. The Qatar World cup has been a controversy for years and the discussion leading up to it has not changed because of the possible human rights violations. The comments I have scraped are the top 100 mostv relevant comments and the top 100 most recent commmentts. This only leaves me with 200 comments to deal with. For now I will only keep it at 200 so that I can figure out my plan fro preprocessing but for my next blog post I will scrape more videos that discuss the controversies of the World Cup.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_q<- read_csv(\"_data/comments_q.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\n\ndf_qRev<- read_csv(\"_data/comments_qRev.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_qRev<- df_qRev %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_qFull <- rbind(df_q,df_qRev)\n\ndf_qFull_new <- df_qFull %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_qFull_new <- df_qFull_new %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n  \n  \n  \n\ncorpus_qFull <- corpus(df_qFull_new)\n\ncorpus_sum <- summary(corpus_qFull)\n\n\n\n\nqatar_tokens <- tokens(corpus_qFull,\n    remove_numbers = T,\n    remove_punct = T,\n    remove_symbols = T)\n```\n:::\n\n\n## Key Questions\n\nMy Goal is to:\n\n  - Find the overall Sentiment of the comments (Positive and Negative, Other emotions)\n  \n  - What is the main focus in the comments (what topic is most important to the people in the comments)\n  \n  - Based on the most important topics and the sentiment of those comments, are those comments classified correctly as     \n    positive or negative? If yes, are those comments \"socially correct\"? (logical/acceptable POV vs Stereotyped/Stigmatized \n    POV)\n  \n\n## DFM and DFM_TFIDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqatar_tokens <-tokens_tolower(qatar_tokens)\nqatar_tokens <- tokens_select(qatar_tokens, \n                              pattern = c(stopwords(\"en\"),\"[a-z]\",\"t\",\"m\",\"=\",\"ve\",\"don\",\"s\"),\n                              selection = \"remove\")\n\n\n\n\nqatar_dfm <- dfm(qatar_tokens)\n\n\n\ntopfeatures(qatar_dfm,100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        qatar         world           cup         video          like \n           76            73            64            46            42 \n         just       country       workers        people           can \n           39            38            35            35            27 \n    countries          fifa        rights           one      football \n           25            23            22            22            21 \n       really          know          good         human         great \n           21            21            20            19            19 \n         feel          work      stadiums            go          time \n           18            18            17            17            17 \n         even          quot           see          well     something \n           16            16            16            15            15 \n         much          also          make      watching           say \n           15            15            14            14            14 \n      migrant         think          love         watch           get \n           14            13            13            13            13 \n         want          fans         still          many        videos \n           12            12            12            12            12 \n          new       channel    corruption          made          hate \n           11            11            11            11            11 \n          job         lgbtq          laws         money          died \n           11            10            10            10            10 \n       middle           god       support           lot          keep \n           10            10            10            10            10 \n        place         thing            wc        worker       believe \n            9             9             9             9             9 \n         east           man          take           now international \n            9             9             9             9             9 \n   tournament           gay       corrupt          said            us \n            9             8             8             8             8 \n         arab          ever          poor       western         first \n            8             8             8             8             8 \n      slavery          seen           pay          west         saudi \n            8             8             8             8             8 \n   conditions      research          host     companies    government \n            8             8             8             8             8 \n      someone       however        deaths        thanks           bro \n            7             7             7             7             7 \n          sad          back         right           etc        labour \n            7             7             7             7             7 \n```\n:::\n\n```{.r .cell-code}\nqatar_dfm_tfidf <- dfm_tfidf(qatar_dfm)\n\ntopfeatures(qatar_dfm_tfidf,55)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        world         qatar           cup         video          like \n     43.00394      42.88463      40.57007      34.05668      32.68235 \n      workers        people       country          just           can \n     32.07589      30.85921      30.60153      30.34790      25.76455 \n       rights     countries          quot          fifa          know \n     24.65263      24.36115      24.29622      22.41226      22.39054 \n     football           one        really         human          work \n     22.39054      21.43781      21.37614      21.29090      21.28899 \n         good         great          feel      stadiums          time \n     20.35823      20.25811      20.17033      19.55913      19.55913 \n      migrant          even           see            go     something \n     19.51005      18.92355      18.92355      18.57327      17.74083 \n          job          much          also           get          well \n     17.57465      17.25806      17.25806      16.85665      16.80861 \n         make      watching         still          many           say \n     16.55811      16.55811      16.10907      16.10907      16.10752 \n        think         watch          fans          love          laws \n     15.82729      15.82729      15.55998      15.37538      15.18514 \n       videos           new          hate          want     companies \n     15.06327      14.76665      14.76665      14.60981      14.55635 \n   government international          made    corruption         lgbtq \n     14.55635      14.37926      14.26332      14.26332      13.93575 \n```\n:::\n:::\n\n\nThe words that stand out to me the most from the TFIDF are workers, rights, human, good, great, migrant, laws and hate an corruption. All of these words fall within the top 55 features of the TFIDF. Based off these findings within a small scope, I can infer that one major topic in these comments is the discussion of have migrant workers have been treated in the build up to the Qatar world Cup. \n\n## word Cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(qatar_dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 6 documents, 1,790 features (99.17% sparse) and 0 docvars.\n       features\ndocs    bullshit lgbtq rule perfect homophobic start everywhere disgrace forgot\n  text1        1     1    1       1          1     1          1        0      0\n  text2        0     0    0       0          0     0          0        1      1\n  text3        0     0    0       0          0     0          0        0      0\n  text4        0     0    0       0          0     0          0        0      0\n  text5        0     0    0       0          0     0          0        0      0\n  text6        0     0    0       0          0     0          0        0      0\n       features\ndocs    microsoft\n  text1         0\n  text2         1\n  text3         0\n  text4         0\n  text5         0\n  text6         0\n[ reached max_nfeat ... 1,780 more features ]\n```\n:::\n\n```{.r .cell-code}\nset.seed(1245)\n\ntextplot_wordcloud(qatar_dfm, min_count = 10, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThis show a visual of the frequency of the terms in the dfm.\n\n## Zipf's Law\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_counts <- as.data.frame(sort(colSums(qatar_dfm),dec=T))\ncolnames(word_counts) <- c(\"Frequency\")\nword_counts$Rank <- c(1:ncol(qatar_dfm))\nhead(word_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Frequency Rank\nqatar        76    1\nworld        73    2\ncup          64    3\nvideo        46    4\nlike         42    5\njust         39    6\n```\n:::\n\n```{.r .cell-code}\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooking at my distribution of words it seems like 8 terms in mt DFM appear way more frequently than the rest of the terms. Byadding more data I belive this will smooth out.\n\n# Feature Co-occurrence matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq_fcm <- fcm(qatar_dfm)\n\n# keep only top features.\nsmall_fcm <- fcm_select(q_fcm, pattern = names(topfeatures(q_fcm, 50)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_fcm))\n\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output-display}\n![](BlogPost3_NayanJani_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe features that pop out the most from the Feature Co-occurrence matrix are conditions, workers and pay due to the size of their point and location within the Semantic Network. The grouping of words job,pay,companies,workers,system and conditions suggests that there is a thematic similarity between them. This could imply a topic that includes these words.\n\n\n## Next Post\n\nFor my next post I will add more data to my corpus and begin dictionary analysis.\n",
    "supporting": [
      "BlogPost3_NayanJani_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}