{
  "hash": "a5a40efa99808870b965d1db77ccc13f",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3\"\nauthor: \"Miranda Manka\"\ndesription: \"Working with data\"\ndate: \"10/15/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Miranda Manka\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n\n## Data\n\nThe dataset contains 550 different articles from Arlnow (local news site in Northern Virginia) from March 2020 to September 2022. I decided to go back to March because that is when covid was officially declared a pandemic in the U.S. I may scrape more to get the months before March. I may also try to find a similar site for another county/city in another state to compare the two and see similarities and differences. \n\nHere I am just reading in the data from the csv I created and dropping the extra column that was created in the write.csv and renaming a column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narlnow_covid = read_csv(\"_data/arlnow_covid_posts.csv\", col_names = TRUE, show_col_types = FALSE)\narlnow_covid = subset(arlnow_covid, select = -c(1))\narlnow_covid = rename(arlnow_covid, text_field = raw_text)\n```\n:::\n\n\n## Analysis\n\nMost of this analysis follows the week 6 tutorial we were given, I found it very helpful and I wanted to note where a lot of the code/information came from as I use a lot of it for this post.\n\n### Corpus, Summary, and Tokens\n\n\n::: {.cell}\n\n```{.r .cell-code}\narlnow_covid_corpus = corpus(arlnow_covid, docid_field = \"doc_id\", text_field = \"text_field\")\narlnow_covid_summary = summary(arlnow_covid_corpus)\narlnow_covid_corpus_tokens = tokens(arlnow_covid_corpus, remove_punct = T)\n```\n:::\n\n\n### Document-Feature Matrix (DFM)\n\nI started to do this in my last blog post but I didn't have it quite right. I learned more about it this past week and now have a better sense of what is happening and why/how to use the dfm. I have now incorporated the updated code from the week 6 tutorial for a more clean and accurate usage, without having to use a lot of different packages and code.\n\nIn the DFM, the rows indicate documents, the columns indicate words, and the value of each cell in the matrix is the count of the word (column) for the document (row).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the dfm - this part includes basic pre-processing, including removing punctuation and numbers, makes words lowercase, and removes all english stop words. I also decided to remove some words relevant to this analysis, including \"arlington\", \"county\", \"virginia\", \"$\"; these terms are not really helpful to look at as they describe the location. This is done to clean the data and get it ready for further analysis.\narlnow_covid_dfm = tokens(arlnow_covid_corpus,\n                                    remove_punct = TRUE,\n                                    remove_numbers = TRUE) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english')) %>%\n                           dfm_remove(c(\"arlington\", \"county\", \"virginia\", \"$\"))\n\n# summary of the dfm\narlnow_covid_dfm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 550 documents, 15,270 features (98.86% sparse) and 3 docvars.\n       features\ndocs    hundred parents say public schools prioritize recreating pre-covid\n  text1       2       5   2      2       5          1          1         2\n  text2       0       0   0      1       0          0          0         0\n  text3       0       0   0      4       1          0          0         0\n  text4       0       0   0      1       0          0          0         0\n  text5       0       3   0      1       3          0          0         0\n  text6       0       0   0      3       0          0          0         0\n       features\ndocs    normalcy classroom\n  text1        2         1\n  text2        0         0\n  text3        0         0\n  text4        0         0\n  text5        0         0\n  text6        0         0\n[ reached max_ndoc ... 544 more documents, reached max_nfeat ... 15,260 more features ]\n```\n:::\n\n```{.r .cell-code}\n# most frequent terms (features)\ntopfeatures(arlnow_covid_dfm, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     said       new     cases     covid    school  covid-19    public      also \n     1032       799       600       573       534       507       498       488 \n     year    health community       one      last    people     local       can \n      464       454       437       436       419       414       400       399 \n     week       two     board  pandemic \n      385       371       368       357 \n```\n:::\n\n```{.r .cell-code}\n# can also look at the first text to see what words were uniquely used there (this can be done for any specific article/text)\nfirst_text_words = as.vector(colSums(arlnow_covid_dfm) == arlnow_covid_dfm[\"text1\",])\ncolnames(arlnow_covid_dfm)[first_text_words]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"recreating\"      \"electronic\"      \"informal\"        \"reside\"         \n[5] \"elementary-aged\" \"golden\"          \"pre-2020\"        \"ever-present\"   \n```\n:::\n:::\n\n\n#### Wordcloud\n\nThe size of the word corresponds to the frequency of the term in the corpus (how often it appears).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_wordcloud(arlnow_covid_dfm, min_count = 50, max_words = 50, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost3_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n#### Zipf's Law\n\nWord frequencies are distributed according to Zipf's law, where the frequency of any word is inversely proportional to its rank in the frequency table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first, we need to create a word frequency variable and the rankings\nword_counts = as.data.frame(sort(colSums(arlnow_covid_dfm), dec = T))\ncolnames(word_counts) = c(\"Frequency\")\nword_counts$Rank = c(1:ncol(arlnow_covid_dfm))\nhead(word_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Frequency Rank\nsaid          1032    1\nnew            799    2\ncases          600    3\ncovid          573    4\nschool         534    5\ncovid-19       507    6\n```\n:::\n\n```{.r .cell-code}\n# now we can plot this - and the plot seems to look as we expect\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost3_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Updating our DFMs, trimming and making smaller\n# trim based on the overall frequency (i.e., the word counts)\nsmaller_dfm = dfm_trim(arlnow_covid_dfm, min_termfreq = 200)\n\n# trim based on the proportion of documents that the feature appears in; here, \n# the feature needs to appear in more than 10% of documents (chapters)\nsmaller_dfm = dfm_trim(smaller_dfm, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 550 documents, 54 features (67.27% sparse) and 3 docvars.\n       features\ndocs    public schools according pandemic since school board says covid high\n  text1      2       5         1        1     1      6     2    1     4    1\n  text2      1       0         1        0     1      0     0    0     6    0\n  text3      4       1         0        0     1      0     0    0     4    0\n  text4      1       0         1        1     1      0     0    0     8    0\n  text5      1       3         0        0     0     11     3    2     4    3\n  text6      3       0         2        4     4      0     1    1     3    0\n[ reached max_ndoc ... 544 more documents, reached max_nfeat ... 44 more features ]\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(smaller_dfm, min_count = 50,\n                   random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost3_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n### Feature Co-occurrence Matrix\n\nThis tells us less about how words within the corpus relate to one another. The idea here is to construct a matrix that presents the number of times word{a} appears in the same document as word{b}.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm = dfm_trim(arlnow_covid_dfm, min_termfreq = 100)\nsmaller_dfm = dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm = fcm(smaller_dfm)\n\n# check the dimensions\ndim(smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28 28\n```\n:::\n:::\n\n\n#### Semantic Network\n\nThis can be used to look at relationships between words/themes in the corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pull the top features\nmyFeatures = names(topfeatures(smaller_fcm, 30))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm = fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28 28\n```\n:::\n\n```{.r .cell-code}\n# compute size weight for vertices in network\nsize = log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n```\n\n::: {.cell-output-display}\n![](MirandaManka_BlogPost3_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# I think this is a really interesting graph because it is similar to a word cloud in showing most used words, but it shows the connections between words, which I think is really helpful and I can use in further analysis.\n```\n:::\n",
    "supporting": [
      "MirandaManka_BlogPost3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}