{
  "hash": "f2ad788696c4f4473d309f14f77a8acc",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3: Pre-processing\"\nauthor: \"Andrea Mah\"\ndesription: \"Initial data exploration\"\ndate: \"10/24/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - BlogPost3\n  - Andrea Mah\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\nFor my project, I plan to analyze speeches given by world leaders at the UN Climate conferences. My goal for the past week was to get my data into good shape. Previously, I was able to import all the pdfs into R. However, I did not have metadata associated with those documents, many speeches were not in english, and the data needed to be cleaned.\n\n\n\nMy first step was to detect the language of the speeches and to subset my corpus to only include english texts. Fortunately, there were multiple packages which I could use to detect language. Ultimately I chose \"cld2\" which was reported to have high accuracy. After importing the files, I used the detect_language() command to create a vector representing the language of each document. Then I subsetted the data and saved a new file with only the english texts.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cleanNLP)\nlibrary(tidytext)\nlibrary(plyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n------------------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    compact\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(pdftools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing poppler version 22.04.0\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\n\n#creating list of names of files to read in\nfile_list <- list.files(pattern=\"*.pdf\")\n#\nall_files <- lapply(file_list, function(file){\n  txt <- pdf_text(file)\n  txt <- str_c(txt, collapse = \" \")\n  data.frame(File = file,text = txt)\n})\n#\n#bind\nresult <- do.call(\"rbind\", all_files)\n\n#checking import worked\nView(result)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in View(result): invalid 'x' argument\n```\n:::\n\n```{.r .cell-code}\n#detecting language? \nrequire(cld2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: cld2\n```\n:::\n\n```{.r .cell-code}\nt_start = proc.time()\ncld2_vec = cld2::detect_language(text = result$text, plain_text = TRUE, lang_code = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as_string(text): Parameter 'text' must be a connection or character vector\n```\n:::\n\n```{.r .cell-code}\n#bind result with data\nresult$language <- cld2_vec\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'cld2_vec' not found\n```\n:::\n\n```{.r .cell-code}\n#create subset data with only english language speeches\nen.result <-result[which(result$language == \"en\"),]\n#Save as dataframe\nsave(en.result, file = \"speeches.RData\")\nload(\"speeches.RData\")\n```\n:::\n\n\n\nMy next step was to get some metadata ('docvars') that I could use with my documents. The filenames in the downloaded speeches contained a lot of useful information. They had which conference the speech was from, the date the speech was delivered, and the speaker (in most cases, a country). I couldn't find anything in R to help me use the filename to extract that information (although I'm sure something exists.) What I ended up doing was exporting the list of file names from my english speeches dataframe, importing that into excel, and using a series of \"TEXTBEFORE\" and \"TEXTAFTER\" functions to isolate the information I wanted. I manually added in the year of each speech, which was easy to do after sorting the files in alphabetical order. I saved this 'metadata' as a csv. \n\nAfter importing the csv into R, I used left_join to merge the speeches with the metadata by file name. Now when I made the speech dataframe into a corpus I was able to see my metadata. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#remove the \"language\" column\nen.result <- en.result[,c(1,2)]\n\n#isolate only the filename column to export\nen.result.names <- en.result[,1]\n#Here I exported a csv\nwrite.csv(en.result.names, \"en.result.csv\")\n\n#using excel \"=textbefore()\" and \"=textafter()\" commands, I was able to isolate\n#Year and Speaker(country) using the file names. I saved this as a csv to import as my corpus metadata.\n\n#importing metadata file\nmetadata <- read.csv(\"metadata_docs.csv\", header = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in file(file, \"rt\"): cannot open file 'metadata_docs.csv': No such file\nor directory\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in file(file, \"rt\"): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\n#renaming column to match\nmetadata$File <- metadata$filename\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'metadata' not found\n```\n:::\n\n```{.r .cell-code}\n#joining my speeches with my metadata\nspeech.meta <- left_join(metadata, en.result, by = \"File\", all.x = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(metadata, en.result, by = \"File\", all.x = T): object 'metadata' not found\n```\n:::\n\n```{.r .cell-code}\n#saving this dataframe...\nsave(file =  \"speech.meta.RData\", speech.meta)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in save(file = \"speech.meta.RData\", speech.meta): object 'speech.meta' not found\n```\n:::\n:::\n\n\nNext, I needed to clean up the text and do some pre-processing. I didn't have much success using built in commands to remove numbers or other things, but a different approach using gsub() seemed to work. I'm sure the code I'm using could be simplified a lot. But at least this ended up with the result I wanted. After getting rid of things that I didn't want, I created a dfm, where I also removed stopwords and only included features which appeared a minimum of 10 times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Cleaning up the speeches\nload(\"speech.meta.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file\n'speech.meta.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\ntext <- speech.meta$text\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"$\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"~\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"<\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\">\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"1st\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"2nd\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"3rd\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"4th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"5th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"6th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"7th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"8th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"9th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"0th\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"1\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"2\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"3\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"4\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"5\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"6\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"7\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"8\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"9\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"0\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"%\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\"#\", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <-gsub(\" th \", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <-gsub(\" t \", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <-gsub(\" l \", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <-gsub(\" d \", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ntext <- gsub(\" mr. \", \" \", text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\nspeech.meta$text <- text\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in speech.meta$text <- text: object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nspeech.meta$text_field <- speech.meta$text\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nspeech.meta$docid_field <- speech.meta$File\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\n#Make this into a corpus\nspeech_corpus <- corpus(speech.meta)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(speech.meta): object 'speech.meta' not found\n```\n:::\n\n```{.r .cell-code}\nspeech_tokens <- tokens(speech_corpus, remove_punc = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(speech_corpus, remove_punc = T): object 'speech_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n#Create a DFM without punctuation or stopwords\ndfm_speech <- dfm(speech_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(speech_tokens): object 'speech_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ndfm_speech <- dfm_remove(dfm_speech, stopwords(\"english\")) %>%\n  dfm_trim(min_termfreq = 10, verbose = F)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm_select(x, ..., selection = \"remove\"): object 'dfm_speech' not found\n```\n:::\n:::\n\n\nNext I was excited to just explore the dfm. \n\n::: {.cell}\n\n```{.r .cell-code}\n#get some information about the dfm\nndoc(dfm_speech)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ndoc(dfm_speech): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\nhead(featnames(dfm_speech), 25)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in featnames(dfm_speech): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\n#See what's common\ntopfeatures(dfm_speech, 50)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(dfm_speech, 50): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\n#see what's common within each year\ntopfeatures(dfm_speech, 5, groups = year)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in topfeatures(dfm_speech, 5, groups = year): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\n#make a wordcloud \nset.seed(2222)\ntextplot_wordcloud(dfm_speech, min_count = 100, random_order = F)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud(dfm_speech, min_count = 100, random_order = F): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\ndfm.1995 <- subset(dfm_speech, year == \"1995\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in subset(dfm_speech, year == \"1995\"): object 'dfm_speech' not found\n```\n:::\n:::\n\n\n\nFinally, since I now had some metadata, I wanted to see if I could actually use it. I found some code online which was showing how to plot frequency of terms using ggplot(). I did this for the overall corpus, but could not figure out how to select specific terms and plot them by year... \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#trying out some frequencies? gets the top 10 features overall\nts_freq <- textstat_frequency(dfm_speech, n = 20)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textstat_frequency(dfm_speech, n = 20): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\nts_freq\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'ts_freq' not found\n```\n:::\n\n```{.r .cell-code}\n#what about top features by year? \nts_freq_byyear <- textstat_frequency(dfm_speech, n = 10, group = year)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textstat_frequency(dfm_speech, n = 10, group = year): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\nts_freq_byyear\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'ts_freq_byyear' not found\n```\n:::\n\n```{.r .cell-code}\nts_freq_byspeaker <- textstat_frequency(dfm_speech, n = 10, group = speaker)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textstat_frequency(dfm_speech, n = 10, group = speaker): object 'dfm_speech' not found\n```\n:::\n\n```{.r .cell-code}\nts_freq_byspeaker\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'ts_freq_byspeaker' not found\n```\n:::\n\n```{.r .cell-code}\ntopterms <- ggplot(data = ts_freq, aes(x = feature)) +\n  geom_bar(aes(y = frequency), stat = \"identity\") +\n  theme(panel.background = element_rect(fill = \"white\"), \n        axis.line = element_line(colour = \"black\"),\n        axis.text = element_text(size = 12),\n        axis.title.x = element_text(size =12, vjust = -1),\n        axis.title.y = element_text(size =12),\n        legend.key = element_blank(),\n        legend.position = \"top\",\n        legend.text = element_text(size = 14),\n        legend.title = element_blank())\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(data = ts_freq, aes(x = feature)): object 'ts_freq' not found\n```\n:::\n\n```{.r .cell-code}\ntopterms\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'topterms' not found\n```\n:::\n:::\n\n\nNow that my dataset is actually clean and ready to be analyzed, I'm excited to try topic modelling, to learn more about what types of stats I can calculate using the dfm, and figuring our more interesting ways to visualize the data. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}