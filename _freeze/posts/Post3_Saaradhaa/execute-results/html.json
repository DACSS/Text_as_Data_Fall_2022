{
  "hash": "2b47b4f3a553a6f6906a1ad986225384",
  "result": {
    "markdown": "---\ntitle: \"Post 3\"\nauthor: \"Saaradhaa M\"\ndescription: \"Cleaning PDFs\"\ndate: \"10/16/2022\"\neditor: visual\nformat:\n    html:\n        df-print: paged\n        toc: true\n        code-copy: true\n        code-tools: true\n        css: \"styles.css\"\ncategories: \n  - post 3\n  - saaradhaa\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(readtext)\nlibrary(phrasemachine)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(phrasemachine): there is no package called 'phrasemachine'\n```\n:::\n:::\n\n\n## Introduction\n\nIn this post, I will focus on cleaning and pre-processing my data into a format that is useful for analysis. So far, I have been struggling with this quite a bit. This post will specifically focus on cleaning the PDFs, pre-processing them in R, then trying out different descriptives.\n\n## Manual Cleaning\n\n-   To recap, I downloaded 50 PDFs of oral histories. I generated word clouds in Blog Post 2 to get some sense of how I need to clean them. I also skimmed through the PDFs to understand the content better.\n\n-   I deleted PDFs 42-50 (representing students of fine arts teachers). Methodologically, it doesn't make sense to include this group. When I skimmed through the transcripts, the actual age range of students varies, so they don't neatly represent young people. Further, for those who are actually young, they represent young people who are more attached to their cultural roots anyway, and might be fundamentally different from the young people who comment on Reddit.\n\n-   This leaves me with 41 transcripts. However, I had a lot of trouble figuring out a standardised way to clean all of them, as the oral histories were collected over a number of years by different researchers - hence, not all the PDFs had a consistent format.\n\n-   For transcripts 1 to 34, I initially wanted to convert them to HTML to remove **bold** text, which represents most of what needs to be removed. I was unable to use R packages (poppler, pdftohtml, pdf2html) to convert them to HTML and do this, since these packages only work on older versions of R.\n\n-   So unfortunately, I had to do some steps manually:\n\n    -   Using Actions in Adobe Acrobat, I converted PDF to HTML to get rid of the running header and footer, then converted to plain text.\n\n    -   I manually removed the header for transcripts 12, 14, 21 and 22, and 'narrator', 'date', 'interviewed by', 'place' and 'end of interview' for all.\n\n    -   I manually removed the post-script for transcript 28 - it's a post-interview message from the interviewee, which does not represent meaningful oral history data.\n\n    -   I manually removed the header, index and glossary for transcripts 35 to 41.\n\n## Processing in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# list out files.\nfile_list <- dir(\"~/Desktop/2022_Fall/GitHub/Text_as_Data_Fall_2022/posts/Transcripts\", \n             full.names = TRUE, pattern = \"*.txt\")\n\n# create list of text files.\ntranscripts <- readtext(paste0(file_list), docvarnames = c(\"transcript\", \"text\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in list_files(file, ignore_missing_files, FALSE, cache, verbosity): File '' does not exist.\n```\n:::\n\n```{.r .cell-code}\n# remove references to 'interviewer:' and 'interviewee:', as well as line breaks, '\\n'. I found this website really helpful in testing out regex: https://spannbaueradam.shinyapps.io/r_regex_tester/\ntranscripts$text <- str_replace_all(transcripts$text, \"[a-zA-Z]+:\", \"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\ntranscripts$text <- str_replace_all(transcripts$text, \"\\n\", \"\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stri_replace_all_regex(string, pattern, fix_replacement(replacement), : object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\n# create 'quanteda' corpus. \noh_corpus <- corpus(transcripts$text)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(transcripts$text): object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\n# create my own list of stopwords, based on qualitative reading of the first transcript.\nto_keep <- c(\"do\", \"does\", \"did\", \"would\", \"should\", \"could\", \"ought\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"because\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"over\", \"under\", \"no\", \"nor\", \"not\")\nStopwords <- stopwords(\"en\")\nStopwords <- Stopwords[!(Stopwords %in% to_keep)]\n\n# create tokens, remove punctuation, numbers and stopwords, then convert to lowercase.\noh_tokens <- tokens(oh_corpus, remove_punct = T, remove_numbers = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(oh_corpus, remove_punct = T, remove_numbers = T): object 'oh_corpus' not found\n```\n:::\n\n```{.r .cell-code}\noh_tokens <- tokens_select(oh_tokens, pattern = Stopwords, selection = \"remove\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(oh_tokens, pattern = Stopwords, selection = \"remove\"): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\noh_tokens <- tokens_tolower(oh_tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_tolower(oh_tokens): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n# get summary of corpus.\noh_summary <- summary(oh_corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(oh_corpus): object 'oh_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# what's the average number of 'types' per interview? ~1868.\nmean(oh_summary$Types)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mean(oh_summary$Types): object 'oh_summary' not found\n```\n:::\n\n```{.r .cell-code}\n# how many tokens in total? ~24k.\nsum(max(ntoken(oh_corpus)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ntoken(oh_corpus): object 'oh_corpus' not found\n```\n:::\n\n```{.r .cell-code}\n# create metadata. moving forward, i can potentially add extra metadata, like gender of interviewee and year of immigration.\ndocvars(oh_corpus) <- oh_summary\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'oh_summary' not found\n```\n:::\n:::\n\n\n## Text Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# i want to re-create the wordcloud from Blog Post 2, except now with cleaned data - just for the wordcloud, i'm going to remove all stopwords, so that they don't show up in it.\ndfm <- oh_tokens %>% dfm() %>% dfm_trim(min_termfreq = 10, verbose = FALSE, min_docfreq = .1, docfreq_type = \"prop\") %>% dfm_remove(stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(.): object 'oh_tokens' not found\n```\n:::\n\n```{.r .cell-code}\ntextplot_wordcloud(dfm, max_words=50, color=\"purple\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: textplot_wordcloud() only works on dfm, keyness objects.\n```\n:::\n:::\n\n\nThe wordcloud looks a lot better! There's definitely some references to migration (come, going, went), and temporal aspects (time, years). Conversations also involved references to family, school, work and India (likely the largest sub-group of South Asian interviewees).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create fcm.\nfcm <- fcm(dfm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: fcm() only works on character, corpus, dfm, tokens objects.\n```\n:::\n\n```{.r .cell-code}\n# keep only top features.\nsmall_fcm <- fcm_select(fcm, pattern = names(topfeatures(fcm, 50)), selection = \"keep\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: fcm_select() only works on fcm objects.\n```\n:::\n\n```{.r .cell-code}\n# compute weights.\nsize <- log(colSums(small_fcm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'colSums': object 'small_fcm' not found\n```\n:::\n\n```{.r .cell-code}\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_network(small_fcm, vertex_size = size/max(size) * 4): object 'small_fcm' not found\n```\n:::\n:::\n\n\nI made a network plot, just out of curiosity - I don't know why there is a '\\<' and '\\>' even after removing punctuation. Will I need to remove these manually?\n\n## N-Grams\n\nI want to test out n-grams with the first transcript to get a sense of what kind of terms I may need to further remove.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocuments <- transcripts$text[1]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'transcripts' not found\n```\n:::\n\n```{.r .cell-code}\nphrases <- phrasemachine(documents, minimum_ngram_length = 2, maximum_ngram_length = 4, return_phrase_vectors = TRUE, return_tag_sequences = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in phrasemachine(documents, minimum_ngram_length = 2, maximum_ngram_length = 4, : could not find function \"phrasemachine\"\n```\n:::\n\n```{.r .cell-code}\nphrases[[1]]$phrases[1:100]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'phrases' not found\n```\n:::\n:::\n\n\nJust from the above phrases, it seems like my leaning towards topic modelling might be good for the transcripts. We can see references to South Asia (Mahatma Gandhi, Bombay, East Indians) and the US (Tufts, Massachusetts). There are also references to WW2, which could potentially come up as a topic.\n\n## Next Blog Post\n\n-   Generate all comments for the Reddit data, as well as more posts. I may need to use Python for this.\n\n-   Use str_match() compare how many times \"culture\" appears in the oral histories vs. Reddit data, and do a statistical test.\n\n-   Try out CleanNLP (Week 4 Tutorial).\n",
    "supporting": [
      "Post3_Saaradhaa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}