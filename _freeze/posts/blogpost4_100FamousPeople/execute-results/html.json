{
  "hash": "11dec950e2e3f0d21642a8663b68b585",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 4 - Data Visualizations\"\nauthor: \"Adithya Parupudi\"\ndesription: \"Data Visualizations - Word cloud etc\"\ndate: \"10/11/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Adithya Parupudi\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(tm) # for stop words removal\nlibrary(ggplot2) # for graphs\nlibrary(tokenizers)\n```\n:::\n\n\nReading data from CSV file\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeople_titles <- read_csv(\"PeopleTitles.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 130 Columns: 3\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): people_names_temp, temp3 dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n:::\n\n```{.r .cell-code}\nall_data <- read_csv(\"100FamousPeople.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 116 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): people_names, links, content dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n:::\n:::\n\n\nRemoved all stop words using the SnowBallC package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxyz <- all_data %>% unnest_tokens(words, content, token=\"words\", strip_punct = TRUE)\n\n# removed stop words using filter() function\nxyz <- xyz %>% filter(!(words %in% stopwords(source = \"smart\")))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `filter()`:\n! Problem while computing `..1 = !(words %in% stopwords(source =\n  \"smart\"))`.\nCaused by error in `stopwords()`:\n! unused argument (source = \"smart\")\n```\n:::\n\n```{.r .cell-code}\n# for stemmimg\nlibrary(SnowballC)\nnew_xyz <- xyz %>%\n  mutate(stem = wordStem(words)) %>%\n  count(stem, sort = TRUE)\n```\n:::\n\n\nAttempts to remove stop words. DIfferent chunks of code. \n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenised_data <- all_data %>%\n#   unnest_tokens(tokens,content)\n# # arg 1 -> output columns\n# # arg 2 - column from dataset which is going to be tokenized\n# length(tokenised_data$tokens)\n# \n# data(\"stop_words\")\n# \n# removed_stop_words <- tokenised_data %>%\n#   anti_join(stop_words, by=character())\n# \n# \n# 155144\n# summary(removed_stop_words)\n\n\n\n\nremoved_stop_words <- removeWords(all_data$content %>% tolower(), stopwords())\n```\n:::\n\n\n\nCreated a corpus, removed stop words, punctuation, numbers, whitespaces, converted to lowercase.\nThen created document term matrix and generated a word cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# only content picked\nonly_content <- all_data$content\n# corpus created\ncreate_corpus <- Corpus(VectorSource(only_content))\n\nlibrary(tm)\n# cleaning data using tm library\ncreate_corpus <- create_corpus %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removeNumbers): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., removePunctuation): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(., stripWhitespace): transformation drops\ndocuments\n```\n:::\n\n```{.r .cell-code}\ncreate_corpus <- tm_map(create_corpus, content_transformer(tolower))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(create_corpus, content_transformer(tolower)):\ntransformation drops documents\n```\n:::\n\n```{.r .cell-code}\ncreate_corpus <- tm_map(create_corpus, removeWords, stopwords(\"english\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(create_corpus, removeWords,\nstopwords(\"english\")): transformation drops documents\n```\n:::\n\n```{.r .cell-code}\n# creating document term matrix\n\ndtm <- TermDocumentMatrix(create_corpus) \nmatrix <- as.matrix(dtm) \nwords <- sort(rowSums(matrix),decreasing=TRUE) \ndf <- data.frame(word = names(words),freq=words)\n\n# generate word cloud\nlibrary(wordcloud)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nset.seed(1234) # for reproducibility \nwordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](blogpost4_100FamousPeople_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncreate_corpus <- corpus(dataset$content)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(dataset$content): object 'dataset' not found\n```\n:::\n\n```{.r .cell-code}\ncorpus_summary <- summary(create_corpus)\ncorpus_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Length Class             Mode\n1   2      PlainTextDocument list\n2   2      PlainTextDocument list\n3   2      PlainTextDocument list\n4   2      PlainTextDocument list\n5   2      PlainTextDocument list\n6   2      PlainTextDocument list\n7   2      PlainTextDocument list\n8   2      PlainTextDocument list\n9   2      PlainTextDocument list\n10  2      PlainTextDocument list\n11  2      PlainTextDocument list\n12  2      PlainTextDocument list\n13  2      PlainTextDocument list\n14  2      PlainTextDocument list\n15  2      PlainTextDocument list\n16  2      PlainTextDocument list\n17  2      PlainTextDocument list\n18  2      PlainTextDocument list\n19  2      PlainTextDocument list\n20  2      PlainTextDocument list\n21  2      PlainTextDocument list\n22  2      PlainTextDocument list\n23  2      PlainTextDocument list\n24  2      PlainTextDocument list\n25  2      PlainTextDocument list\n26  2      PlainTextDocument list\n27  2      PlainTextDocument list\n28  2      PlainTextDocument list\n29  2      PlainTextDocument list\n30  2      PlainTextDocument list\n31  2      PlainTextDocument list\n32  2      PlainTextDocument list\n33  2      PlainTextDocument list\n34  2      PlainTextDocument list\n35  2      PlainTextDocument list\n36  2      PlainTextDocument list\n37  2      PlainTextDocument list\n38  2      PlainTextDocument list\n39  2      PlainTextDocument list\n40  2      PlainTextDocument list\n41  2      PlainTextDocument list\n42  2      PlainTextDocument list\n43  2      PlainTextDocument list\n44  2      PlainTextDocument list\n45  2      PlainTextDocument list\n46  2      PlainTextDocument list\n47  2      PlainTextDocument list\n48  2      PlainTextDocument list\n49  2      PlainTextDocument list\n50  2      PlainTextDocument list\n51  2      PlainTextDocument list\n52  2      PlainTextDocument list\n53  2      PlainTextDocument list\n54  2      PlainTextDocument list\n55  2      PlainTextDocument list\n56  2      PlainTextDocument list\n57  2      PlainTextDocument list\n58  2      PlainTextDocument list\n59  2      PlainTextDocument list\n60  2      PlainTextDocument list\n61  2      PlainTextDocument list\n62  2      PlainTextDocument list\n63  2      PlainTextDocument list\n64  2      PlainTextDocument list\n65  2      PlainTextDocument list\n66  2      PlainTextDocument list\n67  2      PlainTextDocument list\n68  2      PlainTextDocument list\n69  2      PlainTextDocument list\n70  2      PlainTextDocument list\n71  2      PlainTextDocument list\n72  2      PlainTextDocument list\n73  2      PlainTextDocument list\n74  2      PlainTextDocument list\n75  2      PlainTextDocument list\n76  2      PlainTextDocument list\n77  2      PlainTextDocument list\n78  2      PlainTextDocument list\n79  2      PlainTextDocument list\n80  2      PlainTextDocument list\n81  2      PlainTextDocument list\n82  2      PlainTextDocument list\n83  2      PlainTextDocument list\n84  2      PlainTextDocument list\n85  2      PlainTextDocument list\n86  2      PlainTextDocument list\n87  2      PlainTextDocument list\n88  2      PlainTextDocument list\n89  2      PlainTextDocument list\n90  2      PlainTextDocument list\n91  2      PlainTextDocument list\n92  2      PlainTextDocument list\n93  2      PlainTextDocument list\n94  2      PlainTextDocument list\n95  2      PlainTextDocument list\n96  2      PlainTextDocument list\n97  2      PlainTextDocument list\n98  2      PlainTextDocument list\n99  2      PlainTextDocument list\n100 2      PlainTextDocument list\n101 2      PlainTextDocument list\n102 2      PlainTextDocument list\n103 2      PlainTextDocument list\n104 2      PlainTextDocument list\n105 2      PlainTextDocument list\n106 2      PlainTextDocument list\n107 2      PlainTextDocument list\n108 2      PlainTextDocument list\n109 2      PlainTextDocument list\n110 2      PlainTextDocument list\n111 2      PlainTextDocument list\n112 2      PlainTextDocument list\n113 2      PlainTextDocument list\n114 2      PlainTextDocument list\n115 2      PlainTextDocument list\n116 2      PlainTextDocument list\n```\n:::\n\n```{.r .cell-code}\ndocvars(create_corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: docvars() only works on corpus, dfm, readtext, tokens objects.\n```\n:::\n\n```{.r .cell-code}\n# # get a list of years from each name\n# all %>% html_nodes(\"ol li\") %>% html_text2() %>% \n#   str_detect(.,\"\\\\(.*>\\)\")\n```\n:::\n",
    "supporting": [
      "blogpost4_100FamousPeople_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}