{
  "hash": "9adfe8e945f673babe1bd1c9fde63766",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 2: Data\"\nauthor: \"Andrea Mah\"\ndesription: \"Initial data exploration\"\ndate: \"10/02/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw1\n  - challenge1\n  - my name\n  - dataset\n  - ggplot2\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: package or namespace load failed for 'tidyverse' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 1.0.4 is already loaded, but >= 1.0.6 is required\n```\n:::\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\nFor my project, I plan to analyze statements given during UN conferences COP1 - COP15 which occured between 1995 and 2021.\n\nI was provided with these statements during conference opening, closing, and high-level segment sessions through a request to the UNFCCC archivists. \n\nThese statements were provided as pdfs, separated by year and forum (opening, closing, high-level). \n\nI was able to read all files in at once using the following code: \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: package or namespace load failed for 'tidytext' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 1.0.4 is already loaded, but >= 1.0.6 is required\n```\n:::\n\n```{.r .cell-code}\nlibrary(plyr)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: package or namespace load failed for 'tidyverse' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 1.0.4 is already loaded, but >= 1.0.6 is required\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"mMatrix\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"unpackedMatrix\" of class \"replValueSp\"; definition not updated\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nlibrary(pdftools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing poppler version 22.04.0\n```\n:::\n\n```{.r .cell-code}\nlibrary(cleanNLP)\n\n#creating list of names of files to read in\n#file_list <- list.files(pattern=\"*.pdf\")\n\n#all_files <- lapply(file_list, function(file){\n#  text <- pdf_text(file)\n # text <- str_c(text, collapse = \" \")\n  #data.frame(file = file,text = text)\n#})\n\n#bind\n#result <- do.call(\"rbind\", all_files)\n\n#for the sake of the blog though, the wd can't be changed, so I load my existing data file\nload(\"~/Work/Current Projects/TaD/Text_as_Data_Fall_2022/result.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file 'C:/\nUsers/srika/OneDrive/Documents/Work/Current Projects/TaD/Text_as_Data_Fall_2022/\nresult.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n:::\n\n\n\nThis resulted in the expected number of documents (1808) being created as a dataframe where one column was the name of the file, and the other column included the text of the pdf.\n\nI got errors when trying to annotate. So, at first, I explored frequency of statements which mentioned my terms of interest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Making a corpus version to look at summary\nresult_corpus <- corpus(result)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(result): object 'result' not found\n```\n:::\n\n```{.r .cell-code}\nsc <- summary(result_corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(result_corpus): object 'result_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nhead(sc)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(sc): object 'sc' not found\n```\n:::\n\n```{.r .cell-code}\n#how many tokens?\n\n\nrc_tokens <- tokens(result_corpus, remove_punct = T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(result_corpus, remove_punct = T): object 'result_corpus' not found\n```\n:::\n\n```{.r .cell-code}\nsum(max(ntoken(rc_tokens)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ntoken(rc_tokens): object 'rc_tokens' not found\n```\n:::\n\n```{.r .cell-code}\n#How many rows contain \"resilient\" and derivatives?\nlength(str_which(result$text, \" [Rr]esilien.* \")) #72\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in str_which(result$text, \" [Rr]esilien.* \"): could not find function \"str_which\"\n```\n:::\n\n```{.r .cell-code}\n#How many statements includeadaptation or mitigation (and related words)\nlength(str_which(result$text, \" [Aa]dapt.* \" )) #939\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in str_which(result$text, \" [Aa]dapt.* \"): could not find function \"str_which\"\n```\n:::\n\n```{.r .cell-code}\nlength(str_which(result$text, \" [Mm]itigat.* \")) #638\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in str_which(result$text, \" [Mm]itigat.* \"): could not find function \"str_which\"\n```\n:::\n\n```{.r .cell-code}\n#\n#how many speeches have words about hope, (excluding hopeless)\nlength(str_which(result$text, \" [Hh]opes?|[Hh]opeful.*|[Hh]oping \")) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in str_which(result$text, \" [Hh]opes?|[Hh]opeful.*|[Hh]oping \"): could not find function \"str_which\"\n```\n:::\n:::\n\nOne issue I have is that I currently have no meta-data associated with the documents. The questions I really want to ask need more information than I've been able to include at this point. My dataframe has 2 columns, one which is the file name and a second which includes the text of that file. The file names do follow some conventions, they include which COP conference the speech was at as well as the country or speaker. I am thinking about how I could use the filename to extract that information, although it isn't always located in the same position in the filename, e.g. \"High Level Segment Statement COP12 Netherlands 20061115.pdf\" vs. \"Opening Statement COP10 UNFCCC Executive Secretary 20041206.pdf My main research questions are about change over time, so the one way I could think to do this is to create a separate corpus for each conference that I could add a \"year\" variable to, and then to combine those corpora. I don't think it would be that much work to do, given that there are only 15 conferences, but I am wondering if there might be a better way...\n\nAnother question I had was whether there are best practices or standards for making sure the data imports correctly when dealing with a large amount of data. My data seems to have been imported ok but there were some source files which were scanned, the pdfs were formatted very differently from each other, and the quality was not the best, so I'm not sure right now what the best way is to check the accuracy of pdf_text(), without looking at every single file, or if there's a minimum amount I should check (10%? 25%?) to see if they imported correctly?\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}