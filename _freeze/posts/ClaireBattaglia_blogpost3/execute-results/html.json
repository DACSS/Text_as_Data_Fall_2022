{
  "hash": "04c2f8a0919e07395861f78d23f15265",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3\"\nauthor: \"Claire Battaglia\"\ndesription: \"Blog Post 3\"\ndate: \"10/16/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Claire Battaglia\n  - text-as-data\n  - blog post 3\n  - open-text survey response\n  - Document Term Matrix\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(plyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load tidy data from blog post 2\nload(\"CFA_tidy.RData\")\n```\n:::\n\n\n## Creating My Corpus\n\nI am most interested in the question \"What changes would you like to see for Missoulaâ€™s food system?\" so I'll start there. First I'll create a corpus of the responses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create corpus for \"change\"\nchange_corpus <- corpus(CFA_tidy$change)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: NA is replaced by empty string\n```\n:::\n\n```{.r .cell-code}\n# get summary\nchange_summary <- summary(change_corpus)\n\n# add indicator (not sure I need this but it won't hurt)\nchange_summary$question <- \"change\"\n\n# preview\nhead(change_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Text Types Tokens Sentences question\n1 text1     7      7         1   change\n2 text2     0      0         0   change\n3 text3    10     10         1   change\n4 text4    75     98         2   change\n5 text5    38     40         2   change\n6 text6     0      0         0   change\n```\n:::\n:::\n\n\nI have created a corpus of the responses in which each text is an individual response. The summary shows how many types, tokens, and sentences are in each response. This is utterly uninteresting information for this particular corpus and research question but it's good to practice.\n\n## Pre-Processing Decisions\n\nNext I need to make some decisions about pre-processing. Some options include:\n\n* Remove capitalization. - *I don't see any reason not to.*\n* Remove punctuation. - *Again, I don't see any reason not to.*\n* Remove stop words. - *I will probably want to remove stop words but I am going to examine the list and may customize it.*\n* Remove numbers. - *I am not going to remove numbers. The question, especially in the larger context of the survey, could easily yield responses that contain numbers that are meaningful.*\n* Stem - *Stemming makes me nervous and I don't see any reason it would be necessary so I am not going to stem.*\n* Infrequently used terms - *Not sure about this yet.*\n* N-grams - *Not sure about this yet.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize\nchange_tokens <- tokens(change_corpus, \n    remove_punct = T) # remove punctuation\nchange_tokens <- tokens_tolower(change_tokens) # remove capitalization\n\nsave(change_tokens, file = \"change_tokens.RData\")\n\n# view\nprint(change_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 389 documents.\ntext1 :\n[1] \"more\"          \"concentration\" \"on\"            \"the\"          \n[5] \"basic\"         \"good\"          \"groups\"       \n\ntext2 :\ncharacter(0)\n\ntext3 :\n[1] \"i'd\"          \"like\"         \"to\"           \"see\"          \"more\"        \n[6] \"storytelling\" \"around\"       \"regenerative\" \"agriculture\" \n\ntext4 :\n [1] \"factual\"        \"evidence-based\" \"relationships\"  \"are\"           \n [5] \"not\"            \"widely\"         \"understood\"     \"between\"       \n [9] \"local\"          \"food\"           \"resilience\"     \"climate\"       \n[ ... and 70 more ]\n\ntext5 :\n [1] \"whatever\"    \"it\"          \"takes\"       \"to\"          \"get\"        \n [6] \"nutritious\"  \"sustainable\" \"food\"        \"into\"        \"the\"        \n[11] \"hands\"       \"of\"         \n[ ... and 25 more ]\n\ntext6 :\ncharacter(0)\n\n[ reached max_ndoc ... 383 more documents ]\n```\n:::\n:::\n\n\nThings I'm thinking about right now:\n\n* Would it be meaningful to/how could I capture frequency of concept \"more or less\" of something? Could I sort everything respondents want more or less of into broad categories? I.e. 75% of respondents want more of something. Of those that want more of something, 35% want more education, 10% want more food assistance benefits, etc. Is this what STM allows me to do?\n\n## Document Feature Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create document feature matrix\nchange_dfm <- dfm(change_tokens)\n\nsave(change_dfm, file = \"change_dfm.RData\")\n\n# view\nprint(change_dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 389 documents, 1,295 features (99.12% sparse) and 0 docvars.\n       features\ndocs    more concentration on the basic good groups i'd like to\n  text1    1             1  1   1     1    1      1   0    0  0\n  text2    0             0  0   0     0    0      0   0    0  0\n  text3    1             0  0   0     0    0      0   1    1  1\n  text4    0             0  0   1     0    0      0   0    0  2\n  text5    0             0  0   1     0    0      0   0    1  1\n  text6    0             0  0   0     0    0      0   0    0  0\n[ reached max_ndoc ... 383 more documents, reached max_nfeat ... 1,285 more features ]\n```\n:::\n:::\n\n\nAgain, this yields an enormous volume of uninteresting, unsurprising data (part of the reason is likely that I haven't removed any stopwords yet).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get 10 most frequent terms\ntopfeatures(change_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   to   and  food  more local   the   for    of     a     i \n  208   205   163   138   126   120   108   101    83    69 \n```\n:::\n:::\n\n\nI'm going to create a separate object for my corpus with the stopwords removed so that I can compare the document feature matrices and most frequent words side-by-side.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new object\nchange_tokens_no_stop <- tokens_select(change_tokens,\n                                       pattern = stopwords(\"en\"),\n                                       selection = \"remove\")\n# create new dfm\nchange_no_stop_dfm <- dfm(change_tokens_no_stop)\n\n# get 10 most frequent terms\ntopfeatures(change_no_stop_dfm, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      food      local       like    farmers       land        see affordable \n       163        126         40         40         40         35         33 \n   locally  community    options \n        30         29         23 \n```\n:::\n:::\n\n\nHmm, interesting. Much more informative but some of the most frequent words are 1) completely expected, and 2) not substantively meaningful on their own.\n\nFor example, I would expect \"food\" to be mentioned frequently but without any sense of what the respondent has said about it, it's not meaningful. The survey is about the food system so their answer *should* have something to do with food.\n\nI'll need to think about this more.\n\nJust for fun, though, I'll create a word cloud of the 10 most frequent terms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create wordcloud\ntextplot_wordcloud(change_no_stop_dfm, min_count = 5, max_words = 50, random_order = FALSE)\n```\n\n::: {.cell-output-display}\n![](ClaireBattaglia_blogpost3_files/figure-html/wordcloud-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "ClaireBattaglia_blogpost3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}