{
  "hash": "2e53d36a84873d119595bbbebc032bbf",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 2\"\nauthor: \"Nayan Jani\"\ndescription: \"Getting my data\"\ndate: \"10/12/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Blog\n  - Stigma\n  - Sports\n  - Nayan\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(quanteda)\n\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## Literature Review\n\nThe first article I looked at talked about racial bias in Officials from the Italian Serie A. The goal of this study was to see if the trained Officials are subject to bias against Black and dark-skinned players and penalize them more than other players. The data contains information for each player in the Serie A from the 2009/10 season to the 2020/21 season. The study used three versions of the Football Manager videogame (Football Manager, 2011, 2018, 2021) to collect data\non player skin tones, This skin tone variable is a continuous variable that ranges from 1, lightest skin tone, to 20, darkest skin tone. For red and yellow cards, the study used data from Footystats (2021) and data for fouls were available from WhoScored (2021) and from FBREF (2021). The main hypothesis of the study is that bias against darker-skinned players has likely resulted in unfair patterns of refereeing, including the distribution of a greater number of foul calls, yellow cards, and ejections (red cards). The methods usedin this study were OLS and Poisson Regression. The study found that skin tone does affect referee decisions, especially with respect to fouls committed and yellow cards, and more weakly with respect to red cards. Overall, I found this study interesting because it is looking into racial bias that actually effects the game. This shows that the racial stigmas are still a problem in sports and are effecting the integrity of the game.\n\nThe Second article I read discussed racial bias in National Football League officiating. The goal of this study was to examine potential racial bias regarding holding penalties in the National Football League (NFL). The conatains info from the 2013 to 2014 through 2015 to 2016 NFL seasons that includes the races of officials and players involved in holding penalties. The two types of analysis are used to determine racial bias, player-level analysis and a game-level analysis. The outcome of the player analysis is a dichotomous variable where it indicates a any combination of a white/black official calls a penalty on a white/black player. The dependent variable in the game-level analysis is the percentage of holding penalties called on Black players per game. The player-level analysis uses multinomial linear regression and the game-level analysis uses linear regression. The results showed no evidence of racial bias in the calling of holding penalties by White officials and Black players were more likely to have holding penalties called on them earlier in the game by all officials. Overall I found this article intersting because there is a lot of grey areas when calling a holding call and it is cool to see if racial bias has any effect on this type of call. If the study was able to determine a stronger relationship between racial bias and holding calls, it could lead to a more fair game and can remove a lot of bad calls.\n\n\n## My Project Idea\n\nThe topic I want to look into is Sports Fans. I want to find out what groups of sports fans are more socially correct than others. What I mean by socially correct is that these groups of fans do not have any prejudice or enforce stigmas towards other groups of people. The groups of fans I would like to analyze are Soccer, NFL, NBA and UFC fans. To analyze this groups of fans, I will look into their textual responses of certain topics. For soccer fans I will look at their discussion about including LGBTQ in this years world cup in Qatar. For UFC I will look into the responses of fans to including certain fighters in their Hispanic heritage montage. For NFL. I will look at the responses of fans to the Deshaun Watson vs Calvin Ridley punishments. For NBA, I will look at the fans responses to the Ime Udoka vs Robert Sarver punishments. The data I will use will come from Youtube API. Most of the these fan discussions come from comments on Youtube and I believe analyzing the language they use will determine if certain groups of fans can be more socially correct. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_q<- read_csv(\"_data/comments_q.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): I’ll try to get the next video essay out in less than a month lol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\n\ncorpus_q <- corpus(df_q)\n\ncorpusQ_sum <- summary(corpus_q)\ncorpusQ_sum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 99 documents, showing 99 documents:\n\n   Text Types Tokens Sentences\n  text1    11     15         1\n  text2    16     17         3\n  text3    46     51         1\n  text4    13     13         1\n  text5    86    125         7\n  text6     3      3         1\n  text7    48     55         3\n  text8    18     20         1\n  text9    28     32         2\n text10    20     27         2\n text11    19     19         1\n text12    18     18         1\n text13     8      8         1\n text14    18     25         1\n text15     2      2         1\n text16    48     59         4\n text17    23     24         2\n text18    73     97         2\n text19    85    150         6\n text20    51     69         3\n text21    22     25         2\n text22    26     28         4\n text23    12     12         1\n text24    23     26         1\n text25    17     24         1\n text26    34     40         2\n text27    80    124         6\n text28    14     14         2\n text29    70     85         3\n text30    14     14         2\n text31    42     59         1\n text32    51     70         2\n text33    12     16         1\n text34     6      6         1\n text35     9     11         2\n text36    12     12         1\n text37    23     23         1\n text38    26     32         1\n text39     3      3         1\n text40     5      5         1\n text41   114    222         7\n text42    20     21         2\n text43    22     27         1\n text44    29     33         2\n text45     6      6         1\n text46    22     25         4\n text47    24     26         3\n text48    81    109         1\n text49    16     21         2\n text50    16     31         3\n text51    15     15         1\n text52    26     32         1\n text53    34     39         2\n text54     9     11         1\n text55    12     12         1\n text56     6      6         1\n text57    12     12         1\n text58     2      2         1\n text59    20     22         1\n text60    54     77         2\n text61    26     29         3\n text62     7      7         1\n text63    19     19         2\n text64     4      6         1\n text65    19     22         2\n text66    58     77         3\n text67     4      4         1\n text68    17     24         1\n text69    42     53         3\n text70    15     19         1\n text71    66     84         5\n text72     1      1         1\n text73    25     30         1\n text74    17     17         1\n text75    45     63         1\n text76    11     11         1\n text77    22     35         1\n text78    46     64         4\n text79     9      9         1\n text80    23     28         3\n text81    11     14         1\n text82    51     59         2\n text83    12     14         1\n text84     7      7         1\n text85    22     25         2\n text86    85    125         2\n text87    27     54         4\n text88     9      9         1\n text89    22     27         2\n text90    33     41         1\n text91    15     15         1\n text92     8      8         1\n text93   128    180         4\n text94     6      6         1\n text95     5      5         1\n text96    71     94         3\n text97    21     28         1\n text98    98    179        10\n text99    11     12         1\n```\n:::\n\n```{.r .cell-code}\ndf_nba <- read_csv(\"_data/comments_nba.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 98 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Thoughts on Malika and Stephen A having a disagreement?\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_nba<- df_nba %>%\n  rename(text = \"Thoughts on Malika and Stephen A having a disagreement?\")\n\n\ncorpus_nba <- corpus(df_nba)\n\ncorpusNBA_sum <- summary(corpus_nba)\ncorpusNBA_sum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 98 documents, showing 98 documents:\n\n   Text Types Tokens Sentences\n  text1    97    178         6\n  text2    64     99         2\n  text3    33     37         1\n  text4    22     26         2\n  text5     7      7         1\n  text6    60     67         4\n  text7    14     14         2\n  text8     7      7         1\n  text9     7     10         2\n text10    10     13         3\n text11    27     27         1\n text12    39     47         4\n text13    23     27         3\n text14    15     16         1\n text15     5      6         1\n text16    11     11         1\n text17    45     55         2\n text18    26     31         4\n text19    19     24         1\n text20     7      7         1\n text21    54     87         2\n text22    19     27         1\n text23    39     47         1\n text24    25     27         1\n text25    31     37         1\n text26    66     93         1\n text27     5      5         1\n text28    10     10         1\n text29     9     16         2\n text30    29     29         1\n text31    16     18         1\n text32     7      7         1\n text33    25     32         1\n text34     3      3         1\n text35    29     39         2\n text36    10     15         2\n text37     5      5         1\n text38    19     19         3\n text39   104    158         7\n text40     1      1         1\n text41    27     32         3\n text42    18     21         2\n text43    26     34         1\n text44     8      8         1\n text45     3      3         1\n text46    13     18         3\n text47    11     11         2\n text48     4      4         1\n text49    39     51         2\n text50    23     26         2\n text51    26     33         5\n text52     6      6         1\n text53    16     16         2\n text54    60     80         4\n text55    19     22         4\n text56    11     13         1\n text57    11     16         2\n text58    42     64         4\n text59    14     19         2\n text60    52     67         8\n text61    20     21         2\n text62     5      5         1\n text63    82    125         8\n text64    16     16         2\n text65    21     25         3\n text66    30     36         5\n text67    23     25         1\n text68    20     23         1\n text69    22     27         1\n text70    31     40         4\n text71    64     94         2\n text72    22     31         3\n text73    35     42         2\n text74     7      7         1\n text75     5      5         1\n text76     8      8         1\n text77    10     10         1\n text78    42     52         4\n text79    14     14         1\n text80    32     33         2\n text81     5      5         1\n text82     3      3         1\n text83    18     19         1\n text84     8      8         1\n text85    37     45         4\n text86    35     41         1\n text87    13     14         1\n text88    38     48         4\n text89    39     48         8\n text90    12     13         1\n text91     7      9         1\n text92    22     27         4\n text93     8     12         2\n text94    19     19         2\n text95    22     23         1\n text96    16     17         1\n text97    24     27         1\n text98    13     15         2\n```\n:::\n\n```{.r .cell-code}\ndf_nfl <- read_csv(\"_data/comments_nfl.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 99 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): What crime did he commit?\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndf_nfl<- df_nfl %>%\n  rename(text = \"What crime did he commit?\")\n\n\ncorpus_nfl <- corpus(df_nfl)\n\ncorpusNFL_sum <- summary(corpus_nfl)\ncorpusNFL_sum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 99 documents, showing 99 documents:\n\n   Text Types Tokens Sentences\n  text1     4      4         1\n  text2    66     83         7\n  text3    44     52         4\n  text4     8      8         1\n  text5    24     26         3\n  text6     7      7         1\n  text7    21     23         1\n  text8    39     43         2\n  text9     8      8         1\n text10    36     41         3\n text11    26     28         2\n text12    19     20         1\n text13     8      8         1\n text14    23     33         2\n text15    29     35         5\n text16    32     46         3\n text17    26     30         3\n text18    17     22         1\n text19     5      5         1\n text20     3      3         1\n text21    21     29         1\n text22    24     26         2\n text23    21     23         3\n text24    41     56         4\n text25    14     14         1\n text26     7      7         1\n text27    20     20         1\n text28     9      9         2\n text29     8      9         1\n text30     6      7         2\n text31    31     43         2\n text32    29     35         2\n text33    20     24         1\n text34    12     12         1\n text35     8      8         2\n text36    15     15         1\n text37    20     21         2\n text38    33     37         2\n text39    10     10         1\n text40    20     22         1\n text41    10     10         1\n text42    39     47         1\n text43    15     15         1\n text44    15     20         1\n text45    65     82         2\n text46    19     21         3\n text47    12     12         2\n text48    13     15         1\n text49    10     10         1\n text50     7      7         1\n text51    24     26         1\n text52     4      4         1\n text53    23     27         2\n text54    20     21         2\n text55    19     21         3\n text56    12     12         1\n text57    73     92         5\n text58   117    219        17\n text59    12     15         2\n text60    30     36         3\n text61    57     73         6\n text62     8      8         1\n text63    26     30         3\n text64     1      1         1\n text65    19     20         2\n text66    32     37         4\n text67     7      7         1\n text68    15     15         1\n text69    60     82         1\n text70    40     49         7\n text71     4      4         1\n text72    11     12         1\n text73    91    125         7\n text74     9      9         1\n text75    13     13         1\n text76    31     39         2\n text77    18     19         1\n text78     7      7         1\n text79     6      9         1\n text80    13     14         2\n text81    44     56         5\n text82    19     19         1\n text83     9      9         1\n text84    25     42         2\n text85    22     26         3\n text86    18     21         2\n text87    37     43         1\n text88     7      7         1\n text89    22     22         1\n text90    53     67         3\n text91     9      9         1\n text92    57     73         4\n text93    80    138         9\n text94    43     63         3\n text95    21     25         1\n text96     4      4         1\n text97    23     25         2\n text98     9      9         1\n text99     9      9         1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_nba_tokens <- tokens(corpus_nba)\n\ncorpus_nba_tokens <-  tokens(corpus_nba, \n    remove_punct = T,\n    remove_numbers = T,\n    remove_symbols =T)\n\nprint(corpus_nba_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 98 documents.\ntext1 :\n [1] \"I\"        \"can\"      \"#39\"      \"t\"        \"believe\"  \"this\"    \n [7] \"is\"       \"actually\" \"a\"        \"debate\"   \"in\"       \"America\" \n[ ... and 126 more ]\n\ntext2 :\n [1] \"She\"    \"acts\"   \"like\"   \"she\"    \"is\"     \"owed\"   \"stuff\"  \"br\"    \n [9] \"info\"   \"that's\" \"none\"   \"of\"    \n[ ... and 63 more ]\n\ntext3 :\n [1] \"My\"       \"question\" \"is\"       \"why\"      \"do\"       \"they\"    \n [7] \"allow\"    \"people\"   \"like\"     \"her\"      \"to\"       \"be\"      \n[ ... and 23 more ]\n\ntext4 :\n [1] \"That\"    \"is\"      \"NOT\"     \"why\"     \"we\"      \"are\"     \"here\"   \n [8] \"I'm\"     \"rolling\" \"The\"     \"way\"     \"she\"    \n[ ... and 3 more ]\n\ntext5 :\n[1] \"Who\"   \"did\"   \"this\"  \"man\"   \"sleep\" \"with\" \n\ntext6 :\n [1] \"I\"         \"#39\"       \"m\"         \"with\"      \"Candace\"   \"Owens\"    \n [7] \"Basicslly\" \"shouldnt\"  \"even\"      \"hire\"      \"women\"     \"because\"  \n[ ... and 48 more ]\n\n[ reached max_ndoc ... 92 more documents ]\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}