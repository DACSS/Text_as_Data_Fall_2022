{
  "hash": "d77ef3bebe95ec0da415f404e2327883",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 3: Preprocessing Medical Textbooks\"\nauthor: \"Emily Miller\"\ndesription: \"Processing medical textbooks and visuallizing using word clouds.\"\ndate: \"10/16/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw3\n  - blogpost3\n  - Emily Miller\n  - internetarchivebooks\n  - internetarchive\n  - tidyverse\n  - tidytext\n  - stringdist\n  - fuzzyjoin\n  - quanteda\n---\n\n\n\n\n# Recap\n\n**Questions:**\n\n-   Is there a difference in language use between penises and vulvas?\n-   Is there a change in language use over time?\n\n**How:** Compare language use in anatomy textbooks\n\n# Overview\nThis blog post goes over the steps I take to clean my dataset and visualize my work. Because I had to go through so many steps for the initial cleaning, I decided to dedicate a blog post to it. I follow generally the following outline:\n\n1. Dataset initial cleaning, where I remove punctuation and extra characters\n2. Cleaning using dictionaries that I build from book indexes\n+ Hunspell for identifying mispelled words\n3. Visualizing with a word cloud\n\n# Dataset Initial Cleaning\n\n### PDFs from the Inter-library Loan\n\nI downloaded a number of other books that I couldn't retrieve from the Internet Archive through UMass's Internlibrary Loan system. Most of the metadata (title, dates) are contained in the filenames of the PDFs, so I add this information in the code below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp<- tibble(raw_files = list.files(path = \"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/unabsorbed_pdfs\"))\n#add information about the dates of the files where the file doesn't describe it\npdfs<- temp %>% \n  rowwise %>% \n  mutate(files = str_remove_all(raw_files, \"_\"),\n         files = str_replace_all(files, \"\\\\s+\", \"_\"),\n         dates = str_extract(files, \"(?<=\\\\()\\\\d{4}(?=\\\\))\"),\n         dates = as.numeric(dates)) %>% \n  mutate(dates = case_when(\n    str_detect(files, \"Advances_in_Anatomy,_Embryology\") ~ 2004,\n    str_detect(files, \"\\\\(Current_Clinical_Urology\")~ 2007,\n    T~dates\n  ))\n\n#clean the titles\npdfs<- pdfs %>% \n  mutate(titles = str_extract(files, \"(?<=-).*(?=\\\\,|-.*-|-\\\\w|\\\\()\"),\n         titles = str_replace_all(titles, \"\\\\.\" , \"_\"),\n         titles = str_replace_all( titles, \"Evidence-Based\", \"Evidence_Based\"),\n         titles = str_replace(titles, \"-|,\", \" \"),\n         titles = ifelse(str_detect(titles, \" \"), \n                         str_extract(titles, \".*(?= )\"), titles),\n         titles = str_replace_all(titles, \"_+\" , \" \"),\n         titles = trimws(titles, which = \"both\"), \n         titles = tolower(titles))\n\npdfs<-pdfs %>% \n  mutate(titles = case_when(\n    raw_files == \"3-540-37363-2.pdf\" ~ \"applied physiology in intensive care medicine\",\n    raw_files == \"978-1-59745-142-0.pdf\" ~\"obstetrics in family medicine\",\n    raw_files == \"978-1-59745-469-8.pdf\" ~ \"womens health in clinical practice\",\n    raw_files == \"b138098.pdf\" ~ \"scheins common sense emergency abdominal surgery\",\n    T ~ titles\n  ), \n  dates = case_when(\n    raw_files == \"b138098.pdf\" ~ 2005,\n    raw_files == \"978-1-59745-469-8.pdf\" ~ 2008,\n    raw_files == \"978-1-59745-142-0.pdf\" ~ 2006,\n    raw_files == \"3-540-37363-2.pdf\" ~2006,\n    T~ dates\n  ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npdfs %>% head\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 4\n# Rowwise: \n# … with 4 variables: raw_files <chr>, files <chr>, dates <dbl>, titles <chr>\n```\n:::\n:::\n\n\nNext, I get the names of the files to read into R from my directories. Because I needed to remove the punctuation of the file names for the OCR (it was throwing some errors), I have to also modify the filenames in R so I can join them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdfs1<- pdfs %>% \n  mutate(temp = str_replace_all(raw_files, c(\"\\\\.\" = \"QQQ\",\n                                         \"[[:punct:]]|\\\\s+\" = \"\",\n                                         \"QQQ\" = \".\",\n                                         \"\\\\.pdf\" = \"\")),\n         temp2 = str_replace_all(raw_files, c(\"\\\\.pdf$\" = \"\")),\n         temp2 =ifelse(str_detect(titles, \"clinical anatomy a revision\"),\n                       \"HaroldEllisClinicalAnatomyARevisionandAppliedAnatomyforClinicalStudents\", temp2)) %>% \n  pivot_longer(matches(\"temp\"), values_to = \"files1\") %>% \n  select(titles, dates, files =files1) %>% \n  unique\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `pivot_longer_spec()`:\n! Can't combine `temp` <character> and `temp2` <logical>.\n```\n:::\n\n```{.r .cell-code}\ntemp<- tibble(raw_files = list.files(path = \"/Users/emmanuelmiller/Desktop/thetext\",\n                                     full.names = T)) %>% \n  mutate(files = str_extract(raw_files, \"(?<=thetext.).+(?=_Page)\"),\n         files = str_remove(files, \"edite*d*_*\"))\n\npdfs1<- pdfs1 %>% \n  full_join(., temp)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in full_join(., temp): object 'pdfs1' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npdfs1 %>% head\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'pdfs1' not found\n```\n:::\n:::\n\n\nI made an excel sheet and documented the volume and edition numbers of the books I use. I could extract the metadata using code for this part of the dataset, but for the sake of consistency with the IA dataset, and ensuring correct metadata, I decided to do this portion by hand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbook_metadata <- readxl::read_excel(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/book_metadata.xlsx\") \n```\n\n::: {.cell-output .cell-output-error}\n```\nError: `path` does not exist: '/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/book_metadata.xlsx'\n```\n:::\n\n```{.r .cell-code}\npdfs2<-pdfs1 %>% \n  left_join(., book_metadata %>% select(-titles, -dates, -type), \n            by = \"files\") %>% \n  group_by(titles, dates) %>% \n  fill(matches(\"edition|volume|part\"), .direction = c(\"updown\")) %>% \n  ungroup\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., book_metadata %>% select(-titles, -dates, -type), : object 'pdfs1' not found\n```\n:::\n\n```{.r .cell-code}\npdfs2 %>% head %>% kable\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'pdfs2' not found\n```\n:::\n:::\n\n\n\n### Internet Archive Data\n\nLike the inter-library loan dataset, I also needed to extract metadata for the files I downloaded. I decided to do this manually because I had not yet finished cleaning the OCR and I wanted to ensure that I didn't have duplicate books. I found that 4 of the books had incorrect dates and 2 of the books were from before 1875. The only reason why I found metadata for these books was from a note from the library in the back of the book. I decided to carry out metadata-searching manually.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntexts<- book_metadata %>% \n  filter(type == \"ia\") %>% \n  select(-type)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., type == \"ia\"): object 'book_metadata' not found\n```\n:::\n\n```{.r .cell-code}\n###make sure titles match\n# texts<- dat8 %>% \n#   select(titles, dates, files=hrefs) %>% \n#   right\n\ntemp<- tibble(raw_files = list.files(path = \"/Users/emmanuelmiller/Desktop/text_ia\",\n                                     full.names = T)) %>% \n  mutate(files = str_extract(raw_files, \"(?<=text_ia.).+(?=_Page)\"),\n         files = str_remove(files, \"edits_|edite*d*_*\"))\n\ntexts<- texts %>% \n  full_join(., temp)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in full_join(., temp): object 'texts' not found\n```\n:::\n:::\n\n\nI combine both the inter-library loan and the internet archive datasets. This will help me \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# texts %>% names\n# pdfs2 %>% names\n\ntexts<- texts %>% \n  mutate(type = \"ia\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., type = \"ia\"): object 'texts' not found\n```\n:::\n\n```{.r .cell-code}\npdfs2<- pdfs2 %>% \n  relocate(raw_files, .after= \"part\") %>% \n  mutate(type = \"ill\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in relocate(., raw_files, .after = \"part\"): object 'pdfs2' not found\n```\n:::\n\n```{.r .cell-code}\ndat<- bind_rows(texts, pdfs2) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in list2(...): object 'texts' not found\n```\n:::\n\n```{.r .cell-code}\ndat<-dat %>% \n    select(-files) %>% \n  filter(!is.na(raw_files))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., -files): object 'dat' not found\n```\n:::\n\n```{.r .cell-code}\n# bind_rows(texts, pdfs1) %>% \n#   arrange(type, files) %>% \n#   select(type, titles, dates, files) %>%unique %>% \n#   openxlsx::write.xlsx(., file = \"temp.xlsx\")\n```\n:::\n\n\nFinally, I added page numbers and read in the files. I saved this because it takes a long time to load in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat<- dat %>% \n  mutate(page = str_extract(raw_files, \"(?<=Page_)\\\\d+\"),\n         page = as.numeric(page)) \n\ndat<-dat %>% \n  mutate(dat =map(raw_files, ~read_lines(.)))\n\nsave(dat, file = \"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/dat.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/dat.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file '/\nUsers/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/\ndat.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\ndat %>% head \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'dat' not found\n```\n:::\n\n```{.r .cell-code}\n# save(dat, file = \"dat.RData\")\n```\n:::\n\n\nBecause the separator was a new line, there are multiple rows for 1 page. I combine them into 1 page here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat<- dat %>% \n  select(-raw_files) %>% \n  mutate(dat = map(dat,~str_c(., collapse = \"\\n\"))) %>% \n  unnest_longer(dat)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., -raw_files): object 'dat' not found\n```\n:::\n\n```{.r .cell-code}\ndat %>% tail %>% mutate(dat = str_sub(dat, 1, 100))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tail(.): object 'dat' not found\n```\n:::\n:::\n\n\n# Cleaning\n\nSome of the files in the Internet Archive dataset have a notice from google saying that their copyrights have expired. Because these aren't part of the actual dataset, I remove them. I also remove empty pages, which I assume were because of some images being printed on one side. It may also be possible that these were censored, but it is unclear. If this were the case, it may affect the accuracy of the conclusions I draw in this pilot study. In a larger work, I would recommend obtaining all books from interlibrary loan as PDFs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1<- dat %>% \n  filter(!str_detect(dat, \"(?i)google\")) %>% \n  mutate(dat = str_remove(dat, \"^\\\\s+$\")) %>% \n  na_if(\"\") %>%\n  filter(!is.na(dat))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., !str_detect(dat, \"(?i)google\")): object 'dat' not found\n```\n:::\n:::\n\n\nI noticed that there were many whitespaces on each page. I decided to remove whitespaces since I would like to do sentence-level analyses. I also removed extraneous punctuation. Because I plan to tokenize using the Quanteda package, I will keep some essential punctuation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1<- dat1 %>% \n  mutate(dat = str_replace_all(dat, c(\n                       \"\\\\t\" = \" \",\n                       \"\\\\n\" = \" \",\n                       \"\\\\r\" = \" \",\n                       \"\\\\|\" = \"\",\n                       \"\\\\d+\" = \" \",\n                       \"(?<=\\\\w)\\\\s*-\\\\s*(?=\\\\w)\" = \"IWASAHYPHEN\",\n                       \"'\" = \"IWASASINGLEQUOTE\",\n                       \"\\\\.\" = \"IWASAPERIOD\",\n                       \",\" = \"IWASACOMMA\",\n                       \"[[:punct:]]+\" = \" \",\n                       \"\\\\s+\" = \" \"\n                       )),\n         dat = str_remove_all(dat, \"[^[:alnum:]\\\\s]\"),\n         dat = str_replace_all(dat, c(\n           \"IWASAHYPHEN\" = \"-\",\n           \"IWASAPERIOD\" = \".\",\n           \"IWASACOMMA\" = \",\",\n           \"IWASASINGLEQUOTE\" = \"'\"\n         ))\n                     )\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = str_replace_all(dat, c(`\\\\t` = \" \", `\\\\n` = \" \", : object 'dat1' not found\n```\n:::\n\n```{.r .cell-code}\ndat1 %>% \n  mutate(dat = str_sub(dat, 1, 100)) %>% \n  group_by(titles, dates) %>% \n  slice(1) %>% \n  ungroup %>% \n  kable\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = str_sub(dat, 1, 100)): object 'dat1' not found\n```\n:::\n:::\n\n\nI noticed that there are many single characters that are a result of either noise being transcribed or because of images that were transcribed. I remove these single characters and duplicate periods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1<- dat1 %>% \n  mutate(dat = stri_replace_all_regex(dat, paste0(\"(?<=\\\\.|\\\\s)\", \n                                                  letters[1:26],\n                                                  \"(?=\\\\.|\\\\s)\"), rep(\"\", 26),\n                                        vectorize_all = F),\n         dat = stri_replace_all_regex(dat, paste0(\"\\\\b\", \n                                                  letters[1:26], \n                                                  \"\\\\b\"), rep(\"\", 26),\n                                        vectorize_all = F)) %>% \n  mutate(dat = stri_replace_all_regex(dat, paste0(\"(?<=\\\\.|\\\\s)\", \n                                            LETTERS[1:26],\n                                            \"(?=\\\\.|\\\\s)\"), rep(\"\", 26),\n                                        vectorize_all = F), \n         dat = stri_replace_all_regex(dat, paste0(\"\\\\b\", \n                                            LETTERS[1:26],\n                                            \"\\\\b\"), rep(\"\", 26), \n                                        vectorize_all = F)) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = stri_replace_all_regex(dat, paste0(\"(?<=\\\\.|\\\\s)\", : object 'dat1' not found\n```\n:::\n\n```{.r .cell-code}\ndat1<-dat1 %>% \n  mutate(dat = str_replace_all(dat, c(\"\\\\s+\" = \" \",\n                                          \"\\\\.+\" = \".\",\n                                          \"(\\\\s?\\\\.)+\" = \".\",\n                                      \"^\\\\s+$\" = \"\"\n                                          )\n                                 )) %>% \n  na_if(\"\") %>% \n  filter(!is.na(dat))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = str_replace_all(dat, c(`\\\\s+` = \" \", `\\\\.+` = \".\", : object 'dat1' not found\n```\n:::\n\n```{.r .cell-code}\ndat1 %>% \n  mutate(dat = str_sub(dat, 1, 100)) %>% \n  group_by(titles, dates) %>% \n  slice(1) %>% \n  ungroup%>% \n  kable\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = str_sub(dat, 1, 100)): object 'dat1' not found\n```\n:::\n:::\n\n\n# Cleaning Using Dictionaries\n### Overview\n\nMany pages contain letters that aren't reflective of the text. This is partly a result of attempting to read diagrams with words or places where there was lots of background noise that couldn't be removed. Some of the older texts have words that were misspelled. In order to further remove some of these strings of characters, I will use the Hunspell package to check whether a word is misspelled, and if it is, I remove it from the corpus. I will go into this further in the next section and that I will need to take this into consideration if using certain analytical methods such as machine learning. One issue I noticed with this approach is that some words that are in the medical lexicon are marked as misspelled. In order to counter this, I incorporate words from medical books to help with this process.\n\nI used selection criteria as follows. I decided to choose books that have been OCR'd by a human and therefore are more certainly reflective the original text, or if they already came in a text format like epub. For the quantity of books that I use I had to consider the availability of this high-quality OCR data as well as my research question. I am looking at 2 different time periods, pre-1970s and post-1970s, and my corpus is rather small with roughly 30 books per time period. Unfortunately, there are not many well-OCR'd older medical texts widely available on the internet. After speaking with Dr. Song, I decided that I would use 2 books to build this medical dictionary (so 1 book per time period). \n\nI also considered 2 different approaches for getting just medical vocabulary. I first considered tokenizing the whole book, removing stopwords, and using the whole thing as a medical dictionary. I found that using this approach, in combination with fuzzy-matching (which I do later), allowed shorter ~3 letter words which were wrongly transcribed to be marked as \"good\". I then considered using only the index of the book. This automatically selects keywords that are reviewed in the text and are primarily related to anatomy or pathology. I decided on the latter approach as it was more stringent and allowed fewer false-positives.\n\nI review the cleaning and application of this method inn 3 sections.\n\n### Pre-1970's Text Acquisition and Cleaning\n\nFor the pre-1970s text, I reviewed project Gutenberg because their books have been transcribed by a submitter. I searched \"anatomy\" and \"gynecology\". I considered books only relating to humans and targeted towards the medical profession. I chose \"Structure and Functions of The Body\" by Annette Fiske, published in 1911. This is one of the few that reviewed vaginal anatomy. Typographical changes were made by the transcriber, which adds a layer of review to this transcriptions.\n\nI decided to webscrape the text file from the Project Gutenberg website as this was less trouble than fighting with the gutenbergr R package or Firefox on downloading the text version.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make dictionary -- gutenberg\npolite::bow(\"https://gutenberg.org/files/64754/64754-0.txt\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<polite session> https://gutenberg.org/files/64754/64754-0.txt\n    User-agent: polite R package\n    robots.txt: 44 rules are defined for 30 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n:::\n\n```{.r .cell-code}\nguten<- rvest::read_html(\"https://gutenberg.org/files/64754/64754-0.txt\") %>% \n  rvest::html_nodes(.,xpath=\"/html/body/*\") %>% \n  rvest::html_text()\n```\n:::\n\n\nNext I select only the index portion of the text and remove extraneous punctuation and words (like \"et seq\" which is a way to reference page numbers). I also made duplicate words which had both \"'s\" and without.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nguten1<-tibble(book = guten) %>% \n  mutate(index = str_extract(book, \"(?<=INDEX\\\\.)[\\\\s\\\\S]*(?=SAUNDERS. BOOKS FOR NURSES)\")) %>% \n  select(-book)\n\nguten2<- guten1 %>% \n  mutate(index = str_replace_all(index, c(\".et seq..|.et seq.\" = \"\",\n                                          \"\\\\d+\" = \"\",\n                                          \",\" = \"\",\n                                          \"æ\" = \"ae\"\n                                          ))) %>% \n  mutate(other0 = str_extract(index, \"\\\\w+(?='s)\"),\n         other = str_remove_all(index, \"-\"),\n         across(everything(), ~str_replace_all(., c(\"[:punct:]+\"= \"\")))) %>% \n  pivot_longer(everything(), values_to = \"index\") %>% \n  select(-name) %>% \n  na_if(\"\") %>% \n  filter(!is.na(index)) %>% \n  separate_rows(index, sep = \"\\\\n\") %>% \n  mutate(index = trimws(index, which = \"both\"),\n         nchar = nchar(index))\n\nguten2 %>% slice(1:20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   index                             nchar\n   <chr>                             <int>\n 1 \"\"                                    0\n 2 \"\"                                    0\n 3 \"\"                                    0\n 4 \"Abdomen\"                             7\n 5 \"muscles\"                             7\n 6 \"nerves\"                              6\n 7 \"regions\"                             7\n 8 \"contents\"                            8\n 9 \"Abdominal aorta\"                    15\n10 \"Abducens nerve\"                     14\n11 \"Abscess\"                             7\n12 \"Absorbent vessels or lymphatics\"    31\n13 \"Absorption of food\"                 18\n14 \"in intestines\"                      13\n15 \"in mouth\"                            8\n16 \"in stomach\"                         10\n17 \"Accommodation of eye\"               20\n18 \"Acetabulum\"                         10\n19 \"Acromion process\"                   16\n20 \"Adams apple\"                        11\n```\n:::\n:::\n\n\nAfter, I tokenize each word in the index and remove stopwords (such as \"of\") from the corpus. I then reviewed words that were 3-4 characters to confirm that they were medically related. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nguten3<-guten2 %>% \n  mutate(index = tolower(index),\n         index = str_replace_all(index, c(\"'s\" = \"\",\n                                          \"'\" = \"\",\n                                          '\\\\\"' = ''))) %>% \n  separate_rows(index, sep = \" \") %>% \n  mutate(index = trimws(index, which = \"both\"),\n         nchar = nchar(index)) %>% \n  na_if(\"\") %>% \n  filter(!is.na(index) & !(index %in% tm::stopwords()) ) %>% \n  unique\n\nguten3 %>% arrange(nchar) %>% slice(1:20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   index nchar\n   <chr> <int>\n 1 os        2\n 2 eye       3\n 3 air       3\n 4 arm       3\n 5 leg       3\n 6 bow       3\n 7 hip       3\n 8 ear       3\n 9 end       3\n10 fat       3\n11 sac       3\n12 pia       3\n13 lid       3\n14 pus       3\n15 red       3\n16 sty       3\n17 tie       3\n18 vas       3\n19 wry       3\n20 food      4\n```\n:::\n:::\n\n\n### Post 1970's Text Acquisition and Cleaning\n\nTo acquire a post-1970's text, I searched \"gynecology\" and \"anatomy\" with tags \"medical\" in the UMass Amherst library catalog. I then searched for \"epub\" as the format. I chose \"Clinically Oriented Anatomy 7th edition\" by Keith Moore et al., published in 2014. I converted the epub to a text document using convertio.co (which is a website that I found by googling \"epub to txt convert\"). I downloaded the converted book and selected only the index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoore<- read_file(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/Keith-L.-Moore-PhD-FIAC-FRSM-FAAA_-Anne-M.-R.-Agur-BSc-_OT_-MSc-PhD_-Arthur-F.txt\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: '/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/Keith-L.-Moore-PhD-FIAC-FRSM-FAAA_-Anne-M.-R.-Agur-BSc-_OT_-MSc-PhD_-Arthur-F.txt' does not exist.\n```\n:::\n\n```{.r .cell-code}\nmoore1<- tibble(book = moore) %>% \n  mutate(index = str_extract(book, \"(?<=Index)\\\\s+A\\\\s+[\\\\s\\\\S]*$\")) %>% \n  select(-book) %>% \n  separate_rows(index, sep = \"\\\\n\\\\n\") %>% \n  na_if(\"\") %>% \n  filter(!is.na(index))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval_tidy(xs[[j]], mask): object 'moore' not found\n```\n:::\n:::\n\n\nLike the pre-1970's dictionary, I remove punctuation and make both forms of words with \"'s\" and without.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoore2<- moore1 %>% \n  mutate(other0 = str_extract(index, \"\\\\w+(?='s)\"),\n         other = str_remove_all(index, \"-\"),\n         across(everything(), ~str_replace_all(., c(\"[:punct:]+\"= \"\",\n                                                    \"\\\\d+\"= \"\"))),\n         across(everything(), ~tolower(.))) %>% \n  pivot_longer(everything(), values_to = \"index\") %>% \n  select(-name) %>% \n  na_if(\"\") %>% \n  filter(!is.na(index))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., other0 = str_extract(index, \"\\\\w+(?='s)\"), other = str_remove_all(index, : object 'moore1' not found\n```\n:::\n:::\n\n\nFinally, I tokenize the index and remove stopwords. I reviewed words that were 2-3 characters long to confirm they were medically related. I found that there are times when French (such as \"Le\" for \"Le Fort fracture\") or acronyms are used (such as \"ap\" for anteposterior or \"pa\" posteroanterior). Some were remains of numbering (such as \"1st\" being \"st\"). Others were single letters because of being contractions with numbers (\"c1\", which is the first cervical vertebra, was converted to \"c\") or because single letters were written at the top of each page. I also noticed roman numerals were included because of the numbering of the cranial nerves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoore3<- moore2 %>% \n  separate_rows(index, sep = \"\\\\s+\") %>% \n  na_if(\"\") %>% \n  unique %>% \n  mutate(nchar = nchar(index))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in separate_rows(., index, sep = \"\\\\s+\"): object 'moore2' not found\n```\n:::\n\n```{.r .cell-code}\nmoore3 %>% arrange(nchar) %>% slice(1:40)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in arrange(., nchar): object 'moore3' not found\n```\n:::\n:::\n\n\nI ended up removing words which had fewer than 3 characters because these short words would already be removed with earlier cleaning of the corpus, and because they usually co-occur with more substantive nouns within the index (ex. the whole string containing \"c1\" was \"c1 spine\" or \"l2 vertebra\" meaning the second lumbar vertebra). I also found that short strings in a dictionary tended to lead to false-positive words (that were made by Tesseract misreading grainy pictures).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoore3<-moore3 %>% \n  filter(nchar > 2) %>% \n  filter(!(index %in% letters) & \n           !is.na(index) & \n           !(index %in% tm::stopwords()) &\n           !(index %in% c(\"vs\", tolower(as.roman(1:100)))))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., nchar > 2): object 'moore3' not found\n```\n:::\n\n```{.r .cell-code}\nmoore3 %>% arrange(nchar) %>% slice(1:10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in arrange(., nchar): object 'moore3' not found\n```\n:::\n:::\n\n\nI also included a dictionary, the Oxford Concise Color Medical Dictionary, 3rd edition from 2010. I also converted this epub to text using the same converter as above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noxford<-read_file(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/Oxford-Quick-Reference-Jonathan.txt\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: '/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/Oxford-Quick-Reference-Jonathan.txt' does not exist.\n```\n:::\n:::\n\n\nLike the other 2 examples, words from the index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noxford1<-tibble(dat = oxford) %>% \n  mutate(index = str_extract(dat, \"A & E medicine[\\\\s\\\\S]+zymotic disease(?=\\\\s{4}OXFORD QUICK)\"),#\n         index1 = str_extract(dat, \"Affixesabdomin.[\\\\s\\\\S]+(?=a- \\\\(an-\\\\) prefix)\"),\n         index1 = str_replace(dat, \"Affixesabdomin\", \"Affixes \\n\\nabdomin\")) %>% \n  select(-dat) %>% \n  pivot_longer(everything(), values_to = \"index\") %>% \n  select(-name) %>% \n  mutate(index = str_split(index, pattern = \"\\n\\n\")) %>% \n  unnest_longer(index) %>% \n  na_if(\"\") %>% \n  filter(!is.na(index))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval_tidy(xs[[j]], mask): object 'oxford' not found\n```\n:::\n\n```{.r .cell-code}\noxford1<-oxford1 %>% slice(14: nrow(oxford1))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in slice(., 14:nrow(oxford1)): object 'oxford1' not found\n```\n:::\n\n```{.r .cell-code}\noxford1 %>% head\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'oxford1' not found\n```\n:::\n:::\n\n\nI retrieved some affixes and abbreviations included here. I chose to remove them from the subset of words I'll use for the dictionary because with the fuzzy matching they match words that are from white noise on the page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noxford_affix<- oxford1 %>% \n  filter(str_detect(index, \"^-\\\\w+$|^\\\\w+-$\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., str_detect(index, \"^-\\\\w+$|^\\\\w+-$\")): object 'oxford1' not found\n```\n:::\n\n```{.r .cell-code}\noxford_abbrev <- oxford1 %>% \n  mutate(index2 = str_split(index, \"[[:punct:]\\\\s]+\")) %>% \n  unnest_longer(index2) %>% \n  filter(str_detect(index2, \"^[[:upper:]]{2,}$\") & \n           !(index2 %in% as.roman(1:50)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., index2 = str_split(index, \"[[:punct:]\\\\s]+\")): object 'oxford1' not found\n```\n:::\n\n```{.r .cell-code}\noxford2<- oxford1 %>% \n  left_join(.,oxford_abbrev) %>% \n  filter(!str_detect(index, \"^-\\\\w+$|^\\\\w+-$\") & \n           is.na(index2)) %>% \n  select(-index2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., oxford_abbrev): object 'oxford1' not found\n```\n:::\n\n```{.r .cell-code}\noxford2 %>% slice(10:15)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in slice(., 10:15): object 'oxford2' not found\n```\n:::\n:::\n\n\nLike previously, I make multiple matching for possesive nouns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noxford2<- oxford2 %>% \n  mutate(other0 = str_extract(index, \"\\\\w+(?='s)\"),\n         other = str_replace_all(index, \"[[:punct:]]\", \" \"),\n         across(everything(), ~str_replace_all(., c(\"[:punct:]+\"= \"\",\n                                                    \"\\\\d+\"= \"\"))),\n         across(everything(), ~tolower(.))) %>% \n  pivot_longer(everything(), values_to = \"index\") %>% \n  select(-name) %>% \n  na_if(\"\") %>% \n  filter(!is.na(index)) %>% unique\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., other0 = str_extract(index, \"\\\\w+(?='s)\"), other = str_replace_all(index, : object 'oxford2' not found\n```\n:::\n\n```{.r .cell-code}\noxford2 %>% slice(10:15)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in slice(., 10:15): object 'oxford2' not found\n```\n:::\n:::\n\n\nfinally I remove stop words and roman numerials and strings shorter than 2 characters (for the same reason as before).\n\n\n::: {.cell}\n\n```{.r .cell-code}\noxford3<- oxford2 %>% \n  separate_rows(index, sep = \"\\\\s+\") %>% \n  na_if(\"\") %>% \n  unique %>% \n  mutate(nchar = nchar(index))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in separate_rows(., index, sep = \"\\\\s+\"): object 'oxford2' not found\n```\n:::\n\n```{.r .cell-code}\noxford3<-oxford3 %>% \n  filter(nchar > 2) %>% \n  filter(!(index %in% letters) & \n           !is.na(index) & \n           !(index %in% tm::stopwords()) &\n           !(index %in% c(\"vs\", tolower(as.roman(1:100)))))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., nchar > 2): object 'oxford3' not found\n```\n:::\n\n```{.r .cell-code}\noxford3 %>% slice(10:15)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in slice(., 10:15): object 'oxford3' not found\n```\n:::\n:::\n\n\n### Applying Dictionaries\n\nIn order to apply the dictionary, I first tokenize my corpus and save the tokens as a vector. I found that using vectors is faster than using a tibble. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2<- dat1 %>% \n  mutate(dat = strsplit(dat, \".\", fixed = T)) %>%\n  unnest_longer(dat) %>% \n  tidytext::unnest_tokens(., \"tokens\", dat, \n                          token = \"words\", \n                          format = \"text\",\n                          drop = F) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dat = strsplit(dat, \".\", fixed = T)): object 'dat1' not found\n```\n:::\n:::\n\n\nNext I make a separate table where words can be checked against US, Australian, Canadian, and UK English dictionaries. I also make a medical dictionary comprised of the texts from the 2 timepoints and add it to the main dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp<- unique(dat2$tokens)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in unique(dat2$tokens): object 'dat2' not found\n```\n:::\n\n```{.r .cell-code}\n# hunspell::list_dictionaries()\n\ndicts<- tibble(value = temp,\n              en_us = hunspell_check(temp, \n                                     dict = hunspell::dictionary(\"en_US\")), \n              en_au = hunspell_check(temp, dict = \n                                       hunspell::dictionary(\"en_AU\")),\n              en_ca = hunspell_check(temp, dict = \n                                       hunspell::dictionary(\"en_CA\")),\n              en_gb = hunspell_check(temp, dict = \n                                       hunspell::dictionary(\"en_GB\")))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in hunspell_check(temp, dict = hunspell::dictionary(\"en_US\")): is.character(words) is not TRUE\n```\n:::\n\n```{.r .cell-code}\nmed_dict<- rbind(guten3, moore3, oxford3) %>% \n  select(-nchar, value = index) %>% \n  unique\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in rbind(deparse.level, ...): object 'moore3' not found\n```\n:::\n\n```{.r .cell-code}\ndicts1<- dicts %>% \n  left_join(., med_dict %>% mutate(med =\"med\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., med_dict %>% mutate(med = \"med\")): object 'dicts' not found\n```\n:::\n:::\n\n\nI first get a snapshot of the number of words that were marked as \"good\" by both the medical dictionaries and by hunspell. There are 3x more words that don't match the dictionaries than words that do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndicts2<- dicts1 %>% \n  mutate(med = !is.na(med)) %>% \n  rowid_to_column() %>% \n  pivot_longer(-c(rowid, value), values_to = \"dict_sum\") %>% \n  group_by(value) %>% \n  mutate(good = ifelse(any(dict_sum), \"yes\", \"no\")) %>% \n  ungroup\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., med = !is.na(med)): object 'dicts1' not found\n```\n:::\n\n```{.r .cell-code}\ndicts2 %>% count(good)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., good): object 'dicts2' not found\n```\n:::\n:::\n\n### Medical Dictionaries: Fuzzy Join\n\nIn order to use fuzzy matching, I need to reduce the amount of time the algorithm takes because otherwise my computer crashes. One way I do this is by filtering a certain length of word. I first looked at the overall distribution of character lengths.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fuzzyjoin)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(fuzzyjoin): there is no package called 'fuzzyjoin'\n```\n:::\n\n```{.r .cell-code}\nfoo0<- dicts2 %>% \n  select(value, good) %>% \n  unique() %>% \n  mutate(nchar = nchar(value))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., value, good): object 'dicts2' not found\n```\n:::\n\n```{.r .cell-code}\nfoo0 %>% pull(nchar) %>% log %>% hist(., xlab = \"log(letters)\", \n                                      main = \"Histogram of log(Letters) per Token\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in pull(., nchar): object 'foo0' not found\n```\n:::\n:::\n\n\n I found that some of my words were just strings of characters. After discovering this and reviewing different lengths, I decided on a cutoff of 29 as there seemed to be no real words left in there.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo0 %>% \n  filter(nchar >29) %>% \n  arrange(desc(nchar)) %>% \n  slice(1:10) %>% \n  kable\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., nchar > 29): object 'foo0' not found\n```\n:::\n\n```{.r .cell-code}\nfoo<-foo0%>% \n  filter(good == \"no\" & nchar <29)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., good == \"no\" & nchar < 29): object 'foo0' not found\n```\n:::\n:::\n\n\n\nIn order to join, I used the Longest Common Substring algorithm. this measures the number of characters you would need to add or delete to make 2 strings match. What is special about this algorithm compared to others is that it takes into account the order of the characters matched. This is important because many of my strings will have only 1 character misspelled, and the remainder of the word is the same. I also wanted to extra-penalize substitutions, and this gave a stronger selection than Levenshtein distance. I chose 2 because that gives room for either a substitution, 2 deletions, or 2 additions. I used this to join the medical dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoining <- function(df, dict){\n  df<- df %>% \n    stringdist_left_join(., dict, \n                         method = \"lcs\", \n                         distance_col = \"dist\",\n                         max_dist = 2)\n  return(df)\n}\n\ncorrections<-foo %>% \n  filter(nchar >=3) %>% \n  group_by(nchar) %>% \n  group_split()\n\ncorrections<- map_dfr(corrections, ~joining(.x, med_dict))\n\n# save(corrections, file=\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/medcorrections.RData\")\n```\n:::\n\n\nI then counted the number of matches to the original value, and how many matches were at a given distance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/medcorrections.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file '/\nUsers/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/\nmedcorrections.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\ncorrections1<- corrections %>% \n  add_count(value.x, name = \"n_matches\") %>%\n  add_count(value.x, dist, name = \"n_matches_at_dist\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in add_count(., value.x, name = \"n_matches\"): object 'corrections' not found\n```\n:::\n\n```{.r .cell-code}\ncorrections1 %>% \n  count(n_matches, dist, n_matches_at_dist, is.na(value.y)) %>% \n  slice(1:10) %>% \n  kable\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., n_matches, dist, n_matches_at_dist, is.na(value.y)): object 'corrections1' not found\n```\n:::\n:::\n\n\nIn order to choose the best replacement word, I chose words with only 1 match, or if there were multiple options, only the word at the shortest distance. I then added these changes to the collection of tokens as \"changeme\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrections2<- corrections1 %>% \n  mutate(keep = case_when(\n    n_matches == 1 & !is.na(dist) ~ \"yes\",\n    dist == 1 & n_matches_at_dist == 1 ~ \"yes\",\n    T~\"no\"\n  )) %>% \n  filter(keep == \"yes\") %>% \n  select(value = value.x, value.y) %>% \n  unique\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., keep = case_when(n_matches == 1 & !is.na(dist) ~ \"yes\", : object 'corrections1' not found\n```\n:::\n\n```{.r .cell-code}\ndicts3<- dicts2 %>% \n  select(value, good) %>% \n  unique\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., value, good): object 'dicts2' not found\n```\n:::\n\n```{.r .cell-code}\ndicts3<- dicts3 %>% \n  left_join(., corrections2) %>% \n  mutate(good = ifelse(!is.na(value.y.y), \"changeme\", good))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., corrections2): object 'dicts3' not found\n```\n:::\n\n```{.r .cell-code}\ndicts3 %>% count(good)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., good): object 'dicts3' not found\n```\n:::\n:::\n\nAt this point I realized that words above 25 characters weren't actually words so I removed them if they weren't recognized as words in the first place.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndicts4<- dicts3 %>% \n  mutate(nchar = nchar(value)) %>% \n  filter(!(nchar > 25 & good == \"no\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., nchar = nchar(value)): object 'dicts3' not found\n```\n:::\n\n```{.r .cell-code}\ndicts4 %>% count(good)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., good): object 'dicts4' not found\n```\n:::\n:::\n\n\n### Hunspell Dictionary: Stringdist\n\nThis next part was a lot more difficult because hunspell takes a lot longer than just fuzzy joining. I first looked at the distribution of the remaining mispelled characters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmorefoo<- dicts4 %>% \n  select(value, good) %>% \n  unique() %>% \n  mutate(nchar = nchar(value))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., value, good): object 'dicts4' not found\n```\n:::\n\n```{.r .cell-code}\nmorefoo1<-morefoo%>% \n  filter(good == \"no\" & nchar <14 & nchar > 3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., good == \"no\" & nchar < 14 & nchar > 3): object 'morefoo' not found\n```\n:::\n\n```{.r .cell-code}\nmorefoo%>% count(nchar)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., nchar): object 'morefoo' not found\n```\n:::\n:::\n\n\nAs strings get longer, it takes hunspell much longer to parse them. Words under 4 characters were less accurately matched. I decided to use a range from 4-14 for the first round (as this takes a long time to process). I selected the first word from hunspell's recommended words since that was the best estimate of what the word could have meant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this function is taken from https://stackoverflow.com/questions/3318333/split-a-vector-into-chunks\nchunk <- function(x, n) split(x, sort(rank(x) %% n))\n\ntemp<- morefoo1 %>% \n  select(value) %>% \n  pull(value) %>%  \n  chunk(., n = 84)\n\nhunspell_function<- function(wordlist){\n  wah<- hunspell_suggest(wordlist)\n  wee<- tibble(value = wordlist,\n               sugg = wah) %>% \n    unnest_longer(sugg) %>% \n  group_by(value) %>% \n  slice(1) %>% \n  ungroup\n  \n  return(wee)\n}\n\nweeha<- map_dfr(temp, ~{\n  cat(\"hmm\")\n  hunspell_function(.x)\n  })\n\nsave(weeha, file = \"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/hunspellcorrections.RData\")\n```\n:::\n\n\nNext I did values in the upper ranges.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmorefoo1<-morefoo%>% \n  filter(good == \"no\" & nchar > 14 )\n\ntemp<- morefoo1 %>% \n  select(value) %>% \n  pull(value) %>%  \n  chunk(., n = 2)\n\nupper<- map_dfr(temp, ~{\n  cat(\"hmm\")\n  hunspell_function(.x)\n  })\n\nsave(upper, file = \"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/hunspellcorrections_upper.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/hunspellcorrections_upper.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file '/\nUsers/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/\nhunspellcorrections_upper.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nload(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/hunspellcorrections.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file '/\nUsers/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/\nhunspellcorrections.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nupper %>% head()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'upper' not found\n```\n:::\n:::\n\n\nI then used stringdist to calculate the LCS distance of each suggestion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweeha1<- weeha %>% \n  bind_rows(., upper)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in list2(...): object 'weeha' not found\n```\n:::\n\n```{.r .cell-code}\nweeha2<- weeha1 %>% \n  mutate(dist = stringdist::stringdist(value, sugg, \n                           method = \"lcs\"\n                           ))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., dist = stringdist::stringdist(value, sugg, method = \"lcs\")): object 'weeha1' not found\n```\n:::\n\n```{.r .cell-code}\n# weeha1 %>% filter(dist == 3) %>% View\nweeha2 %>% count(dist) %>% head\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., dist): object 'weeha2' not found\n```\n:::\n:::\n\n\nI decided to remain consistent with earlier datasets and only use a distance of 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweeha3<- weeha2 %>% \n  filter(dist <3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., dist < 3): object 'weeha2' not found\n```\n:::\n\n```{.r .cell-code}\nweeha3 %>% arrange(desc(value)) %>% head()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in arrange(., desc(value)): object 'weeha3' not found\n```\n:::\n:::\n\n\nI next added it to the dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndicts5<-dicts4 %>% \n  left_join(., weeha3) %>% \n  mutate(good = ifelse(!is.na(sugg), \"changeme1\", good))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., weeha3): object 'dicts4' not found\n```\n:::\n\n```{.r .cell-code}\ndicts5 %>% count(good)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., good): object 'dicts5' not found\n```\n:::\n:::\n\nLastly, I decided to see how many words contained medical affixes that I obtained from the Oxford dictionary. I used regex_join to see if any matched.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntempo<- oxford_affix %>% \n  filter(nchar(index) >4) %>% \n  mutate(index = trimws(index, which=\"both\"),\n         index = str_replace_all(index, c(\"^-\" = \"^\",\n                                          \"-$\" = \".{0,3}$\")),\n         matches = \"yes\") %>% \n  unique %>% \n  rename(value = index)\n\nfee<- dicts5 %>% \n  filter(good == \"no\") %>% \n  select(value)\n\nfee1<- regex_left_join(fee, tempo)\nsave(fee1, file=\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/affixes.RData\")\n```\n:::\n\n\nI use this as a way of stemming and I replace the word with it. I assume that the stem captures part of the meaning of the original word, and stemming will be done anyway for the word cloud. I couldn't use hunspell as a stemmer because it kept producing NAs so this is the only way that I thought I could for these words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/affixes.RData\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file '/\nUsers/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/\naffixes.RData', probable reason 'No such file or directory'\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n```\n:::\n\n```{.r .cell-code}\nfee2<- fee1 %>% \n  filter(!is.na(matches)) %>% \n  select(value = value.x, repl = value.y) %>% \n  mutate(repl = str_remove_all(repl, \"^|\\\\.[[:punct:]]\\\\d\\\\,\\\\d..\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., !is.na(matches)): object 'fee1' not found\n```\n:::\n\n```{.r .cell-code}\nfee1 %>% filter(!is.na(matches)) %>% slice(10:15)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., !is.na(matches)): object 'fee1' not found\n```\n:::\n:::\n\n\nNext, I add these stems to the dictionary and create a column of replacements. I also remove words that are 2-3 letters long because of reasons I mentioned previosly. Only a few thousand are removed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndicts6<- dicts5 %>% \n  left_join(fee2) %>% \n  mutate(good = ifelse(!is.na(repl), \"changeme2\", good))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in left_join(., fee2): object 'dicts5' not found\n```\n:::\n\n```{.r .cell-code}\ndicts6<-dicts6 %>% \n  mutate(nchar = nchar(value)) %>% \n  mutate(change = case_when(\n    good == \"yes\" ~ value,\n    good == \"changeme\"~ value.y.y,\n    good == \"changeme1\" ~ sugg,\n    good == \"changeme2\" ~ repl,\n    nchar ==2 & !(value %in% tm::stopwords()) ~ NA_character_,\n    T ~ NA_character_\n  )) %>% \n  rename(tokens = value) %>% \n  select(tokens, change)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., nchar = nchar(value)): object 'dicts6' not found\n```\n:::\n\n```{.r .cell-code}\ndicts5 %>% \n  mutate(nchar = nchar(value)) %>%\n  count(foo=(nchar %in% c(2, 3) & good == \"no\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., nchar = nchar(value)): object 'dicts5' not found\n```\n:::\n:::\n\n\nFinally, I use the dictionary I made in order to clean the corpus. Below are examples of cleaned lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3<- dat2 %>% \n  inner_join(., dicts6) %>% \n  filter(!is.na(change)) %>% \n  group_by(titles, dates, edition, volume, part, type, page, dat) %>%\n  summarize(temp = paste0(na.omit(change), collapse = \" \")) %>% \n  ungroup\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in inner_join(., dicts6): object 'dat2' not found\n```\n:::\n\n```{.r .cell-code}\n# save(dat3, file = \"/Users/emmanuelmiller/Desktop/dacss697d/Text_as_Data_Fall_2022/posts/_data/dat3.RData\")\n\ndat3 %>% slice(100:110)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in slice(., 100:110): object 'dat3' not found\n```\n:::\n:::\n\n\n# Word Cloud\n\nIn order to visualize my results, I remove stop words and stem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:hunspell':\n\n    dictionary\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda.textplots)\n\nhmm<- corpus(dat3$temp)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in corpus(dat3$temp): object 'dat3' not found\n```\n:::\n\n```{.r .cell-code}\ndocvars(hmm)<- dat3 %>% select(-dat, -temp)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in select(., -dat, -temp): object 'dat3' not found\n```\n:::\n\n```{.r .cell-code}\nhmm_tok<- tokens(hmm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens(hmm): object 'hmm' not found\n```\n:::\n\n```{.r .cell-code}\nhmm_tok0<-tokens_remove(hmm_tok, pattern = stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"remove\"): object 'hmm_tok' not found\n```\n:::\n\n```{.r .cell-code}\nhmm_tok0<-tokens_remove(hmm_tok0, pattern = c(\"fig\", \"et\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_select(x, ..., selection = \"remove\"): object 'hmm_tok0' not found\n```\n:::\n\n```{.r .cell-code}\nhmm_tok1 <- tokens_wordstem(hmm_tok0)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tokens_wordstem(hmm_tok0): object 'hmm_tok0' not found\n```\n:::\n\n```{.r .cell-code}\nhmm_dfm<- dfm(hmm_tok1) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in dfm(hmm_tok1): object 'hmm_tok1' not found\n```\n:::\n:::\n\n\nBelow is a wordcloud of my whole corpus. I notice that the word \"may\" is one of the most frequent. This is likely because \"may\" indicates uncertainty and is frequently used in scientific publications. For this reason, I don't necessarily consider it a stopword.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_wordcloud(hmm_dfm, max_words = 50)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in textplot_wordcloud(hmm_dfm, max_words = 50): object 'hmm_dfm' not found\n```\n:::\n:::\n",
    "supporting": [
      "EmilyMiller_blogpost3.4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}